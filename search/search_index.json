{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Applied Nonlinear Perron-Frobenius Theory About This website has the aim to provide a self-contained and relatively detailed overview of the material presented at the SIAM LA and ILAS 2021 conferences minitutorial on Applied Nonlinear Perron-Frobenius Theory . The slides of the talks can be found here: Outline The Perron-Frobenius Theorem Network Centrality The Birkhoff-Hopf Theorem Optimization with nonlinear Perron eigenvectors Abstract Nonnegative matrices are pervasive in data mining applications. For example, distance and similarity matrices are fundamental tools for data classification, affinity matrices are key instruments for graph matching, adjacency matrices are at the basis of almost every graph mining algorithm, transition matrices are the main tool for studying stochastic processes on data. The Perron-Frobenius theory makes the algorithms based on these matrices very attractive from a linear algebra point of view. At the same time, as the available data grows both in terms of size and complexity, more and more data mining methods rely on nonlinear mappings rather than just matrices, which however still have some form of nonnegativity .The nonlinear Perron-Frobenius theory allows us to transfer most of the theoretical and computational niceties of nonnegative matrices to the much broader class of nonlinear multihomogeneous operators . These types of operators include for example commonly used maps associated with tensors and are tightly connected to the formulation of nonlinear eigenvalue problems with eigenvector nonlinearities. In this minitutorial we will introduce the concept of multihomogeneous operators and we will present the state-of-the-art version of the nonlinear Perron-Frobenius theorem for nonnegative nonlinear mappings. We will discuss several numerical optimization implications connected to nonlinear and higher-order versions of the Power and the Sinkhorn methods and several open challenges, both from the theoretical and the computational viewpoints. We will also discuss a number of problems in data mining, machine learning and network science which can be cast in terms of nonlinear eigenvector problems and we will show how the nonlinear Perron-Frobenius theory can help solve them. Credits The website (and the tutorial) is developed and maintained by Antoine Gautier and Francesco Tudisco .","title":"Home"},{"location":"#applied_nonlinear_perron-frobenius_theory","text":"","title":"Applied Nonlinear Perron-Frobenius Theory"},{"location":"#about","text":"This website has the aim to provide a self-contained and relatively detailed overview of the material presented at the SIAM LA and ILAS 2021 conferences minitutorial on Applied Nonlinear Perron-Frobenius Theory . The slides of the talks can be found here: Outline The Perron-Frobenius Theorem Network Centrality The Birkhoff-Hopf Theorem Optimization with nonlinear Perron eigenvectors","title":"About"},{"location":"#abstract","text":"Nonnegative matrices are pervasive in data mining applications. For example, distance and similarity matrices are fundamental tools for data classification, affinity matrices are key instruments for graph matching, adjacency matrices are at the basis of almost every graph mining algorithm, transition matrices are the main tool for studying stochastic processes on data. The Perron-Frobenius theory makes the algorithms based on these matrices very attractive from a linear algebra point of view. At the same time, as the available data grows both in terms of size and complexity, more and more data mining methods rely on nonlinear mappings rather than just matrices, which however still have some form of nonnegativity .The nonlinear Perron-Frobenius theory allows us to transfer most of the theoretical and computational niceties of nonnegative matrices to the much broader class of nonlinear multihomogeneous operators . These types of operators include for example commonly used maps associated with tensors and are tightly connected to the formulation of nonlinear eigenvalue problems with eigenvector nonlinearities. In this minitutorial we will introduce the concept of multihomogeneous operators and we will present the state-of-the-art version of the nonlinear Perron-Frobenius theorem for nonnegative nonlinear mappings. We will discuss several numerical optimization implications connected to nonlinear and higher-order versions of the Power and the Sinkhorn methods and several open challenges, both from the theoretical and the computational viewpoints. We will also discuss a number of problems in data mining, machine learning and network science which can be cast in terms of nonlinear eigenvector problems and we will show how the nonlinear Perron-Frobenius theory can help solve them.","title":"Abstract"},{"location":"#credits","text":"The website (and the tutorial) is developed and maintained by Antoine Gautier and Francesco Tudisco .","title":"Credits"},{"location":"chapter1/","text":"The Perron-Frobenius theorem for homogeneous mappings Brief history: Birkhoff (1950): Hilbert metric, Nussbaum: homogeneous mappings, \u2026: strict contractivity. Hilbert projective distance and the Birkhoff-Hopf theorem A key observation for the development of the nonlinear Perron-Frobenius theory was made by Birkhoff, when he noticed that nonnegative matrices are non-expansive with respect to the Hilbert metric induced by the cone \\(C_+\\) . The Hilbert (projective) metric \\(d_H\\colon C_+\\times C_+\\to [0,\\infty]\\) is defined by \\(d_H(0,0)=0\\) , \\(d(x,y)=\\infty\\) if \\(x\\not\\sim y\\) , and \\[d_H(x,y) = \\ln\\Big(\\max_{i,j \\in \\mathcal I} \\frac{x_i}{y_i}\\frac{y_j}{x_j}\\Big) \\quad \\text{ where }\\quad \\mathcal I =\\{i\\colon x_i>0\\},\\] for all \\(x,y\\in C_+\\setminus \\{0\\}\\) such that \\(x\\sim y\\) . Technically, \\(d_H\\) is not a metric but a pseudometric since \\(d(x,y)=0\\) may not imply \\(x=y\\) . However, it is a complete metric on the set of unit vectors contained in the same part of \\(C_+\\) . Lemma. Let \\(\\|\\cdot\\|\\) be a norm on \\(\\RR^n\\) and \\(B=\\{x\\in \\RR^n \\colon \\|x\\|=1\\}\\) . For every \\(x,y\\in C_+\\) it holds \\(d_H(x,y)=0\\) if and only if there exists \\(\\lambda >0\\) such that \\(x= \\lambda y\\) . If \\(P\\subset C_+\\) is a part of \\(C_+\\) , then \\(x\\sim y\\) for all \\(x,y\\in P\\) , and \\((P\\cap B, d_H)\\) is a complete metric space. A direct consequence of the first property in the above Lemma is that for any mapping \\(f\\colon C_+\\to C_+\\) and any \\(x\\in C_+\\) , there exists \\(\\lambda >0\\) such that \\(f(x)=\\lambda x\\) if, and only if, \\(d_H(f(x),x)=0\\) . This observation suggest that the Hilbert metric is particularly appropriate to study eigenvectors of mappings leaving a cone invariant. We should put the picture of my talk here. If \\(M\\) is any nonnegative matrix in \\(\\RR^{n\\times n}\\) , then for every \\(x,y\\in C_{++}\\) it holds \\[\\begin{equation}\\label{eq:nonexpansive} Mx \\preceq M \\Big(\\max_{i=1,\\ldots,n}\\frac{x_i}{y_i}\\Big) y = \\Big(\\max_{i=1,\\ldots,n}\\frac{x_i}{y_i}\\Big) My \\end{equation}\\] and therefore, \\(d_H(Mx,My)\\leq d_H(x,y)\\) , i.e. \\(M\\) is nonnexpansive with respect to the Hilbert metric. The Birkhoff-Hopf theorem refines this observation by offering an explicit formula for the smallest Lipschitz constant of a positive matrix with respect to the Hilbert metric. Theorem. (Birkhoff-Hopf) Let \\(M\\in \\RR^{n\\times n}\\) be a positive matrix and let \\[\\kappa(M)=\\inf\\big\\{\\alpha \\geq 0 \\colon d_H(Mx,My)\\leq d_H(x,y), \\forall x,y\\in C_+, x\\sim y\\big\\},\\] then, it holds \\[\\kappa(M)=\\tanh(\\tfrac{1}{4}\\Delta(M)) \\quad \\text{with}\\quad \\Delta(M)=\\max_{i,j,k,\\ell = 1,\\ldots,n} \\frac{M_{ij}M_{k\\ell}}{M_{i\\ell}M_{kj}}\\] The Birkhoff-Hopf theorem holds for considerably more general settings such as linear mappings defined on general cones in finite or infinite dimensional spaces. We refer to [NBproof] for a very elegant proof in the general setting. Note that from the formula for \\(\\Delta(M)\\) one can see that the smallest Lipschitz constant of \\(M\\) with respect to \\(d_H\\) does not depend on the scale of \\(M\\) , i.e. \\(\\Delta(\\alpha M)=\\Delta(M)\\) for all \\(\\alpha >0\\) . Furthermore, it holds \\(\\Delta(M^\\top)=\\Delta(M)\\) . The quantity \\(\\Delta(M)\\) is the projective diameter of the set \\(M(C_+)\\) and can be characterized as \\[ \\Delta(M)=\\sup\\{d_H(Mx,My)\\colon x,y \\in C_+, x\\sim y\\}. \\] As discussed in the following remark, the Birkhoff-Hopf theorem can be used to prove results of the Perron-Frobenius theorem for positive matrices. Furthermore, \\(\\kappa(M)\\) provides a bound on the linear convergence rate of the sequence converging towards a unique positive eigenvector. Remark. Given a positive matrix \\(M\\in\\RR^{n\\times n}\\) , the Perron-Frobenius theorem implies that \\(M\\) has a unique eigenvector \\(u \\in C_{++}\\) with \\(\\|u\\|=1\\) , and the sequence \\(x^{k}=Ax^{k-1}\\) satisfies \\(\\lim_{k\\to \\infty} x^k/\\|x^k\\|=u\\) for every \\(x^0\\in C_{++}\\) . Thanks to the Birkhoff-Hopf theorem, these results can be obtained as a direct consequence of the Banach fixed point theorem. Indeed, let \\(B\\) be the unit ball in \\(\\RR^n\\) . By the precedent Lemma we know that \\((B\\cap C_++,d_H)\\) is a complete metric space. Since the hyperbolic tangent takes values in \\((0,1)\\) , the Birkhoff-Hopf theorem implies that \\(x\\mapsto Mx\\) is a strict contraction with respect to \\(d_H\\) . Finally, note that the scale invariance of the Hilbert metric implies that \\(x\\mapsto Mx\\) and \\(x\\mapsto Mx/\\|Mx\\|\\) have the same Lipschitz constants. Hence, by the Banach fixed point theorem, the latter mapping has a unique fixed point, \\(u\\) , and its iterates converge to \\(u\\) with rate \\(\\kappa(M)\\) . However, it should be noted that the linear convergence rate provided by the Birkhoff-Hopf theorem is not as tight as the classical ratio between the second and first eigenvalues of \\(M\\) . The Hilbert metric can be used to prove a generalization of the Perron-Frobenius theorem to eigenvectors of nonlinear mappings on cones. We discuss the class of such mappings in the next section. Homogeneous and order-preserving mappings To prove that nonnegative matrices are nonexpansive with respect to \\(d_H\\) in Equation \\(\\eqref{eq:nonexpansive}\\) , we have used two important properties of the nonnegative matrix \\(M\\) : First, we have used the fact that for every \\(x,y\\in C_+\\) satisfying \\(x\\preceq y\\) it holds \\(Mx\\preceq My\\) and second, we have used the homogeneity of \\(M\\) , i.e. if \\(x\\in \\RR^n\\) and \\(\\alpha\\geq 0\\) , then \\(M(\\alpha x)= \\alpha M(x)\\) . This motivates the following definition: Definition. Let \\(K\\subset C_+\\) be a convex cone, \\(f\\colon K \\to C_+\\) and \\(p\\in \\RR\\) . \\(f\\) is \\(\\textit{order-preserving}\\) if \\(f(x)\\preceq f(y)\\) for every \\(x,y\\in K\\) satisfying \\(x\\preceq y\\) . \\(f\\) is (positively) \\(\\textit{homogeneous of degree}\\) \\(p\\) , if for every \\(x\\in K\\) and every \\(\\alpha\\geq 0\\) , it holds \\(f(\\alpha\\,x)=\\alpha\\, f(x)\\) . If \\(f\\) is homogeneous of degree \\(1\\) , we simply say that \\(f\\) is homogeneous. If \\(f\\) is differentiable on an open set containing \\(C_+\\) , then Theorem 1.3.1 of [NB] implies that \\(f\\) is order-preserving if and only if \\(D f(x)\\in C_+\\) for all \\(x\\in C_+\\) . Similarly, the Euler theorem for homogeneous mappings shows that \\(f\\) is homogeneous of degree \\(p\\) if and only if \\(D f(x) x\\, = \\, p\\, f(x)\\) for all \\(x\\in C_+\\) . If \\(C_+\\subset \\RR^1\\) , then being order-preserving is precisely the same as being increasing. The concept reducing to that of deacreasing functions is termed order-reversing. A mapping \\(f\\colon K\\to C_+\\) is order-reversing, if \\(f(y)\\preceq f(x)\\) for every \\(x,y\\in K\\) satisfying \\(x\\preceq y\\) . We will however mainly discuss order-preserving functions. Example. Consider the mappings \\(f\\colon C_+ \\to C_+\\) , \\(g\\colon C_+ \\to C_+\\) and \\(h\\colon C_+ \\to C_+\\) , respectively defined as \\(f\\) o.p. \\(g\\) order reversing \\(h\\) o.p. A similar argument as in Equation \\(\\eqref{eq:nonexpansive}\\) yields the following: Lemma. Let \\(K\\in\\{C_+,C_{++}\\}\\) and let \\(f\\colon K \\to C_+\\) be homogeneous of degree \\(p\\in \\RR\\) . Suppose that \\(f\\) is either order-preserving or order-reversing, then \\[ d_H(f(x),f(y))\\leq |p|\\,d_H(x,y)\\qquad \\forall x,y\\in C_{++}.\\] The classical Perron-Frobenius theorem is concerned with eigenvalues and eigenvectors of linear mappings and their spectral radius. The notions of eigenvalues and eigenvectors for a general mapping \\(f\\colon K\\to C_+\\) with \\(K\\subset C_+\\) can be extended as follows: We say that \\(x\\in C_+\\) is an eigenvector of if there exists \\(\\lambda\\geq 0\\) such that \\(f(x)=\\lambda x\\) , \\(\\lambda\\) is called an eigenvalue. However, the notion of spectral radius is more delicate to generalize. Indeed, without further assumptions on \\(f\\) , the spectrum of \\(f\\) (the set of its eigenvalues) can be empty or unbounded. We consider the following definition: Definition. Let \\(f\\colon C_+ \\to C_+\\) be order-preserving and homogeneous. The spectral radius of \\(f\\) is defined by \\[\\rho(f) = \\lim_{k\\to \\infty} \\|f^k\\|_{C_+}^{1/k},\\] where \\(f^k=f\\circ \\ldots \\circ f\\) is the \\(k\\) -th composition of \\(f\\) with itself and \\(\\|f\\|_{C_+}=\\max\\{\\|f(x)\\| \\colon x\\in C_+, \\|x\\|\\leq 1\\}\\) is the operator norm of \\(f\\) with respect to any norm \\(\\|\\cdot \\|\\) on \\(\\RR^n\\) . The above definition of \\(\\rho(f)\\) is usually referred to as the Bonsall spectral radius of \\(f\\) . Another definition of spectral radii have been proposed, namely the cone spectral radius of \\(f\\) . Nevertheless, it is shown in [specrad] that both definitions are equivalent for homogeneous order-preserving mappings on \\(C_+\\) . Let us further observe that for mappings \\(f\\colon C_+\\to C_+\\) which are homogeneous of degree \\(p\\neq 1\\) , the concept of eigenvalue is less clear as they depend on the scaling of the eigenvector. However, as discussed in the next section, if \\(f\\colon C_{++}\\to C_{++}\\) is order-preserving and homogeneous of degree \\(p<1\\) , then \\(f\\) always have a unique eigenvector. The Perron-Frobenius theorem and the Collatz-Wielandt formula Because we do not assume linearity, the assumptions of the Perron-Frobenius for homogeneous mappings are more delicate. To highlight their connection with the linear case, we first recall the results of the Perron-Frobenius theorem. A first remarkable consequence of the above ovservations is the following version of the Perron-Frobenius theorem concerning mappings which are homogeneous of degree \\(p\\) with \\(|p|<1\\) . Lemma. Let \\(K\\in\\{C_+,C_{++}\\}\\) and let \\(f\\colon K \\to C_+\\) be homogeneous of degree \\(p\\in \\RR\\) . Suppose that \\(f\\) is either order-preserving or order-reversing, then \\[ d_H(f(x),f(y))\\leq |p|\\,d_H(x,y)\\qquad \\forall x,y\\in C_{++}.\\] In the general case of order-preserving, homogeneous mappings the assumptions of the Perron-Frobenius are more technical. In particular, Application: Computing the \\(q\\to p\\) norm of a nonnegative matrix Application: The Sinkhorn method Perron-Frobenius theory for linear mappings Brief history In 1907, in [\u2026], Oskar Perron proves that the spectral radius of primitive matrices is an algebraically simple eigenvalue and the corresponding eigenvector can be scaled to have all entries positive. In 1908, in [\u2026], George Frobenius extends the result by showing that the spectral radius of irreducible matrices is a geometrically simple eigenvalue and the corresponding eigenvector can be scaled to have positive entries. Furthermore, Perron observes that when the spectral radius is an algebraically simple eigenvalue, one can use the power method introduced by \u2026 in \u2026 to compute a maximal eigenvector, i.e. an eigenvector corresponding with the spectral radius as eigenvalue. These results have been then complemented by a broad diversity of results about nonnegative matrix and we refer to [Plemmons] for further readings on the topic. In particular, we will discuss the Collatz-Wielandt ratio, which provides a sup-min and an inf-max characterization of the spectral radius. To be killed We briefly recall here well-known results and concepts of linear algebra which will are useful to understand the subsequent discussion. Given a matrix \\(A\\in\\RR^{n\\times n}\\) , \\(\\lambda\\in \\RR\\) is an eigenvalue of \\(A\\) if there exists \\(x\\in \\RR^n\\) with \\(x\\neq 0\\) and \\(Ax = \\lambda x\\) . A vector \\(x\\) is called an eigenvector of \\(A\\) if \\(x\\neq 0\\) and there exists a corresponding eigenvalue \\(\\lambda\\in \\RR\\) such that \\(Ax=x\\) . An eigenvalue \\(\\lambda\\) of \\(A\\) is algebraically simple if it is a simple root of the polynomial \\(p(t)=\\det(A-tI)\\) , where \\(I\\in\\RR^{n\\times n}\\) is the identity matrix. An eigenvalue \\(\\lambda\\) of \\(A\\) is geometrically simple if \\(\\dim(\\ker(A-\\lambda I))=1\\) , i.e. there is a unique up to scale eigenvector corresponding to \\(\\lambda\\) . Algebraic simplicity implies geometric simplicity but the converse is not true in general. The spectral radius of \\(A\\in\\RR^{n\\times n}\\) is denoted by \\(\\rho(A)\\) and defined as \\[\\rho(A)=\\max\\{|\\lambda| \\colon \\lambda \\text{ is an eigenvalue of }A\\}\\] For \\(p\\in[1,\\infty)\\) , let \\(\\|x\\|_p = \\big(\\sum_{i=1}^n|x_i|^p\\big)^{1/p}\\) denote the usual \\(p\\) -norm for every \\(x\\in\\RR^n\\) . The cone of nonnegative vectors The Perron-Frobenius theory deals with operators that leave a (\u00bfsolid?) normal, convex and pointed cone invariant. The cone of nonnegative vectors is the most common example of such a cone and the one that most often arises in applications. We denote such cone as \\(C_+ \\subseteq \\mathbb R^n\\) or, when the dimension is clear from the context, simply as \\(C_+.\\) The elements of \\(C_+\\) are vectors whose components are all nonnegative. Whereas the interior \\(C_{++}\\) of \\(C_+\\) is the set of entrywise positive vectors: \\(C_+ = \\{x\\in \\mathbb R^n : x_i\\geq 0\\) for all \\(i=1,\\dots,n\\}=\\{x\\in \\mathbb R^n : x\\succeq 0\\}\\) \\(C_{++} = \\{x\\in \\mathbb R^n : x_i> 0\\) for all \\(i=1,\\dots,n\\}=\\{x\\in \\mathbb R^n : x\\succ 0\\}.\\) The cone \\(C_+\\) induces a partial ordering on \\(\\RR^n\\) defined as \\(x\\preceq y\\) if \\(y-x\\in C_+\\) . In particular, as \\(C_+\\) denotes the cone of nonnegative vectors in \\(\\RR^n\\) , we have that \\(x\\preceq y\\) if and only if \\(x_i \\leq y_i\\) for all \\(i=1,\\ldots,n\\) . This partial order induce a equivalence relation \\(\\sim\\) on \\(C_+\\) defined as \\(x\\sim y\\) if there exists \\(\\alpha,\\beta>0\\) such that \\(\\alpha x \\preceq y \\preceq \\beta x\\) . The equivalence classes are called the parts of the cone \\(C_+\\) . For example, \\(C_+\\subset\\RR^2\\) has four parts given \\(\\{0\\}, \\{(s,0)\\colon s>0\\}, \\{(0,t)\\colon t>0\\}, C_{++}\\) . \\(C_+\\) is convex, pointed and normal as \\(\\alpha x\\in C_+\\) for all \\(x\\in C_+\\) and all scalar coefficients \\(\\alpha \\geq 0\\) , \\(C_+\\cap -C_+ = \\{0\\}\\) and for every norm \\(\\|\\cdot\\|\\) on \\(\\RR^n\\) , there exists \\(\\gamma>0\\) such that for all pair of vectors \\(x,y\\in C_+\\) such that \\(x\\preceq y\\) it holds \\(\\|x\\|\\leq \\gamma \\|y\\|\\) . 1 This tutorial will focus only on \\(C_+\\) , however we point out that all the results we will present can be relatively directly transferred to arbitrary normal, convex and pointed cones. Nonnegative, irreducible, primitive and positive matrices The classical Perron-Frobenius theory is mainly concerned with matrices having nonnegative entries. Based on the pattern of their positive entries, multiple results can be derived concerning the simplicity of their eigenvalues of maximal magnitude and the corresponding eigenvectors. To characterize different patterns of positive entries, we recall the definition of primitive and irreducible matrices. We aslo recall characterizations of these definitions as they will be generalized in different ways in the nonlinear setting: Definition. Let \\(A\\in\\RR^{n\\times n}\\) be a matrix. \\(A\\) is nonnegative if \\(A_{ij}\\geq 0\\) for all \\(i,j=1,\\ldots,n\\) . \\(A\\) is positive if \\(A_{ij}>0\\) for all \\(i,j=1,\\ldots,n\\) . \\(A\\) is irreducible if \\(A\\) is nonnegative and there exists an integer \\(m\\) such that \\((I+A)^m\\) is positive. \\(A\\) is primitive if \\(A\\) is nonnegative and there exists an integer \\(m\\) such that \\(A^m\\) is positive. A matrix \\(A\\) is positive, if and only if \\(A^{\\top}\\) is positive. This observation holds for all four definitions above, i.e. \\(A\\) is nonnegative, irreducible, primitive, respectively, if and only if $A^{\\top} has the corresponding property. We illustrate these definitions with an example. Example. Consider the matrices in \\(\\RR^{2\\times 2}\\) defined as \\[\\begin{equation}\\label{def:Example_ABCD} A=\\bigg(\\begin{matrix}0 & 0 \\\\ 1 & 1\\end{matrix}\\bigg), \\qquad B=\\bigg(\\begin{matrix}0 & 1 \\\\ 1 & 0\\end{matrix}\\bigg), \\qquad C=\\bigg(\\begin{matrix}0 & 1 \\\\ 1 & 1\\end{matrix}\\bigg),\\qquad D=\\bigg(\\begin{matrix}1 & 2 \\\\ 1 & 1\\end{matrix}\\bigg) \\end{equation}\\] Then: \\(A,B,C,D\\) are nonnegative; \\(B,C,D\\) are irreducible; \\(C,D\\) are primitive; \\(D\\) is positive; \\(A,B,C\\) are not positive; \\(A,B\\) are not primitive; \\(A\\) is not irreducible. These defintions can be interpret in the context of graphs. Indeed, one can associated a directed graph \\(G(A)=(V,E)\\) to a nonnegative matrix \\(A\\in \\RR^{n\\times n}\\) as follows: The vertexes are \\(V=\\{1,\\ldots,n\\}\\) and there is an edge from \\(i\\) to \\(j\\) , i.e. \\((i,j)\\in E\\) , if \\(A_{ij}>0\\) . A directed path from \\(i\\in V\\) to \\(j\\in V\\) , is a sequence of \\(\\ell\\) edges \\((k_1,k_2),(k_2,k_3),\\ldots,(k_{\\ell},k_{\\ell+1})\\in E\\) such that \\(k_1=1\\) and \\(k_{\\ell+1}= j\\) . The length of a directed path is the number of edges it traverses, namely \\(\\ell\\) . The above definitions can now be equivalently formulated as follows: Proposition. Let \\(A\\in\\RR^{n\\times n}\\) be a nonnegative matrix and \\(G(A)=(V,E)\\) be its associated graph. \\(A\\) is positive if, and only if, \\(G(A)\\) is complete, i.e. \\(E=\\{(i,j)\\mid \\forall i,j\\in V\\}\\) . \\(A\\) is irreducible if, and only if, \\(G(A)\\) is connected, i.e. for all \\(i,j\\in V\\) there exists a directed path from \\(i\\) to \\(j\\) (see Theorem \u2026). \\(A\\) is primitive if, and only if, \\(G(A)\\) is connected and the greatest common divisor of the lengths of all the directed paths starting and ending at the same node equals \\(1\\) (see Theorem 8.5.3 [Horn]). Note that the above characterizations highlight the fact that positivivity, irreducibility and primtivity are concepts related to the pattern of the positive entries rather than the magnitude of these entries. Another way to characterize nonnegative, positive, irreducible and primitive matrices, is to analyze their image on vectors with nonnegative entries. Note for instance that a matrix \\(A\\in\\RR^{n\\times n}\\) is nonnegative if, and only if, \\(Ax\\in C_+\\) for all \\(x\\in C_+\\) . Proposition. Let \\(A\\in\\RR^{n\\times n}\\) be a nonnegative matrix. \\(A\\) is positive if, and only if, \\(Ax\\in C_{++}\\) for all \\(x\\in\\RR^n_+\\setminus\\{0\\}\\) . \\(A\\) is irreducible if, and only if, there exists an integer \\(m\\) such that \\(\\sum_{k= 0}^m A^{k}x\\in C_{++}\\) for all \\(x\\in\\RR^n_+\\setminus\\{0\\}\\) (see Theorem \u2026). \\(A\\) is primitive if, and only if, there exists an integer \\(m\\) such that \\(A^{m}x\\in C_{++}\\) for all \\(x\\in C_+\\setminus\\{0\\}\\) (see Theorem \u2026). From the above characterization and the matrices \\(A,B,C,D\\) of the Example, one can deduce the following inclusions: Lemma. Let \\(n>1\\) be an integer and consider the following sets of matrices: \\[\\begin{equation*} \\begin{array}{l c l} M_{\\text{nneg}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is nonnegative}\\}, && M_{\\text{irr}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is irreducible}\\},\\\\ M_{\\text{prim}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is primitive}\\}, && M_{\\text{pos}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is positive}\\}.\\end{array} \\end{equation*}\\] Then, it hold \\(M_{\\text{nneg}} \\subsetneq M_{\\text{irr}} \\subsetneq M_{\\text{prim}} \\subsetneq M_{\\text{pos}}\\) . We refer to [\u2026] for an extended discussion on nonnegative, irreducible, primitive and positive matrices. The Perron-Frobenius theorem and the Collatz-Wielandt formula We can now state the celebrated theorem of Perron and Frobenius which provides conditions for the existence of a nonnegative eigenvector, i.e. an eigenvector in \\(C_+\\) , the existence and uniqueness of a positive eigenvector, i.e. an eigenvector in \\(C_{++}\\) , and the convergence of a sequence towards a unique positive eigenvector. Note that, by unique eigenvector, we mean unique up to scale so that \\(u\\) and \\(\\alpha u\\) are considered to be the same eigenvector. Theorem (Perron-Frobenius). Let \\(A\\in\\RR^{n\\times n}\\) be a matrix and \\(\\|\\cdot\\|\\) a norm on \\(\\RR^n\\) . If \\(A\\) is nonnegative, then there exists an eigenvector \\(u\\in C_+\\) such that \\(Au=\\rho(A)u\\) . If \\(A\\) is irreducible, then \\(u\\) is the unique up to scale eigenvector of \\(A\\) in \\(\\RR^n_+\\) . Furthermore \\(u\\in C_{++}\\) and \\(\\rho(A)\\) is a geometrically simple eigenvalue. If \\(A\\) is primitive, then \\(\\rho(A)\\) is algebraically simple and for every \\(x^0\\in C_+\\setminus\\{0\\}\\) , the sequence \\((x^k)_{k=0}^{\\infty}\\subset\\RR^n\\) defined as \\(x^k = A x^{k-1}\\) for all \\(k\\geq 1\\) satisfies \\[\\lim_{k\\to \\infty} \\frac{x^k}{\\|x^k\\|}= \\frac{u}{\\|u\\|}.\\] The matrices of the previous example shows that the assumptions in the Perron-Frobenius theorem are well calibrated. Remark. Consider the matrices \\(A,B,C,D\\) defined in \\(\\eqref{def:Example_ABCD}\\) . Then \\(B,C,D\\) have \\((1,1)^\\top\\) as positive eigenvector. The matrix \\(A\\) , is an example of nonnegative matrix which is not irreducible, and have no positive eigenvector. The matrix \\(B\\) , is an example of irreducible matrix which is not primitive, and for which the sequence \\((x^k)_{k=0}^{\\infty}\\) defined in the Perron-Frobenius theorem do not converge whenever \\(x^{0}\\in\\RR^2\\) satisfy \\(x^{0}_1\\neq x^{0}_2\\) . We mention one additional result from Collatz and Wielandt which provides a sup-min and an inf-max characterization of the spectral radius. The expressions in the Collatz-Wielandt formula is closely related to the Hilbert metric which is a key ingredient to generalize the Perron-Frobenius theorem. As illustrated by the applications below, the Collatz-Wielandt formula is useful to prove the maximality of an eigenvalue. Theorem (Collatz-Wielandt). Let \\(A\\in\\RR^{n\\times n}\\) be a nonnegative matrix. Then, it holds \\[\\begin{equation}\\label{def:cw_up} \\rho(A)\\quad =\\quad \\inf_{x\\in\\RR_{++}^n}\\quad \\max_{i=1,\\ldots,n}\\quad \\frac{(Ax)_i}{x_i}. \\end{equation}\\] If additionally, \\(A\\) has an eigenvector \\(u\\in\\RR^n_{++}\\) , then it holds \\[\\begin{equation}\\label{def:cw_down} \\rho(A)\\quad = \\quad\\sup_{\\substack{x\\in\\RR_{+}^n\\\\ x\\neq 0}} \\quad\\min_{\\substack{i=1,\\ldots,n\\\\ x_i >0}}\\quad \\frac{(Ax)_i}{x_i}. \\end{equation}\\] and both, the infimum and the supremum above, are attained at \\(u\\) . The Collatz-Wielandt formula is useful to discuss the maximality of eigenvalues, i.e. the egivenvalue of any positive eigenvector equals the spectral radius. Application: Stochastic matrices and homogeneous Markov chains on \\([n]\\) Let \\(S\\in\\RR^{n\\times n}\\) be a stochastic matrix, i.e. \\(S\\) is nonnegative and \\(S\\mathbf{1}=\\mathbf{1}\\) with \\(\\mathbf{1}=(1,\\ldots,1)^\\top \\in\\RR^n\\) . Let us connsider the set of probability vectors \\(\\Delta_+=\\{x\\in\\RR^n_+\\colon x_1+\\ldots+x_n=1\\}\\) . Note that \\(S\\) is a nonnegative matrix and \\(u=\\frac{1}{n}\\mathbf{1}\\) is a positive eigenvector of \\(S\\) corresponding to the eigenvalue \\(\\lambda=1\\) . We show the existence and uniqueness of a startionary distribution. First, note that by the Collatz-Wielandt formula, \\[\\rho(S) =\\inf_{x\\in\\RR_{++}^n} \\max_{i=1,\\ldots,n} \\frac{(Sx)_i}{x_i} = \\max_{i=1,\\ldots,n} \\frac{(Su)_i}{u_i} = 1. \\] As \\(S^\\top\\) is a nonnegative matrix, by the Perron-Frobenius theorem, we know that there exists \\(\\pi\\in \\RR^n_+\\) with \\(\\pi_1+\\ldots+\\pi_n=1\\) and \\(S^\\top \\pi = \\rho(S^\\top) \\pi\\) . As \\(\\rho(S^\\top)=\\rho(S)=1\\) , we have \\(S^\\top \\pi =\\pi\\) , i.e. \\(\\pi\\) is a stationary distribution of \\(S\\) . This proves the existence. If \\(S\\) is irreducible, then \\(S^T\\) is irreducible as well and by the Perron-Frobenius theorem, \\(\\pi\\) is the unique stationary distribution of \\(S\\) and \\(\\pi_1,\\ldots,\\pi_n>0\\) . Consider a sequence of random variables \\(X_0,X_1,\\ldots\\) with values in \\([n]=\\{1,\\ldots,n\\}\\) . Suppose that \\[\\operatorname{Pr}(X_k=j_k\\mid X_{k-1}=j_{k-1},\\ldots X_0=j_0)=\\operatorname{Pr}(X_1=j_k\\mid X_0=j_0)= S_{j_kj_0}\\] for all \\(k=1,2,\\ldots\\) \\(X_0,X_1,\\ldots\\) is a homogeneous Markov chain. Note that if \\(X_0\\) has distribution \\(x^0\\in\\Delta_+\\) , i.e. \\(\\operatorname{Pr}(X_0 = j)=x^0_j\\) for all \\(j\\in [n]\\) , then the distribution \\(x^k\\in\\Delta_+\\) of \\(X_k\\) satisfies \\(x^k=(S^\\top)^k x^{0}\\) for all \\(k\\geq 0\\) . So, if \\(S\\) is primitive, then so is \\(S^T\\) . Let \\(\\|\\cdot\\|_1\\) be the \\(1\\) -norm, i.e. \\(\\|x\\|_1=|x_1|+\\ldots+|x_n|\\) . Note that as \\(x^k\\in \\Delta_+\\) , it holds \\(\\|x^k\\|_1=1\\) for all \\(k\\geq 0\\) . Suppose, that \\(S\\) is primitive, then the Perron-Frobenius theorem, with \\(\\|\\cdot\\|=\\|\\cdot\\|_1\\) , implies that for every initial distribution \\(x^0\\in \\Delta_+\\) of \\(X_0\\) , it holds \\(\\lim_{k\\to \\infty}x^k = \\pi\\) . Application: Spectral norm of a nonnegative matrix Let \\(\\|\\cdot\\|_2\\) be the \\(2\\) -norm on \\(\\RR^n\\) , the induced matrix norm \\(\\|\\cdot\\|_{2,2}\\) on \\(\\RR^{n\\times n}\\) is the spectral norm defined as \\[ \\|A\\|_{2,2}=\\max_{x\\in\\RR^n}\\{\\|Ax\\|_2 \\colon \\|x\\|_2\\leq 1\\}=\\max_{x\\neq 0}\\frac{\\| Ax \\|_2}{\\| x \\|_2}. \\] The function \\(x\\mapsto \\|Ax\\|_2\\) is continuous on the compact set \\(\\{x\\colon \\|x\\|_2\\leq 1\\}\\) and therefore attains a maximum \\(x^*\\in\\RR^n, x^*\\neq 0\\) . Now, note that since \\(A\\) is nonnegative, we have \\(|Ax| \\leq A|x|\\) where the absolute value and the inequality are understood component-wise. In particular, this implies that \\[ \\|A\\|_{2,2}=\\frac{\\| Ax^* \\|_2}{\\| x^* \\|_2}=\\frac{\\| \\,|Ax^*|\\, \\|_2}{\\|\\,| x^*|\\, \\|_2}\\leq \\frac{\\| \\,|Ax^*|\\, \\|_2}{\\|\\,| x^*|\\, \\|_2}, \\] and therefore we may assume without loss of generality that \\(x^*\\in\\RR^n_+\\) . Now, note that if \\(v\\in\\RR^n_+\\) is a critical point of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) , the gradient of the later function vanishes at \\(x\\) and therefore it satisfies \\[A^TAv = \\lambda v \\qquad \\text{with} \\qquad \\lambda =\\Big(\\frac{\\| Av \\|_2}{\\| v \\|_2}\\Big)^2=\\|A\\|_{2,2}^2, \\] i.e. \\(v\\) is an eigenvector of \\(A^TA\\) . The converse of this statement holds true as well, that is, if \\(v\\) is an eigenvector of \\(A^TA\\) with eigenvalue \\(\\lambda\\) , then \\(v\\) is a critical point of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) and \\(\\lambda = (\\|Ax\\|_2/\\|x\\|_2)^2\\) . Therefore, the eigenvalues of \\(A^TA\\) and the square of the critical values of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) are equal. In particular, this implies that \\(\\rho(A^TA) =\\|A\\|_{2,2}^2\\) . Now, \\(A^TA\\) is a nonnegative matrix and if it is irreducible, by the Perron-Frobenius theorem, \\(A^TA\\) has a unique positive eigenvector \\(u\\in \\RR^n_{+}\\) such that \\(\\|u\\|_2=1\\) . Furthermore, \\(u\\in \\RR^n_{++}\\) and the corresponding eigenvalue is \\(\\rho(A^TA)\\) . From the above discussion: \\(x^*\\) is a global maximizer of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) and we may assume \\(x^*\\in\\RR^n_+\\) . Hence, \\(x^*\\) is a nonnegative eigenvector of \\(A^TA\\) and since \\(u\\) is the unique eigenvector of \\(A^TA\\) , we have \\(x^*/\\|x^*\\|_2 = u\\) . Finally, if \\(A^TA\\) is primitive, then for every \\(x^0\\in\\RR^n_+\\setminus \\{0\\}\\) , the sequence \\(x^k=A^TAx^{k-1}\\) satsifies \\(\\lim_{k\\to\\infty} x^k/\\|x^k\\| = x^*/\\|x^*\\|_2\\) , i.e. this sequence converges to a global maximizer of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) . We conclude by noting that, while the irreducibility of \\(A\\) implies the irreducibility of \\(A^TA\\) , the converse is not true in general. Indeed, the matrix \\(A\\) of \\(\\eqref{def:Example_ABCD}\\) is not irreducible, but \\(A^TA\\) is a positive matrix and is therefore irreducible and primitive. The Perron-Frobenius theorem for multi-homogeneous mappings Nonnegative tensors and their norm Product of cones and multi-homogeneous mappings Eigenvalues and eigenvectors of multi-homogeneous mappings The Perron-Frobenius theorem and the Collatz-Wielandt formula Application: Computing the norm of a nonnegative tensor Application: \u2026 The cone of nonnegative vectors The Perron-Frobenius theory deals with operators that leave a normal, convex and pointed cone invariant. The cone of nonnegative vectors is the most common example of such a cone and the one that most often arises in applications. We denote such cone as \\(C_+ \\subseteq \\mathbb R^n\\) or, when the dimension is clear from the context, simply as \\(C_+.\\) The elements of \\(C_+\\) are vectors whose components are all nonnegative. Whereas the interior \\(C_{++}\\) of \\(C_+\\) is the set of entrywise positive vectors: \\(C_+ = \\{x\\in \\mathbb R^n : x_i\\geq 0\\) for all \\(i=1,\\dots,n\\}=\\{x\\in \\mathbb R^n : x\\succeq 0\\}\\) \\(C_{++} = \\{x\\in \\mathbb R^n : x_i> 0\\) for all \\(i=1,\\dots,n\\}=\\{x\\in \\mathbb R^n : x\\succ 0\\}.\\) \\(C_+\\) is convex, pointed and normal as \\(\\alpha x\\in C_+\\) for all \\(x\\in C_+\\) and all scalar coefficients \\(\\alpha \\geq 0\\) , \\(C_+\\cap -C_+ = \\{0\\}\\) and for all pair of vectors \\(x,y\\in C_+\\) such that \\(x\\preceq y\\) there exists a \\(\\gamma>0\\) such that \\(\\|x\\|\\leq \\gamma \\|y\\|\\) . Here \\(\\|\\cdot\\|\\) is any norm on \\(\\mathbb R^n\\) . This tutorial will focus only on \\(C_+\\) however, however we point out that all the results we will present can be relatively directly transferred to arbitrary normal, convex and pointed cones. Hilbert projective distance One of the key tools for the Perron-Frobenius theory for nonlinear mappings is the Hilbert distance. For any two positive vectors \\(x,y\\succ 0\\) , this is defined as: 1 \\[d_H(x,y) = \\ln\\Big(\\max_{i=1,\\dots,n}\\frac{x_i}{y_i}\\max_{i=1,\\dots,n}\\frac{y_i}{x_i}\\Big)\\] \\(d_H\\) is a metric on the space of rays in \\(C_{++}\\) , that is it holds 2 \\(d_H(\\alpha x, \\beta y) =d_H(x,y)\\) for all scalar coefficients \\(\\alpha,\\beta >0\\) ds Lorem markdownum fuit peragit eunti, cum illuc heu ferrumque gentis. Curvarique loca remoraminaque terram iniustaque tempora luctusque vitam; tela iunxit gratia fontibus ut tempus cupiasque relevasse altis. Mutantur carpere geratis Scorpius. Visa ego delendaque, victae esse pervenit et arentia gentem. 1 2 3 4 5 6 7 8 9 10 if (5 + net) { alignment = jumperPpi; ppl_server = multiplatform(2, -1) + switchCable; } var integerCarrier = uml_p_tween; metafileCdPram -= scareware.barMultiprocessingIo.box(fiber, plugSyn.integrated_add(gif, camera, export_vga_multithreading)); if (processorSdk) { taskCcd = client_clipboard_syntax; } Prope sine coepta: reddi tabo virens expositum amaris saxo medio fons esse Nisi. Dixit dolorem simulamina motus. Gradibus tuta quae addere vero iuvenis candida; ictu Phocaico, tempore addicere pluma terra? Est deus pericula exigere figit Iani rumpere raptos o columbae non et leonis ferre qui quinque arva idem dixit haec; advertite candore. Mora invitusque casses, totidemque et erat in nonne novat iamque: quae? Ab nec fecit et idonea nunc lumine Spartana nunc vertice brevis pendebant levius contraria aere? Iram orba ferarum dei eburnea; voce praestantes, me squamas victor contigit pia! 1 2 3 pitch_ecc.serp_clip = -5 + -1; box.tape_digital(subnet_only); esports_query_multiprocessing.fontPiconetServices -= document_laptop; Ipse nomen, certum ego; heros nam, donec et semper parenti formatae si iuvenum. Insidias dextera anumque est habitandae nubes. Quo Latina, quidem: positisque, inter regia puer iurgia, illa illos ille limine flammifera dracones et. Retenta Metione atque iam quoque Iunone pabula. Canna ora maxima non penates cautum cruorem quid quid falsi. Ara plus templa, nymphe innato parum vestrae , tunc caput instat quo. Tum moras si Euagrus freta ferrove urbs nec etiam ipsis. Nunc quod aut intrarunt et graves urbes, telluris carmen coget corpus. This definition is strictly related to the choice of the nonnegative cone \\(C_+\\) . For general cones, the definition of \\(d_H\\) changes. However, all its properties do not change. \u21a9 \u21a9 Lemmens, Nussbaum, Citation test \u21a9","title":"The Perron-Frobenius theorem for homogeneous mappings"},{"location":"chapter1/#the_perron-frobenius_theorem_for_homogeneous_mappings","text":"","title":"The Perron-Frobenius theorem for homogeneous mappings"},{"location":"chapter1/#brief_history_birkhoff_1950_hilbert_metric_nussbaum_homogeneous_mappings_strict_contractivity","text":"","title":"Brief history: Birkhoff (1950): Hilbert metric, Nussbaum: homogeneous mappings, ...: strict contractivity."},{"location":"chapter1/#hilbert_projective_distance_and_the_birkhoff-hopf_theorem","text":"A key observation for the development of the nonlinear Perron-Frobenius theory was made by Birkhoff, when he noticed that nonnegative matrices are non-expansive with respect to the Hilbert metric induced by the cone \\(C_+\\) . The Hilbert (projective) metric \\(d_H\\colon C_+\\times C_+\\to [0,\\infty]\\) is defined by \\(d_H(0,0)=0\\) , \\(d(x,y)=\\infty\\) if \\(x\\not\\sim y\\) , and \\[d_H(x,y) = \\ln\\Big(\\max_{i,j \\in \\mathcal I} \\frac{x_i}{y_i}\\frac{y_j}{x_j}\\Big) \\quad \\text{ where }\\quad \\mathcal I =\\{i\\colon x_i>0\\},\\] for all \\(x,y\\in C_+\\setminus \\{0\\}\\) such that \\(x\\sim y\\) . Technically, \\(d_H\\) is not a metric but a pseudometric since \\(d(x,y)=0\\) may not imply \\(x=y\\) . However, it is a complete metric on the set of unit vectors contained in the same part of \\(C_+\\) . Lemma. Let \\(\\|\\cdot\\|\\) be a norm on \\(\\RR^n\\) and \\(B=\\{x\\in \\RR^n \\colon \\|x\\|=1\\}\\) . For every \\(x,y\\in C_+\\) it holds \\(d_H(x,y)=0\\) if and only if there exists \\(\\lambda >0\\) such that \\(x= \\lambda y\\) . If \\(P\\subset C_+\\) is a part of \\(C_+\\) , then \\(x\\sim y\\) for all \\(x,y\\in P\\) , and \\((P\\cap B, d_H)\\) is a complete metric space. A direct consequence of the first property in the above Lemma is that for any mapping \\(f\\colon C_+\\to C_+\\) and any \\(x\\in C_+\\) , there exists \\(\\lambda >0\\) such that \\(f(x)=\\lambda x\\) if, and only if, \\(d_H(f(x),x)=0\\) . This observation suggest that the Hilbert metric is particularly appropriate to study eigenvectors of mappings leaving a cone invariant. We should put the picture of my talk here. If \\(M\\) is any nonnegative matrix in \\(\\RR^{n\\times n}\\) , then for every \\(x,y\\in C_{++}\\) it holds \\[\\begin{equation}\\label{eq:nonexpansive} Mx \\preceq M \\Big(\\max_{i=1,\\ldots,n}\\frac{x_i}{y_i}\\Big) y = \\Big(\\max_{i=1,\\ldots,n}\\frac{x_i}{y_i}\\Big) My \\end{equation}\\] and therefore, \\(d_H(Mx,My)\\leq d_H(x,y)\\) , i.e. \\(M\\) is nonnexpansive with respect to the Hilbert metric. The Birkhoff-Hopf theorem refines this observation by offering an explicit formula for the smallest Lipschitz constant of a positive matrix with respect to the Hilbert metric. Theorem. (Birkhoff-Hopf) Let \\(M\\in \\RR^{n\\times n}\\) be a positive matrix and let \\[\\kappa(M)=\\inf\\big\\{\\alpha \\geq 0 \\colon d_H(Mx,My)\\leq d_H(x,y), \\forall x,y\\in C_+, x\\sim y\\big\\},\\] then, it holds \\[\\kappa(M)=\\tanh(\\tfrac{1}{4}\\Delta(M)) \\quad \\text{with}\\quad \\Delta(M)=\\max_{i,j,k,\\ell = 1,\\ldots,n} \\frac{M_{ij}M_{k\\ell}}{M_{i\\ell}M_{kj}}\\] The Birkhoff-Hopf theorem holds for considerably more general settings such as linear mappings defined on general cones in finite or infinite dimensional spaces. We refer to [NBproof] for a very elegant proof in the general setting. Note that from the formula for \\(\\Delta(M)\\) one can see that the smallest Lipschitz constant of \\(M\\) with respect to \\(d_H\\) does not depend on the scale of \\(M\\) , i.e. \\(\\Delta(\\alpha M)=\\Delta(M)\\) for all \\(\\alpha >0\\) . Furthermore, it holds \\(\\Delta(M^\\top)=\\Delta(M)\\) . The quantity \\(\\Delta(M)\\) is the projective diameter of the set \\(M(C_+)\\) and can be characterized as \\[ \\Delta(M)=\\sup\\{d_H(Mx,My)\\colon x,y \\in C_+, x\\sim y\\}. \\] As discussed in the following remark, the Birkhoff-Hopf theorem can be used to prove results of the Perron-Frobenius theorem for positive matrices. Furthermore, \\(\\kappa(M)\\) provides a bound on the linear convergence rate of the sequence converging towards a unique positive eigenvector. Remark. Given a positive matrix \\(M\\in\\RR^{n\\times n}\\) , the Perron-Frobenius theorem implies that \\(M\\) has a unique eigenvector \\(u \\in C_{++}\\) with \\(\\|u\\|=1\\) , and the sequence \\(x^{k}=Ax^{k-1}\\) satisfies \\(\\lim_{k\\to \\infty} x^k/\\|x^k\\|=u\\) for every \\(x^0\\in C_{++}\\) . Thanks to the Birkhoff-Hopf theorem, these results can be obtained as a direct consequence of the Banach fixed point theorem. Indeed, let \\(B\\) be the unit ball in \\(\\RR^n\\) . By the precedent Lemma we know that \\((B\\cap C_++,d_H)\\) is a complete metric space. Since the hyperbolic tangent takes values in \\((0,1)\\) , the Birkhoff-Hopf theorem implies that \\(x\\mapsto Mx\\) is a strict contraction with respect to \\(d_H\\) . Finally, note that the scale invariance of the Hilbert metric implies that \\(x\\mapsto Mx\\) and \\(x\\mapsto Mx/\\|Mx\\|\\) have the same Lipschitz constants. Hence, by the Banach fixed point theorem, the latter mapping has a unique fixed point, \\(u\\) , and its iterates converge to \\(u\\) with rate \\(\\kappa(M)\\) . However, it should be noted that the linear convergence rate provided by the Birkhoff-Hopf theorem is not as tight as the classical ratio between the second and first eigenvalues of \\(M\\) . The Hilbert metric can be used to prove a generalization of the Perron-Frobenius theorem to eigenvectors of nonlinear mappings on cones. We discuss the class of such mappings in the next section.","title":"Hilbert projective distance and the Birkhoff-Hopf theorem"},{"location":"chapter1/#homogeneous_and_order-preserving_mappings","text":"To prove that nonnegative matrices are nonexpansive with respect to \\(d_H\\) in Equation \\(\\eqref{eq:nonexpansive}\\) , we have used two important properties of the nonnegative matrix \\(M\\) : First, we have used the fact that for every \\(x,y\\in C_+\\) satisfying \\(x\\preceq y\\) it holds \\(Mx\\preceq My\\) and second, we have used the homogeneity of \\(M\\) , i.e. if \\(x\\in \\RR^n\\) and \\(\\alpha\\geq 0\\) , then \\(M(\\alpha x)= \\alpha M(x)\\) . This motivates the following definition: Definition. Let \\(K\\subset C_+\\) be a convex cone, \\(f\\colon K \\to C_+\\) and \\(p\\in \\RR\\) . \\(f\\) is \\(\\textit{order-preserving}\\) if \\(f(x)\\preceq f(y)\\) for every \\(x,y\\in K\\) satisfying \\(x\\preceq y\\) . \\(f\\) is (positively) \\(\\textit{homogeneous of degree}\\) \\(p\\) , if for every \\(x\\in K\\) and every \\(\\alpha\\geq 0\\) , it holds \\(f(\\alpha\\,x)=\\alpha\\, f(x)\\) . If \\(f\\) is homogeneous of degree \\(1\\) , we simply say that \\(f\\) is homogeneous. If \\(f\\) is differentiable on an open set containing \\(C_+\\) , then Theorem 1.3.1 of [NB] implies that \\(f\\) is order-preserving if and only if \\(D f(x)\\in C_+\\) for all \\(x\\in C_+\\) . Similarly, the Euler theorem for homogeneous mappings shows that \\(f\\) is homogeneous of degree \\(p\\) if and only if \\(D f(x) x\\, = \\, p\\, f(x)\\) for all \\(x\\in C_+\\) . If \\(C_+\\subset \\RR^1\\) , then being order-preserving is precisely the same as being increasing. The concept reducing to that of deacreasing functions is termed order-reversing. A mapping \\(f\\colon K\\to C_+\\) is order-reversing, if \\(f(y)\\preceq f(x)\\) for every \\(x,y\\in K\\) satisfying \\(x\\preceq y\\) . We will however mainly discuss order-preserving functions. Example. Consider the mappings \\(f\\colon C_+ \\to C_+\\) , \\(g\\colon C_+ \\to C_+\\) and \\(h\\colon C_+ \\to C_+\\) , respectively defined as \\(f\\) o.p. \\(g\\) order reversing \\(h\\) o.p. A similar argument as in Equation \\(\\eqref{eq:nonexpansive}\\) yields the following: Lemma. Let \\(K\\in\\{C_+,C_{++}\\}\\) and let \\(f\\colon K \\to C_+\\) be homogeneous of degree \\(p\\in \\RR\\) . Suppose that \\(f\\) is either order-preserving or order-reversing, then \\[ d_H(f(x),f(y))\\leq |p|\\,d_H(x,y)\\qquad \\forall x,y\\in C_{++}.\\] The classical Perron-Frobenius theorem is concerned with eigenvalues and eigenvectors of linear mappings and their spectral radius. The notions of eigenvalues and eigenvectors for a general mapping \\(f\\colon K\\to C_+\\) with \\(K\\subset C_+\\) can be extended as follows: We say that \\(x\\in C_+\\) is an eigenvector of if there exists \\(\\lambda\\geq 0\\) such that \\(f(x)=\\lambda x\\) , \\(\\lambda\\) is called an eigenvalue. However, the notion of spectral radius is more delicate to generalize. Indeed, without further assumptions on \\(f\\) , the spectrum of \\(f\\) (the set of its eigenvalues) can be empty or unbounded. We consider the following definition: Definition. Let \\(f\\colon C_+ \\to C_+\\) be order-preserving and homogeneous. The spectral radius of \\(f\\) is defined by \\[\\rho(f) = \\lim_{k\\to \\infty} \\|f^k\\|_{C_+}^{1/k},\\] where \\(f^k=f\\circ \\ldots \\circ f\\) is the \\(k\\) -th composition of \\(f\\) with itself and \\(\\|f\\|_{C_+}=\\max\\{\\|f(x)\\| \\colon x\\in C_+, \\|x\\|\\leq 1\\}\\) is the operator norm of \\(f\\) with respect to any norm \\(\\|\\cdot \\|\\) on \\(\\RR^n\\) . The above definition of \\(\\rho(f)\\) is usually referred to as the Bonsall spectral radius of \\(f\\) . Another definition of spectral radii have been proposed, namely the cone spectral radius of \\(f\\) . Nevertheless, it is shown in [specrad] that both definitions are equivalent for homogeneous order-preserving mappings on \\(C_+\\) . Let us further observe that for mappings \\(f\\colon C_+\\to C_+\\) which are homogeneous of degree \\(p\\neq 1\\) , the concept of eigenvalue is less clear as they depend on the scaling of the eigenvector. However, as discussed in the next section, if \\(f\\colon C_{++}\\to C_{++}\\) is order-preserving and homogeneous of degree \\(p<1\\) , then \\(f\\) always have a unique eigenvector.","title":"Homogeneous and order-preserving mappings"},{"location":"chapter1/#the_perron-frobenius_theorem_and_the_collatz-wielandt_formula","text":"Because we do not assume linearity, the assumptions of the Perron-Frobenius for homogeneous mappings are more delicate. To highlight their connection with the linear case, we first recall the results of the Perron-Frobenius theorem. A first remarkable consequence of the above ovservations is the following version of the Perron-Frobenius theorem concerning mappings which are homogeneous of degree \\(p\\) with \\(|p|<1\\) . Lemma. Let \\(K\\in\\{C_+,C_{++}\\}\\) and let \\(f\\colon K \\to C_+\\) be homogeneous of degree \\(p\\in \\RR\\) . Suppose that \\(f\\) is either order-preserving or order-reversing, then \\[ d_H(f(x),f(y))\\leq |p|\\,d_H(x,y)\\qquad \\forall x,y\\in C_{++}.\\] In the general case of order-preserving, homogeneous mappings the assumptions of the Perron-Frobenius are more technical. In particular,","title":"The Perron-Frobenius theorem and the Collatz-Wielandt formula"},{"location":"chapter1/#application_computing_the_qto_p_norm_of_a_nonnegative_matrix","text":"","title":"Application: Computing the \\(q\\to p\\) norm of a nonnegative matrix"},{"location":"chapter1/#application_the_sinkhorn_method","text":"","title":"Application: The Sinkhorn method"},{"location":"chapter1/#perron-frobenius_theory_for_linear_mappings","text":"","title":"Perron-Frobenius theory for linear mappings"},{"location":"chapter1/#brief_history","text":"In 1907, in [\u2026], Oskar Perron proves that the spectral radius of primitive matrices is an algebraically simple eigenvalue and the corresponding eigenvector can be scaled to have all entries positive. In 1908, in [\u2026], George Frobenius extends the result by showing that the spectral radius of irreducible matrices is a geometrically simple eigenvalue and the corresponding eigenvector can be scaled to have positive entries. Furthermore, Perron observes that when the spectral radius is an algebraically simple eigenvalue, one can use the power method introduced by \u2026 in \u2026 to compute a maximal eigenvector, i.e. an eigenvector corresponding with the spectral radius as eigenvalue. These results have been then complemented by a broad diversity of results about nonnegative matrix and we refer to [Plemmons] for further readings on the topic. In particular, we will discuss the Collatz-Wielandt ratio, which provides a sup-min and an inf-max characterization of the spectral radius.","title":"Brief history"},{"location":"chapter1/#to_be_killed","text":"We briefly recall here well-known results and concepts of linear algebra which will are useful to understand the subsequent discussion. Given a matrix \\(A\\in\\RR^{n\\times n}\\) , \\(\\lambda\\in \\RR\\) is an eigenvalue of \\(A\\) if there exists \\(x\\in \\RR^n\\) with \\(x\\neq 0\\) and \\(Ax = \\lambda x\\) . A vector \\(x\\) is called an eigenvector of \\(A\\) if \\(x\\neq 0\\) and there exists a corresponding eigenvalue \\(\\lambda\\in \\RR\\) such that \\(Ax=x\\) . An eigenvalue \\(\\lambda\\) of \\(A\\) is algebraically simple if it is a simple root of the polynomial \\(p(t)=\\det(A-tI)\\) , where \\(I\\in\\RR^{n\\times n}\\) is the identity matrix. An eigenvalue \\(\\lambda\\) of \\(A\\) is geometrically simple if \\(\\dim(\\ker(A-\\lambda I))=1\\) , i.e. there is a unique up to scale eigenvector corresponding to \\(\\lambda\\) . Algebraic simplicity implies geometric simplicity but the converse is not true in general. The spectral radius of \\(A\\in\\RR^{n\\times n}\\) is denoted by \\(\\rho(A)\\) and defined as \\[\\rho(A)=\\max\\{|\\lambda| \\colon \\lambda \\text{ is an eigenvalue of }A\\}\\] For \\(p\\in[1,\\infty)\\) , let \\(\\|x\\|_p = \\big(\\sum_{i=1}^n|x_i|^p\\big)^{1/p}\\) denote the usual \\(p\\) -norm for every \\(x\\in\\RR^n\\) .","title":"To be killed"},{"location":"chapter1/#the_cone_of_nonnegative_vectors","text":"The Perron-Frobenius theory deals with operators that leave a (\u00bfsolid?) normal, convex and pointed cone invariant. The cone of nonnegative vectors is the most common example of such a cone and the one that most often arises in applications. We denote such cone as \\(C_+ \\subseteq \\mathbb R^n\\) or, when the dimension is clear from the context, simply as \\(C_+.\\) The elements of \\(C_+\\) are vectors whose components are all nonnegative. Whereas the interior \\(C_{++}\\) of \\(C_+\\) is the set of entrywise positive vectors: \\(C_+ = \\{x\\in \\mathbb R^n : x_i\\geq 0\\) for all \\(i=1,\\dots,n\\}=\\{x\\in \\mathbb R^n : x\\succeq 0\\}\\) \\(C_{++} = \\{x\\in \\mathbb R^n : x_i> 0\\) for all \\(i=1,\\dots,n\\}=\\{x\\in \\mathbb R^n : x\\succ 0\\}.\\) The cone \\(C_+\\) induces a partial ordering on \\(\\RR^n\\) defined as \\(x\\preceq y\\) if \\(y-x\\in C_+\\) . In particular, as \\(C_+\\) denotes the cone of nonnegative vectors in \\(\\RR^n\\) , we have that \\(x\\preceq y\\) if and only if \\(x_i \\leq y_i\\) for all \\(i=1,\\ldots,n\\) . This partial order induce a equivalence relation \\(\\sim\\) on \\(C_+\\) defined as \\(x\\sim y\\) if there exists \\(\\alpha,\\beta>0\\) such that \\(\\alpha x \\preceq y \\preceq \\beta x\\) . The equivalence classes are called the parts of the cone \\(C_+\\) . For example, \\(C_+\\subset\\RR^2\\) has four parts given \\(\\{0\\}, \\{(s,0)\\colon s>0\\}, \\{(0,t)\\colon t>0\\}, C_{++}\\) . \\(C_+\\) is convex, pointed and normal as \\(\\alpha x\\in C_+\\) for all \\(x\\in C_+\\) and all scalar coefficients \\(\\alpha \\geq 0\\) , \\(C_+\\cap -C_+ = \\{0\\}\\) and for every norm \\(\\|\\cdot\\|\\) on \\(\\RR^n\\) , there exists \\(\\gamma>0\\) such that for all pair of vectors \\(x,y\\in C_+\\) such that \\(x\\preceq y\\) it holds \\(\\|x\\|\\leq \\gamma \\|y\\|\\) . 1 This tutorial will focus only on \\(C_+\\) , however we point out that all the results we will present can be relatively directly transferred to arbitrary normal, convex and pointed cones.","title":"The cone of nonnegative vectors"},{"location":"chapter1/#nonnegative_irreducible_primitive_and_positive_matrices","text":"The classical Perron-Frobenius theory is mainly concerned with matrices having nonnegative entries. Based on the pattern of their positive entries, multiple results can be derived concerning the simplicity of their eigenvalues of maximal magnitude and the corresponding eigenvectors. To characterize different patterns of positive entries, we recall the definition of primitive and irreducible matrices. We aslo recall characterizations of these definitions as they will be generalized in different ways in the nonlinear setting: Definition. Let \\(A\\in\\RR^{n\\times n}\\) be a matrix. \\(A\\) is nonnegative if \\(A_{ij}\\geq 0\\) for all \\(i,j=1,\\ldots,n\\) . \\(A\\) is positive if \\(A_{ij}>0\\) for all \\(i,j=1,\\ldots,n\\) . \\(A\\) is irreducible if \\(A\\) is nonnegative and there exists an integer \\(m\\) such that \\((I+A)^m\\) is positive. \\(A\\) is primitive if \\(A\\) is nonnegative and there exists an integer \\(m\\) such that \\(A^m\\) is positive. A matrix \\(A\\) is positive, if and only if \\(A^{\\top}\\) is positive. This observation holds for all four definitions above, i.e. \\(A\\) is nonnegative, irreducible, primitive, respectively, if and only if $A^{\\top} has the corresponding property. We illustrate these definitions with an example. Example. Consider the matrices in \\(\\RR^{2\\times 2}\\) defined as \\[\\begin{equation}\\label{def:Example_ABCD} A=\\bigg(\\begin{matrix}0 & 0 \\\\ 1 & 1\\end{matrix}\\bigg), \\qquad B=\\bigg(\\begin{matrix}0 & 1 \\\\ 1 & 0\\end{matrix}\\bigg), \\qquad C=\\bigg(\\begin{matrix}0 & 1 \\\\ 1 & 1\\end{matrix}\\bigg),\\qquad D=\\bigg(\\begin{matrix}1 & 2 \\\\ 1 & 1\\end{matrix}\\bigg) \\end{equation}\\] Then: \\(A,B,C,D\\) are nonnegative; \\(B,C,D\\) are irreducible; \\(C,D\\) are primitive; \\(D\\) is positive; \\(A,B,C\\) are not positive; \\(A,B\\) are not primitive; \\(A\\) is not irreducible. These defintions can be interpret in the context of graphs. Indeed, one can associated a directed graph \\(G(A)=(V,E)\\) to a nonnegative matrix \\(A\\in \\RR^{n\\times n}\\) as follows: The vertexes are \\(V=\\{1,\\ldots,n\\}\\) and there is an edge from \\(i\\) to \\(j\\) , i.e. \\((i,j)\\in E\\) , if \\(A_{ij}>0\\) . A directed path from \\(i\\in V\\) to \\(j\\in V\\) , is a sequence of \\(\\ell\\) edges \\((k_1,k_2),(k_2,k_3),\\ldots,(k_{\\ell},k_{\\ell+1})\\in E\\) such that \\(k_1=1\\) and \\(k_{\\ell+1}= j\\) . The length of a directed path is the number of edges it traverses, namely \\(\\ell\\) . The above definitions can now be equivalently formulated as follows: Proposition. Let \\(A\\in\\RR^{n\\times n}\\) be a nonnegative matrix and \\(G(A)=(V,E)\\) be its associated graph. \\(A\\) is positive if, and only if, \\(G(A)\\) is complete, i.e. \\(E=\\{(i,j)\\mid \\forall i,j\\in V\\}\\) . \\(A\\) is irreducible if, and only if, \\(G(A)\\) is connected, i.e. for all \\(i,j\\in V\\) there exists a directed path from \\(i\\) to \\(j\\) (see Theorem \u2026). \\(A\\) is primitive if, and only if, \\(G(A)\\) is connected and the greatest common divisor of the lengths of all the directed paths starting and ending at the same node equals \\(1\\) (see Theorem 8.5.3 [Horn]). Note that the above characterizations highlight the fact that positivivity, irreducibility and primtivity are concepts related to the pattern of the positive entries rather than the magnitude of these entries. Another way to characterize nonnegative, positive, irreducible and primitive matrices, is to analyze their image on vectors with nonnegative entries. Note for instance that a matrix \\(A\\in\\RR^{n\\times n}\\) is nonnegative if, and only if, \\(Ax\\in C_+\\) for all \\(x\\in C_+\\) . Proposition. Let \\(A\\in\\RR^{n\\times n}\\) be a nonnegative matrix. \\(A\\) is positive if, and only if, \\(Ax\\in C_{++}\\) for all \\(x\\in\\RR^n_+\\setminus\\{0\\}\\) . \\(A\\) is irreducible if, and only if, there exists an integer \\(m\\) such that \\(\\sum_{k= 0}^m A^{k}x\\in C_{++}\\) for all \\(x\\in\\RR^n_+\\setminus\\{0\\}\\) (see Theorem \u2026). \\(A\\) is primitive if, and only if, there exists an integer \\(m\\) such that \\(A^{m}x\\in C_{++}\\) for all \\(x\\in C_+\\setminus\\{0\\}\\) (see Theorem \u2026). From the above characterization and the matrices \\(A,B,C,D\\) of the Example, one can deduce the following inclusions: Lemma. Let \\(n>1\\) be an integer and consider the following sets of matrices: \\[\\begin{equation*} \\begin{array}{l c l} M_{\\text{nneg}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is nonnegative}\\}, && M_{\\text{irr}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is irreducible}\\},\\\\ M_{\\text{prim}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is primitive}\\}, && M_{\\text{pos}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is positive}\\}.\\end{array} \\end{equation*}\\] Then, it hold \\(M_{\\text{nneg}} \\subsetneq M_{\\text{irr}} \\subsetneq M_{\\text{prim}} \\subsetneq M_{\\text{pos}}\\) . We refer to [\u2026] for an extended discussion on nonnegative, irreducible, primitive and positive matrices.","title":"Nonnegative, irreducible, primitive and positive matrices"},{"location":"chapter1/#the_perron-frobenius_theorem_and_the_collatz-wielandt_formula_1","text":"We can now state the celebrated theorem of Perron and Frobenius which provides conditions for the existence of a nonnegative eigenvector, i.e. an eigenvector in \\(C_+\\) , the existence and uniqueness of a positive eigenvector, i.e. an eigenvector in \\(C_{++}\\) , and the convergence of a sequence towards a unique positive eigenvector. Note that, by unique eigenvector, we mean unique up to scale so that \\(u\\) and \\(\\alpha u\\) are considered to be the same eigenvector. Theorem (Perron-Frobenius). Let \\(A\\in\\RR^{n\\times n}\\) be a matrix and \\(\\|\\cdot\\|\\) a norm on \\(\\RR^n\\) . If \\(A\\) is nonnegative, then there exists an eigenvector \\(u\\in C_+\\) such that \\(Au=\\rho(A)u\\) . If \\(A\\) is irreducible, then \\(u\\) is the unique up to scale eigenvector of \\(A\\) in \\(\\RR^n_+\\) . Furthermore \\(u\\in C_{++}\\) and \\(\\rho(A)\\) is a geometrically simple eigenvalue. If \\(A\\) is primitive, then \\(\\rho(A)\\) is algebraically simple and for every \\(x^0\\in C_+\\setminus\\{0\\}\\) , the sequence \\((x^k)_{k=0}^{\\infty}\\subset\\RR^n\\) defined as \\(x^k = A x^{k-1}\\) for all \\(k\\geq 1\\) satisfies \\[\\lim_{k\\to \\infty} \\frac{x^k}{\\|x^k\\|}= \\frac{u}{\\|u\\|}.\\] The matrices of the previous example shows that the assumptions in the Perron-Frobenius theorem are well calibrated. Remark. Consider the matrices \\(A,B,C,D\\) defined in \\(\\eqref{def:Example_ABCD}\\) . Then \\(B,C,D\\) have \\((1,1)^\\top\\) as positive eigenvector. The matrix \\(A\\) , is an example of nonnegative matrix which is not irreducible, and have no positive eigenvector. The matrix \\(B\\) , is an example of irreducible matrix which is not primitive, and for which the sequence \\((x^k)_{k=0}^{\\infty}\\) defined in the Perron-Frobenius theorem do not converge whenever \\(x^{0}\\in\\RR^2\\) satisfy \\(x^{0}_1\\neq x^{0}_2\\) . We mention one additional result from Collatz and Wielandt which provides a sup-min and an inf-max characterization of the spectral radius. The expressions in the Collatz-Wielandt formula is closely related to the Hilbert metric which is a key ingredient to generalize the Perron-Frobenius theorem. As illustrated by the applications below, the Collatz-Wielandt formula is useful to prove the maximality of an eigenvalue. Theorem (Collatz-Wielandt). Let \\(A\\in\\RR^{n\\times n}\\) be a nonnegative matrix. Then, it holds \\[\\begin{equation}\\label{def:cw_up} \\rho(A)\\quad =\\quad \\inf_{x\\in\\RR_{++}^n}\\quad \\max_{i=1,\\ldots,n}\\quad \\frac{(Ax)_i}{x_i}. \\end{equation}\\] If additionally, \\(A\\) has an eigenvector \\(u\\in\\RR^n_{++}\\) , then it holds \\[\\begin{equation}\\label{def:cw_down} \\rho(A)\\quad = \\quad\\sup_{\\substack{x\\in\\RR_{+}^n\\\\ x\\neq 0}} \\quad\\min_{\\substack{i=1,\\ldots,n\\\\ x_i >0}}\\quad \\frac{(Ax)_i}{x_i}. \\end{equation}\\] and both, the infimum and the supremum above, are attained at \\(u\\) . The Collatz-Wielandt formula is useful to discuss the maximality of eigenvalues, i.e. the egivenvalue of any positive eigenvector equals the spectral radius.","title":"The Perron-Frobenius theorem and the Collatz-Wielandt formula"},{"location":"chapter1/#application_stochastic_matrices_and_homogeneous_markov_chains_on_n","text":"Let \\(S\\in\\RR^{n\\times n}\\) be a stochastic matrix, i.e. \\(S\\) is nonnegative and \\(S\\mathbf{1}=\\mathbf{1}\\) with \\(\\mathbf{1}=(1,\\ldots,1)^\\top \\in\\RR^n\\) . Let us connsider the set of probability vectors \\(\\Delta_+=\\{x\\in\\RR^n_+\\colon x_1+\\ldots+x_n=1\\}\\) . Note that \\(S\\) is a nonnegative matrix and \\(u=\\frac{1}{n}\\mathbf{1}\\) is a positive eigenvector of \\(S\\) corresponding to the eigenvalue \\(\\lambda=1\\) . We show the existence and uniqueness of a startionary distribution. First, note that by the Collatz-Wielandt formula, \\[\\rho(S) =\\inf_{x\\in\\RR_{++}^n} \\max_{i=1,\\ldots,n} \\frac{(Sx)_i}{x_i} = \\max_{i=1,\\ldots,n} \\frac{(Su)_i}{u_i} = 1. \\] As \\(S^\\top\\) is a nonnegative matrix, by the Perron-Frobenius theorem, we know that there exists \\(\\pi\\in \\RR^n_+\\) with \\(\\pi_1+\\ldots+\\pi_n=1\\) and \\(S^\\top \\pi = \\rho(S^\\top) \\pi\\) . As \\(\\rho(S^\\top)=\\rho(S)=1\\) , we have \\(S^\\top \\pi =\\pi\\) , i.e. \\(\\pi\\) is a stationary distribution of \\(S\\) . This proves the existence. If \\(S\\) is irreducible, then \\(S^T\\) is irreducible as well and by the Perron-Frobenius theorem, \\(\\pi\\) is the unique stationary distribution of \\(S\\) and \\(\\pi_1,\\ldots,\\pi_n>0\\) . Consider a sequence of random variables \\(X_0,X_1,\\ldots\\) with values in \\([n]=\\{1,\\ldots,n\\}\\) . Suppose that \\[\\operatorname{Pr}(X_k=j_k\\mid X_{k-1}=j_{k-1},\\ldots X_0=j_0)=\\operatorname{Pr}(X_1=j_k\\mid X_0=j_0)= S_{j_kj_0}\\] for all \\(k=1,2,\\ldots\\) \\(X_0,X_1,\\ldots\\) is a homogeneous Markov chain. Note that if \\(X_0\\) has distribution \\(x^0\\in\\Delta_+\\) , i.e. \\(\\operatorname{Pr}(X_0 = j)=x^0_j\\) for all \\(j\\in [n]\\) , then the distribution \\(x^k\\in\\Delta_+\\) of \\(X_k\\) satisfies \\(x^k=(S^\\top)^k x^{0}\\) for all \\(k\\geq 0\\) . So, if \\(S\\) is primitive, then so is \\(S^T\\) . Let \\(\\|\\cdot\\|_1\\) be the \\(1\\) -norm, i.e. \\(\\|x\\|_1=|x_1|+\\ldots+|x_n|\\) . Note that as \\(x^k\\in \\Delta_+\\) , it holds \\(\\|x^k\\|_1=1\\) for all \\(k\\geq 0\\) . Suppose, that \\(S\\) is primitive, then the Perron-Frobenius theorem, with \\(\\|\\cdot\\|=\\|\\cdot\\|_1\\) , implies that for every initial distribution \\(x^0\\in \\Delta_+\\) of \\(X_0\\) , it holds \\(\\lim_{k\\to \\infty}x^k = \\pi\\) .","title":"Application: Stochastic matrices and homogeneous Markov chains on \\([n]\\)"},{"location":"chapter1/#application_spectral_norm_of_a_nonnegative_matrix","text":"Let \\(\\|\\cdot\\|_2\\) be the \\(2\\) -norm on \\(\\RR^n\\) , the induced matrix norm \\(\\|\\cdot\\|_{2,2}\\) on \\(\\RR^{n\\times n}\\) is the spectral norm defined as \\[ \\|A\\|_{2,2}=\\max_{x\\in\\RR^n}\\{\\|Ax\\|_2 \\colon \\|x\\|_2\\leq 1\\}=\\max_{x\\neq 0}\\frac{\\| Ax \\|_2}{\\| x \\|_2}. \\] The function \\(x\\mapsto \\|Ax\\|_2\\) is continuous on the compact set \\(\\{x\\colon \\|x\\|_2\\leq 1\\}\\) and therefore attains a maximum \\(x^*\\in\\RR^n, x^*\\neq 0\\) . Now, note that since \\(A\\) is nonnegative, we have \\(|Ax| \\leq A|x|\\) where the absolute value and the inequality are understood component-wise. In particular, this implies that \\[ \\|A\\|_{2,2}=\\frac{\\| Ax^* \\|_2}{\\| x^* \\|_2}=\\frac{\\| \\,|Ax^*|\\, \\|_2}{\\|\\,| x^*|\\, \\|_2}\\leq \\frac{\\| \\,|Ax^*|\\, \\|_2}{\\|\\,| x^*|\\, \\|_2}, \\] and therefore we may assume without loss of generality that \\(x^*\\in\\RR^n_+\\) . Now, note that if \\(v\\in\\RR^n_+\\) is a critical point of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) , the gradient of the later function vanishes at \\(x\\) and therefore it satisfies \\[A^TAv = \\lambda v \\qquad \\text{with} \\qquad \\lambda =\\Big(\\frac{\\| Av \\|_2}{\\| v \\|_2}\\Big)^2=\\|A\\|_{2,2}^2, \\] i.e. \\(v\\) is an eigenvector of \\(A^TA\\) . The converse of this statement holds true as well, that is, if \\(v\\) is an eigenvector of \\(A^TA\\) with eigenvalue \\(\\lambda\\) , then \\(v\\) is a critical point of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) and \\(\\lambda = (\\|Ax\\|_2/\\|x\\|_2)^2\\) . Therefore, the eigenvalues of \\(A^TA\\) and the square of the critical values of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) are equal. In particular, this implies that \\(\\rho(A^TA) =\\|A\\|_{2,2}^2\\) . Now, \\(A^TA\\) is a nonnegative matrix and if it is irreducible, by the Perron-Frobenius theorem, \\(A^TA\\) has a unique positive eigenvector \\(u\\in \\RR^n_{+}\\) such that \\(\\|u\\|_2=1\\) . Furthermore, \\(u\\in \\RR^n_{++}\\) and the corresponding eigenvalue is \\(\\rho(A^TA)\\) . From the above discussion: \\(x^*\\) is a global maximizer of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) and we may assume \\(x^*\\in\\RR^n_+\\) . Hence, \\(x^*\\) is a nonnegative eigenvector of \\(A^TA\\) and since \\(u\\) is the unique eigenvector of \\(A^TA\\) , we have \\(x^*/\\|x^*\\|_2 = u\\) . Finally, if \\(A^TA\\) is primitive, then for every \\(x^0\\in\\RR^n_+\\setminus \\{0\\}\\) , the sequence \\(x^k=A^TAx^{k-1}\\) satsifies \\(\\lim_{k\\to\\infty} x^k/\\|x^k\\| = x^*/\\|x^*\\|_2\\) , i.e. this sequence converges to a global maximizer of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) . We conclude by noting that, while the irreducibility of \\(A\\) implies the irreducibility of \\(A^TA\\) , the converse is not true in general. Indeed, the matrix \\(A\\) of \\(\\eqref{def:Example_ABCD}\\) is not irreducible, but \\(A^TA\\) is a positive matrix and is therefore irreducible and primitive.","title":"Application: Spectral norm of a nonnegative matrix"},{"location":"chapter1/#the_perron-frobenius_theorem_for_multi-homogeneous_mappings","text":"","title":"The Perron-Frobenius theorem for multi-homogeneous mappings"},{"location":"chapter1/#nonnegative_tensors_and_their_norm","text":"","title":"Nonnegative tensors and their norm"},{"location":"chapter1/#product_of_cones_and_multi-homogeneous_mappings","text":"","title":"Product of cones and multi-homogeneous mappings"},{"location":"chapter1/#eigenvalues_and_eigenvectors_of_multi-homogeneous_mappings","text":"","title":"Eigenvalues and eigenvectors of multi-homogeneous mappings"},{"location":"chapter1/#the_perron-frobenius_theorem_and_the_collatz-wielandt_formula_2","text":"","title":"The Perron-Frobenius theorem and the Collatz-Wielandt formula"},{"location":"chapter1/#application_computing_the_norm_of_a_nonnegative_tensor","text":"","title":"Application: Computing the norm of a nonnegative tensor"},{"location":"chapter1/#application","text":"","title":"Application: ..."},{"location":"chapter1/#the_cone_of_nonnegative_vectors_1","text":"The Perron-Frobenius theory deals with operators that leave a normal, convex and pointed cone invariant. The cone of nonnegative vectors is the most common example of such a cone and the one that most often arises in applications. We denote such cone as \\(C_+ \\subseteq \\mathbb R^n\\) or, when the dimension is clear from the context, simply as \\(C_+.\\) The elements of \\(C_+\\) are vectors whose components are all nonnegative. Whereas the interior \\(C_{++}\\) of \\(C_+\\) is the set of entrywise positive vectors: \\(C_+ = \\{x\\in \\mathbb R^n : x_i\\geq 0\\) for all \\(i=1,\\dots,n\\}=\\{x\\in \\mathbb R^n : x\\succeq 0\\}\\) \\(C_{++} = \\{x\\in \\mathbb R^n : x_i> 0\\) for all \\(i=1,\\dots,n\\}=\\{x\\in \\mathbb R^n : x\\succ 0\\}.\\) \\(C_+\\) is convex, pointed and normal as \\(\\alpha x\\in C_+\\) for all \\(x\\in C_+\\) and all scalar coefficients \\(\\alpha \\geq 0\\) , \\(C_+\\cap -C_+ = \\{0\\}\\) and for all pair of vectors \\(x,y\\in C_+\\) such that \\(x\\preceq y\\) there exists a \\(\\gamma>0\\) such that \\(\\|x\\|\\leq \\gamma \\|y\\|\\) . Here \\(\\|\\cdot\\|\\) is any norm on \\(\\mathbb R^n\\) . This tutorial will focus only on \\(C_+\\) however, however we point out that all the results we will present can be relatively directly transferred to arbitrary normal, convex and pointed cones.","title":"The cone of nonnegative vectors"},{"location":"chapter1/#hilbert_projective_distance","text":"One of the key tools for the Perron-Frobenius theory for nonlinear mappings is the Hilbert distance. For any two positive vectors \\(x,y\\succ 0\\) , this is defined as: 1 \\[d_H(x,y) = \\ln\\Big(\\max_{i=1,\\dots,n}\\frac{x_i}{y_i}\\max_{i=1,\\dots,n}\\frac{y_i}{x_i}\\Big)\\] \\(d_H\\) is a metric on the space of rays in \\(C_{++}\\) , that is it holds 2 \\(d_H(\\alpha x, \\beta y) =d_H(x,y)\\) for all scalar coefficients \\(\\alpha,\\beta >0\\) ds Lorem markdownum fuit peragit eunti, cum illuc heu ferrumque gentis. Curvarique loca remoraminaque terram iniustaque tempora luctusque vitam; tela iunxit gratia fontibus ut tempus cupiasque relevasse altis. Mutantur carpere geratis Scorpius. Visa ego delendaque, victae esse pervenit et arentia gentem. 1 2 3 4 5 6 7 8 9 10 if (5 + net) { alignment = jumperPpi; ppl_server = multiplatform(2, -1) + switchCable; } var integerCarrier = uml_p_tween; metafileCdPram -= scareware.barMultiprocessingIo.box(fiber, plugSyn.integrated_add(gif, camera, export_vga_multithreading)); if (processorSdk) { taskCcd = client_clipboard_syntax; } Prope sine coepta: reddi tabo virens expositum amaris saxo medio fons esse Nisi. Dixit dolorem simulamina motus. Gradibus tuta quae addere vero iuvenis candida; ictu Phocaico, tempore addicere pluma terra?","title":"Hilbert projective distance"},{"location":"chapter1/#est_deus_pericula_exigere_figit","text":"Iani rumpere raptos o columbae non et leonis ferre qui quinque arva idem dixit haec; advertite candore. Mora invitusque casses, totidemque et erat in nonne novat iamque: quae? Ab nec fecit et idonea nunc lumine Spartana nunc vertice brevis pendebant levius contraria aere? Iram orba ferarum dei eburnea; voce praestantes, me squamas victor contigit pia! 1 2 3 pitch_ecc.serp_clip = -5 + -1; box.tape_digital(subnet_only); esports_query_multiprocessing.fontPiconetServices -= document_laptop; Ipse nomen, certum ego; heros nam, donec et semper parenti formatae si iuvenum. Insidias dextera anumque est habitandae nubes. Quo Latina, quidem: positisque, inter regia puer iurgia, illa illos ille limine flammifera dracones et. Retenta Metione atque iam quoque Iunone pabula. Canna ora maxima non penates cautum cruorem quid quid falsi. Ara plus templa, nymphe innato parum vestrae , tunc caput instat quo. Tum moras si Euagrus freta ferrove urbs nec etiam ipsis. Nunc quod aut intrarunt et graves urbes, telluris carmen coget corpus. This definition is strictly related to the choice of the nonnegative cone \\(C_+\\) . For general cones, the definition of \\(d_H\\) changes. However, all its properties do not change. \u21a9 \u21a9 Lemmens, Nussbaum, Citation test \u21a9","title":"Est deus pericula exigere figit"},{"location":"sec3/","text":"Multihonogeneous mappings","title":"Multihonogeneous mappings"},{"location":"sec3/#multihonogeneous_mappings","text":"","title":"Multihonogeneous mappings"},{"location":"ch1/sec1/","text":"Linear mappings Brief history In 1907, in [\u2026], Oskar Perron proves that the spectral radius of primitive matrices is an algebraically simple eigenvalue and the corresponding eigenvector can be scaled to have all entries positive. In 1908, in [\u2026], George Frobenius extends the result by showing that the spectral radius of irreducible matrices is a geometrically simple eigenvalue and the corresponding eigenvector can be scaled to have positive entries. Furthermore, Perron observes that when the spectral radius is an algebraically simple eigenvalue, one can use the power method introduced by \u2026 in \u2026 to compute a maximal eigenvector, i.e. an eigenvector corresponding with the spectral radius as eigenvalue. These results have been then complemented by a broad diversity of results about nonnegative matrix and we refer to [Plemmons] for further readings on the topic. In particular, we will discuss the Collatz-Wielandt ratio, which provides a sup-min and an inf-max characterization of the spectral radius. To be killed We briefly recall here well-known results and concepts of linear algebra which will are useful to understand the subsequent discussion. Given a matrix \\(A\\in\\RR^{n\\times n}\\) , \\(\\lambda\\in \\RR\\) is an eigenvalue of \\(A\\) if there exists \\(x\\in \\RR^n\\) with \\(x\\neq 0\\) and \\(Ax = \\lambda x\\) . A vector \\(x\\) is called an eigenvector of \\(A\\) if \\(x\\neq 0\\) and there exists a corresponding eigenvalue \\(\\lambda\\in \\RR\\) such that \\(Ax=x\\) . An eigenvalue \\(\\lambda\\) of \\(A\\) is algebraically simple if it is a simple root of the polynomial \\(p(t)=\\det(A-tI)\\) , where \\(I\\in\\RR^{n\\times n}\\) is the identity matrix. An eigenvalue \\(\\lambda\\) of \\(A\\) is geometrically simple if \\(\\dim(\\ker(A-\\lambda I))=1\\) , i.e. there is a unique up to scale eigenvector corresponding to \\(\\lambda\\) . Algebraic simplicity implies geometric simplicity but the converse is not true in general. The spectral radius of \\(A\\in\\RR^{n\\times n}\\) is denoted by \\(\\rho(A)\\) and defined as \\[\\rho(A)=\\max\\{|\\lambda| \\colon \\lambda \\text{ is an eigenvalue of }A\\}\\] For \\(p\\in[1,\\infty)\\) , let \\(\\|x\\|_p = \\big(\\sum_{i=1}^n|x_i|^p\\big)^{1/p}\\) denote the usual \\(p\\) -norm for every \\(x\\in\\RR^n\\) . The cone of nonnegative vectors The Perron-Frobenius theory deals with operators that leave a (\u00bfsolid?) normal, convex and pointed cone invariant. The cone of nonnegative vectors is the most common example of such a cone and the one that most often arises in applications. We denote such cone as \\(C_+ \\subseteq \\mathbb R^n\\) or, when the dimension is clear from the context, simply as \\(C_+.\\) The elements of \\(C_+\\) are vectors whose components are all nonnegative. Whereas the interior \\(C_{++}\\) of \\(C_+\\) is the set of entrywise positive vectors: \\(C_+ = \\{x\\in \\mathbb R^n : x_i\\geq 0\\) for all \\(i=1,\\dots,n\\}=\\{x\\in \\mathbb R^n : x\\succeq 0\\}\\) \\(C_{++} = \\{x\\in \\mathbb R^n : x_i> 0\\) for all \\(i=1,\\dots,n\\}=\\{x\\in \\mathbb R^n : x\\succ 0\\}.\\) The cone \\(C_+\\) induces a partial ordering on \\(\\RR^n\\) defined as \\(x\\preceq y\\) if \\(y-x\\in C_+\\) . In particular, as \\(C_+\\) denotes the cone of nonnegative vectors in \\(\\RR^n\\) , we have that \\(x\\preceq y\\) if and only if \\(x_i \\leq y_i\\) for all \\(i=1,\\ldots,n\\) . This partial order induce a equivalence relation \\(\\sim\\) on \\(C_+\\) defined as \\(x\\sim y\\) if there exists \\(\\alpha,\\beta>0\\) such that \\(\\alpha x \\preceq y \\preceq \\beta x\\) . The equivalence classes are called the parts of the cone \\(C_+\\) . For example, \\(C_+\\subset\\RR^2\\) has four parts given \\(\\{0\\}, \\{(s,0)\\colon s>0\\}, \\{(0,t)\\colon t>0\\}, C_{++}\\) . \\(C_+\\) is convex, pointed and normal as \\(\\alpha x\\in C_+\\) for all \\(x\\in C_+\\) and all scalar coefficients \\(\\alpha \\geq 0\\) , \\(C_+\\cap -C_+ = \\{0\\}\\) and for every norm \\(\\|\\cdot\\|\\) on \\(\\RR^n\\) , there exists \\(\\gamma>0\\) such that for all pair of vectors \\(x,y\\in C_+\\) such that \\(x\\preceq y\\) it holds \\(\\|x\\|\\leq \\gamma \\|y\\|\\) . 1 This tutorial will focus only on \\(C_+\\) , however we point out that all the results we will present can be relatively directly transferred to arbitrary normal, convex and pointed cones. Nonnegative, irreducible, primitive and positive matrices The classical Perron-Frobenius theory is mainly concerned with matrices having nonnegative entries. Based on the pattern of their positive entries, multiple results can be derived concerning the simplicity of their eigenvalues of maximal magnitude and the corresponding eigenvectors. To characterize different patterns of positive entries, we recall the definition of primitive and irreducible matrices. We aslo recall characterizations of these definitions as they will be generalized in different ways in the nonlinear setting: Definition. Let \\(A\\in\\RR^{n\\times n}\\) be a matrix. \\(A\\) is nonnegative if \\(A_{ij}\\geq 0\\) for all \\(i,j=1,\\ldots,n\\) . \\(A\\) is positive if \\(A_{ij}>0\\) for all \\(i,j=1,\\ldots,n\\) . \\(A\\) is irreducible if \\(A\\) is nonnegative and there exists an integer \\(m\\) such that \\((I+A)^m\\) is positive. \\(A\\) is primitive if \\(A\\) is nonnegative and there exists an integer \\(m\\) such that \\(A^m\\) is positive. A matrix \\(A\\) is positive, if and only if \\(A^{\\top}\\) is positive. This observation holds for all four definitions above, i.e. \\(A\\) is nonnegative, irreducible, primitive, respectively, if and only if $A^{\\top} has the corresponding property. We illustrate these definitions with an example. Example. Consider the matrices in \\(\\RR^{2\\times 2}\\) defined as \\[\\begin{equation}\\label{def:Example_ABCD} A=\\bigg(\\begin{matrix}0 & 0 \\\\ 1 & 1\\end{matrix}\\bigg), \\qquad B=\\bigg(\\begin{matrix}0 & 1 \\\\ 1 & 0\\end{matrix}\\bigg), \\qquad C=\\bigg(\\begin{matrix}0 & 1 \\\\ 1 & 1\\end{matrix}\\bigg),\\qquad D=\\bigg(\\begin{matrix}1 & 2 \\\\ 1 & 1\\end{matrix}\\bigg) \\end{equation}\\] Then: \\(A,B,C,D\\) are nonnegative; \\(B,C,D\\) are irreducible; \\(C,D\\) are primitive; \\(D\\) is positive; \\(A,B,C\\) are not positive; \\(A,B\\) are not primitive; \\(A\\) is not irreducible. These defintions can be interpret in the context of graphs. Indeed, one can associated a directed graph \\(G(A)=(V,E)\\) to a nonnegative matrix \\(A\\in \\RR^{n\\times n}\\) as follows: The vertexes are \\(V=\\{1,\\ldots,n\\}\\) and there is an edge from \\(i\\) to \\(j\\) , i.e. \\((i,j)\\in E\\) , if \\(A_{ij}>0\\) . A directed path from \\(i\\in V\\) to \\(j\\in V\\) , is a sequence of \\(\\ell\\) edges \\((k_1,k_2),(k_2,k_3),\\ldots,(k_{\\ell},k_{\\ell+1})\\in E\\) such that \\(k_1=1\\) and \\(k_{\\ell+1}= j\\) . The length of a directed path is the number of edges it traverses, namely \\(\\ell\\) . The above definitions can now be equivalently formulated as follows: Proposition. (Graph characterization) Let \\(A\\in\\RR^{n\\times n}\\) be a nonnegative matrix and \\(G(A)=(V,E)\\) be its associated graph. \\(A\\) is positive if, and only if, \\(G(A)\\) is complete, i.e. \\(E=\\{(i,j)\\mid \\forall i,j\\in V\\}\\) . \\(A\\) is irreducible if, and only if, \\(G(A)\\) is connected, i.e. for all \\(i,j\\in V\\) there exists a directed path from \\(i\\) to \\(j\\) (see Theorem \u2026). \\(A\\) is primitive if, and only if, \\(G(A)\\) is connected and the greatest common divisor of the lengths of all the directed paths starting and ending at the same node equals \\(1\\) (see Theorem 8.5.3 [Horn]). Note that the above characterizations highlight the fact that positivivity, irreducibility and primtivity are concepts related to the pattern of the positive entries rather than the magnitude of these entries. Another way to characterize nonnegative, positive, irreducible and primitive matrices, is to analyze their image on vectors with nonnegative entries. Note for instance that a matrix \\(A\\in\\RR^{n\\times n}\\) is nonnegative if, and only if, \\(Ax\\in C_+\\) for all \\(x\\in C_+\\) . Proposition. (operator characterization) Let \\(A\\in\\RR^{n\\times n}\\) be a nonnegative matrix. \\(A\\) is positive if, and only if, \\(Ax\\in C_{++}\\) for all \\(x\\in\\RR^n_+\\setminus\\{0\\}\\) . \\(A\\) is irreducible if, and only if, there exists an integer \\(m\\) such that \\(\\sum_{k= 0}^m A^{k}x\\in C_{++}\\) for all \\(x\\in\\RR^n_+\\setminus\\{0\\}\\) (see Theorem \u2026). \\(A\\) is primitive if, and only if, there exists an integer \\(m\\) such that \\(A^{m}x\\in C_{++}\\) for all \\(x\\in C_+\\setminus\\{0\\}\\) (see Theorem \u2026). From the above characterization and the matrices \\(A,B,C,D\\) of the Example, one can deduce the following inclusions: Lemma. Let \\(n>1\\) be an integer and consider the following sets of matrices: \\[\\begin{equation*} \\begin{array}{l c l} M_{\\text{nneg}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is nonnegative}\\}, && M_{\\text{irr}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is irreducible}\\},\\\\ M_{\\text{prim}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is primitive}\\}, && M_{\\text{pos}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is positive}\\}.\\end{array} \\end{equation*}\\] Then, it hold \\(M_{\\text{nneg}} \\subsetneq M_{\\text{irr}} \\subsetneq M_{\\text{prim}} \\subsetneq M_{\\text{pos}}\\) . We refer to [\u2026] for an extended discussion on nonnegative, irreducible, primitive and positive matrices. The Perron-Frobenius theorem and the Collatz-Wielandt formula We can now state the celebrated theorem of Perron and Frobenius which provides conditions for the existence of a nonnegative eigenvector, i.e. an eigenvector in \\(C_+\\) , the existence and uniqueness of a positive eigenvector, i.e. an eigenvector in \\(C_{++}\\) , and the convergence of a sequence towards a unique positive eigenvector. Note that, by unique eigenvector, we mean unique up to scale so that \\(u\\) and \\(\\alpha u\\) are considered to be the same eigenvector. Theorem (Perron-Frobenius). Let \\(A\\in\\RR^{n\\times n}\\) be a matrix and \\(\\|\\cdot\\|\\) a norm on \\(\\RR^n\\) . If \\(A\\) is nonnegative, then there exists an eigenvector \\(u\\in C_+\\) such that \\(Au=\\rho(A)u\\) . If \\(A\\) is irreducible, then \\(u\\) is the unique up to scale eigenvector of \\(A\\) in \\(\\RR^n_+\\) . Furthermore \\(u\\in C_{++}\\) and \\(\\rho(A)\\) is a geometrically simple eigenvalue. If \\(A\\) is primitive, then \\(\\rho(A)\\) is algebraically simple and for every \\(x^0\\in C_+\\setminus\\{0\\}\\) , the sequence \\((x^k)_{k=0}^{\\infty}\\subset\\RR^n\\) defined as \\(x^k = A x^{k-1}\\) for all \\(k\\geq 1\\) satisfies \\[\\lim_{k\\to \\infty} \\frac{x^k}{\\|x^k\\|}= \\frac{u}{\\|u\\|}.\\] The matrices of the previous example shows that the assumptions in the Perron-Frobenius theorem are well calibrated. Remark. Consider the matrices \\(A,B,C,D\\) defined in \\(\\eqref{def:Example_ABCD}\\) . Then \\(B,C,D\\) have \\((1,1)^\\top\\) as positive eigenvector. The matrix \\(A\\) , is an example of nonnegative matrix which is not irreducible, and have no positive eigenvector. The matrix \\(B\\) , is an example of irreducible matrix which is not primitive, and for which the sequence \\((x^k)_{k=0}^{\\infty}\\) defined in the Perron-Frobenius theorem do not converge whenever \\(x^{0}\\in\\RR^2\\) satisfy \\(x^{0}_1\\neq x^{0}_2\\) . We mention one additional result from Collatz and Wielandt which provides a sup-min and an inf-max characterization of the spectral radius. The expressions in the Collatz-Wielandt formula is closely related to the Hilbert metric which is a key ingredient to generalize the Perron-Frobenius theorem. As illustrated by the applications below, the Collatz-Wielandt formula is useful to prove the maximality of an eigenvalue. Theorem (Collatz-Wielandt). Let \\(A\\in\\RR^{n\\times n}\\) be a nonnegative matrix. Then, it holds \\[\\begin{equation}\\label{def:cw_up} \\rho(A)\\quad =\\quad \\inf_{x\\in\\RR_{++}^n}\\quad \\max_{i=1,\\ldots,n}\\quad \\frac{(Ax)_i}{x_i}. \\end{equation}\\] If additionally, \\(A\\) has an eigenvector \\(u\\in\\RR^n_{++}\\) , then it holds \\[\\begin{equation}\\label{def:cw_down} \\rho(A)\\quad = \\quad\\sup_{\\substack{x\\in\\RR_{+}^n\\\\ x\\neq 0}} \\quad\\min_{\\substack{i=1,\\ldots,n\\\\ x_i >0}}\\quad \\frac{(Ax)_i}{x_i}. \\end{equation}\\] and both, the infimum and the supremum above, are attained at \\(u\\) . The Collatz-Wielandt formula is useful to discuss the maximality of eigenvalues, i.e. the egivenvalue of any positive eigenvector equals the spectral radius. Application: Stochastic matrices and homogeneous Markov chains on \\([n]\\) Let \\(S\\in\\RR^{n\\times n}\\) be a stochastic matrix, i.e. \\(S\\) is nonnegative and \\(S\\mathbf{1}=\\mathbf{1}\\) with \\(\\mathbf{1}=(1,\\ldots,1)^\\top \\in\\RR^n\\) . Let us connsider the set of probability vectors \\(\\Delta_+=\\{x\\in\\RR^n_+\\colon x_1+\\ldots+x_n=1\\}\\) . Note that \\(S\\) is a nonnegative matrix and \\(u=\\frac{1}{n}\\mathbf{1}\\) is a positive eigenvector of \\(S\\) corresponding to the eigenvalue \\(\\lambda=1\\) . We show the existence and uniqueness of a startionary distribution. First, note that by the Collatz-Wielandt formula, \\[\\rho(S) =\\inf_{x\\in\\RR_{++}^n} \\max_{i=1,\\ldots,n} \\frac{(Sx)_i}{x_i} = \\max_{i=1,\\ldots,n} \\frac{(Su)_i}{u_i} = 1. \\] As \\(S^\\top\\) is a nonnegative matrix, by the Perron-Frobenius theorem, we know that there exists \\(\\pi\\in \\RR^n_+\\) with \\(\\pi_1+\\ldots+\\pi_n=1\\) and \\(S^\\top \\pi = \\rho(S^\\top) \\pi\\) . As \\(\\rho(S^\\top)=\\rho(S)=1\\) , we have \\(S^\\top \\pi =\\pi\\) , i.e. \\(\\pi\\) is a stationary distribution of \\(S\\) . This proves the existence. If \\(S\\) is irreducible, then \\(S^T\\) is irreducible as well and by the Perron-Frobenius theorem, \\(\\pi\\) is the unique stationary distribution of \\(S\\) and \\(\\pi_1,\\ldots,\\pi_n>0\\) . Consider a sequence of random variables \\(X_0,X_1,\\ldots\\) with values in \\([n]=\\{1,\\ldots,n\\}\\) . Suppose that \\[\\operatorname{Pr}(X_k=j_k\\mid X_{k-1}=j_{k-1},\\ldots X_0=j_0)=\\operatorname{Pr}(X_1=j_k\\mid X_0=j_0)= S_{j_kj_0}\\] for all \\(k=1,2,\\ldots\\) \\(X_0,X_1,\\ldots\\) is a homogeneous Markov chain. Note that if \\(X_0\\) has distribution \\(x^0\\in\\Delta_+\\) , i.e. \\(\\operatorname{Pr}(X_0 = j)=x^0_j\\) for all \\(j\\in [n]\\) , then the distribution \\(x^k\\in\\Delta_+\\) of \\(X_k\\) satisfies \\(x^k=(S^\\top)^k x^{0}\\) for all \\(k\\geq 0\\) . So, if \\(S\\) is primitive, then so is \\(S^T\\) . Let \\(\\|\\cdot\\|_1\\) be the \\(1\\) -norm, i.e. \\(\\|x\\|_1=|x_1|+\\ldots+|x_n|\\) . Note that as \\(x^k\\in \\Delta_+\\) , it holds \\(\\|x^k\\|_1=1\\) for all \\(k\\geq 0\\) . Suppose, that \\(S\\) is primitive, then the Perron-Frobenius theorem, with \\(\\|\\cdot\\|=\\|\\cdot\\|_1\\) , implies that for every initial distribution \\(x^0\\in \\Delta_+\\) of \\(X_0\\) , it holds \\(\\lim_{k\\to \\infty}x^k = \\pi\\) . Application: Spectral norm of a nonnegative matrix Let \\(\\|\\cdot\\|_2\\) be the \\(2\\) -norm on \\(\\RR^n\\) , the induced matrix norm \\(\\|\\cdot\\|_{2,2}\\) on \\(\\RR^{n\\times n}\\) is the spectral norm defined as \\[ \\|A\\|_{2,2}=\\max_{x\\in\\RR^n}\\{\\|Ax\\|_2 \\colon \\|x\\|_2\\leq 1\\}=\\max_{x\\neq 0}\\frac{\\| Ax \\|_2}{\\| x \\|_2}. \\] The function \\(x\\mapsto \\|Ax\\|_2\\) is continuous on the compact set \\(\\{x\\colon \\|x\\|_2\\leq 1\\}\\) and therefore attains a maximum \\(x^*\\in\\RR^n, x^*\\neq 0\\) . Now, note that since \\(A\\) is nonnegative, we have \\(|Ax| \\leq A|x|\\) where the absolute value and the inequality are understood component-wise. In particular, this implies that \\[ \\|A\\|_{2,2}=\\frac{\\| Ax^* \\|_2}{\\| x^* \\|_2}=\\frac{\\| \\,|Ax^*|\\, \\|_2}{\\|\\,| x^*|\\, \\|_2}\\leq \\frac{\\| \\,|Ax^*|\\, \\|_2}{\\|\\,| x^*|\\, \\|_2}, \\] and therefore we may assume without loss of generality that \\(x^*\\in\\RR^n_+\\) . Now, note that if \\(v\\in\\RR^n_+\\) is a critical point of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) , the gradient of the later function vanishes at \\(x\\) and therefore it satisfies \\[A^TAv = \\lambda v \\qquad \\text{with} \\qquad \\lambda =\\Big(\\frac{\\| Av \\|_2}{\\| v \\|_2}\\Big)^2=\\|A\\|_{2,2}^2, \\] i.e. \\(v\\) is an eigenvector of \\(A^TA\\) . The converse of this statement holds true as well, that is, if \\(v\\) is an eigenvector of \\(A^TA\\) with eigenvalue \\(\\lambda\\) , then \\(v\\) is a critical point of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) and \\(\\lambda = (\\|Ax\\|_2/\\|x\\|_2)^2\\) . Therefore, the eigenvalues of \\(A^TA\\) and the square of the critical values of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) are equal. In particular, this implies that \\(\\rho(A^TA) =\\|A\\|_{2,2}^2\\) . Now, \\(A^TA\\) is a nonnegative matrix and if it is irreducible, by the Perron-Frobenius theorem, \\(A^TA\\) has a unique positive eigenvector \\(u\\in \\RR^n_{+}\\) such that \\(\\|u\\|_2=1\\) . Furthermore, \\(u\\in \\RR^n_{++}\\) and the corresponding eigenvalue is \\(\\rho(A^TA)\\) . From the above discussion: \\(x^*\\) is a global maximizer of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) and we may assume \\(x^*\\in\\RR^n_+\\) . Hence, \\(x^*\\) is a nonnegative eigenvector of \\(A^TA\\) and since \\(u\\) is the unique eigenvector of \\(A^TA\\) , we have \\(x^*/\\|x^*\\|_2 = u\\) . Finally, if \\(A^TA\\) is primitive, then for every \\(x^0\\in\\RR^n_+\\setminus \\{0\\}\\) , the sequence \\(x^k=A^TAx^{k-1}\\) satsifies \\(\\lim_{k\\to\\infty} x^k/\\|x^k\\| = x^*/\\|x^*\\|_2\\) , i.e. this sequence converges to a global maximizer of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) . We conclude by noting that, while the irreducibility of \\(A\\) implies the irreducibility of \\(A^TA\\) , the converse is not true in general. Indeed, the matrix \\(A\\) of \\(\\eqref{def:Example_ABCD}\\) is not irreducible, but \\(A^TA\\) is a positive matrix and is therefore irreducible and primitive. The Perron-Frobenius theorem for multi-homogeneous mappings Nonnegative tensors and their norm Product of cones and multi-homogeneous mappings Eigenvalues and eigenvectors of multi-homogeneous mappings The Perron-Frobenius theorem and the Collatz-Wielandt formula Application: Computing the norm of a nonnegative tensor Application: \u2026 More generally, it can be shown that every closed cone in a finite dimensional space is normal (see Lemma 1.2.5 [NB]). \u21a9","title":"Linear mappings"},{"location":"ch1/sec1/#linear_mappings","text":"","title":"Linear mappings"},{"location":"ch1/sec1/#brief_history","text":"In 1907, in [\u2026], Oskar Perron proves that the spectral radius of primitive matrices is an algebraically simple eigenvalue and the corresponding eigenvector can be scaled to have all entries positive. In 1908, in [\u2026], George Frobenius extends the result by showing that the spectral radius of irreducible matrices is a geometrically simple eigenvalue and the corresponding eigenvector can be scaled to have positive entries. Furthermore, Perron observes that when the spectral radius is an algebraically simple eigenvalue, one can use the power method introduced by \u2026 in \u2026 to compute a maximal eigenvector, i.e. an eigenvector corresponding with the spectral radius as eigenvalue. These results have been then complemented by a broad diversity of results about nonnegative matrix and we refer to [Plemmons] for further readings on the topic. In particular, we will discuss the Collatz-Wielandt ratio, which provides a sup-min and an inf-max characterization of the spectral radius.","title":"Brief history"},{"location":"ch1/sec1/#to_be_killed","text":"We briefly recall here well-known results and concepts of linear algebra which will are useful to understand the subsequent discussion. Given a matrix \\(A\\in\\RR^{n\\times n}\\) , \\(\\lambda\\in \\RR\\) is an eigenvalue of \\(A\\) if there exists \\(x\\in \\RR^n\\) with \\(x\\neq 0\\) and \\(Ax = \\lambda x\\) . A vector \\(x\\) is called an eigenvector of \\(A\\) if \\(x\\neq 0\\) and there exists a corresponding eigenvalue \\(\\lambda\\in \\RR\\) such that \\(Ax=x\\) . An eigenvalue \\(\\lambda\\) of \\(A\\) is algebraically simple if it is a simple root of the polynomial \\(p(t)=\\det(A-tI)\\) , where \\(I\\in\\RR^{n\\times n}\\) is the identity matrix. An eigenvalue \\(\\lambda\\) of \\(A\\) is geometrically simple if \\(\\dim(\\ker(A-\\lambda I))=1\\) , i.e. there is a unique up to scale eigenvector corresponding to \\(\\lambda\\) . Algebraic simplicity implies geometric simplicity but the converse is not true in general. The spectral radius of \\(A\\in\\RR^{n\\times n}\\) is denoted by \\(\\rho(A)\\) and defined as \\[\\rho(A)=\\max\\{|\\lambda| \\colon \\lambda \\text{ is an eigenvalue of }A\\}\\] For \\(p\\in[1,\\infty)\\) , let \\(\\|x\\|_p = \\big(\\sum_{i=1}^n|x_i|^p\\big)^{1/p}\\) denote the usual \\(p\\) -norm for every \\(x\\in\\RR^n\\) .","title":"To be killed"},{"location":"ch1/sec1/#the_cone_of_nonnegative_vectors","text":"The Perron-Frobenius theory deals with operators that leave a (\u00bfsolid?) normal, convex and pointed cone invariant. The cone of nonnegative vectors is the most common example of such a cone and the one that most often arises in applications. We denote such cone as \\(C_+ \\subseteq \\mathbb R^n\\) or, when the dimension is clear from the context, simply as \\(C_+.\\) The elements of \\(C_+\\) are vectors whose components are all nonnegative. Whereas the interior \\(C_{++}\\) of \\(C_+\\) is the set of entrywise positive vectors: \\(C_+ = \\{x\\in \\mathbb R^n : x_i\\geq 0\\) for all \\(i=1,\\dots,n\\}=\\{x\\in \\mathbb R^n : x\\succeq 0\\}\\) \\(C_{++} = \\{x\\in \\mathbb R^n : x_i> 0\\) for all \\(i=1,\\dots,n\\}=\\{x\\in \\mathbb R^n : x\\succ 0\\}.\\) The cone \\(C_+\\) induces a partial ordering on \\(\\RR^n\\) defined as \\(x\\preceq y\\) if \\(y-x\\in C_+\\) . In particular, as \\(C_+\\) denotes the cone of nonnegative vectors in \\(\\RR^n\\) , we have that \\(x\\preceq y\\) if and only if \\(x_i \\leq y_i\\) for all \\(i=1,\\ldots,n\\) . This partial order induce a equivalence relation \\(\\sim\\) on \\(C_+\\) defined as \\(x\\sim y\\) if there exists \\(\\alpha,\\beta>0\\) such that \\(\\alpha x \\preceq y \\preceq \\beta x\\) . The equivalence classes are called the parts of the cone \\(C_+\\) . For example, \\(C_+\\subset\\RR^2\\) has four parts given \\(\\{0\\}, \\{(s,0)\\colon s>0\\}, \\{(0,t)\\colon t>0\\}, C_{++}\\) . \\(C_+\\) is convex, pointed and normal as \\(\\alpha x\\in C_+\\) for all \\(x\\in C_+\\) and all scalar coefficients \\(\\alpha \\geq 0\\) , \\(C_+\\cap -C_+ = \\{0\\}\\) and for every norm \\(\\|\\cdot\\|\\) on \\(\\RR^n\\) , there exists \\(\\gamma>0\\) such that for all pair of vectors \\(x,y\\in C_+\\) such that \\(x\\preceq y\\) it holds \\(\\|x\\|\\leq \\gamma \\|y\\|\\) . 1 This tutorial will focus only on \\(C_+\\) , however we point out that all the results we will present can be relatively directly transferred to arbitrary normal, convex and pointed cones.","title":"The cone of nonnegative vectors"},{"location":"ch1/sec1/#nonnegative_irreducible_primitive_and_positive_matrices","text":"The classical Perron-Frobenius theory is mainly concerned with matrices having nonnegative entries. Based on the pattern of their positive entries, multiple results can be derived concerning the simplicity of their eigenvalues of maximal magnitude and the corresponding eigenvectors. To characterize different patterns of positive entries, we recall the definition of primitive and irreducible matrices. We aslo recall characterizations of these definitions as they will be generalized in different ways in the nonlinear setting: Definition. Let \\(A\\in\\RR^{n\\times n}\\) be a matrix. \\(A\\) is nonnegative if \\(A_{ij}\\geq 0\\) for all \\(i,j=1,\\ldots,n\\) . \\(A\\) is positive if \\(A_{ij}>0\\) for all \\(i,j=1,\\ldots,n\\) . \\(A\\) is irreducible if \\(A\\) is nonnegative and there exists an integer \\(m\\) such that \\((I+A)^m\\) is positive. \\(A\\) is primitive if \\(A\\) is nonnegative and there exists an integer \\(m\\) such that \\(A^m\\) is positive. A matrix \\(A\\) is positive, if and only if \\(A^{\\top}\\) is positive. This observation holds for all four definitions above, i.e. \\(A\\) is nonnegative, irreducible, primitive, respectively, if and only if $A^{\\top} has the corresponding property. We illustrate these definitions with an example. Example. Consider the matrices in \\(\\RR^{2\\times 2}\\) defined as \\[\\begin{equation}\\label{def:Example_ABCD} A=\\bigg(\\begin{matrix}0 & 0 \\\\ 1 & 1\\end{matrix}\\bigg), \\qquad B=\\bigg(\\begin{matrix}0 & 1 \\\\ 1 & 0\\end{matrix}\\bigg), \\qquad C=\\bigg(\\begin{matrix}0 & 1 \\\\ 1 & 1\\end{matrix}\\bigg),\\qquad D=\\bigg(\\begin{matrix}1 & 2 \\\\ 1 & 1\\end{matrix}\\bigg) \\end{equation}\\] Then: \\(A,B,C,D\\) are nonnegative; \\(B,C,D\\) are irreducible; \\(C,D\\) are primitive; \\(D\\) is positive; \\(A,B,C\\) are not positive; \\(A,B\\) are not primitive; \\(A\\) is not irreducible. These defintions can be interpret in the context of graphs. Indeed, one can associated a directed graph \\(G(A)=(V,E)\\) to a nonnegative matrix \\(A\\in \\RR^{n\\times n}\\) as follows: The vertexes are \\(V=\\{1,\\ldots,n\\}\\) and there is an edge from \\(i\\) to \\(j\\) , i.e. \\((i,j)\\in E\\) , if \\(A_{ij}>0\\) . A directed path from \\(i\\in V\\) to \\(j\\in V\\) , is a sequence of \\(\\ell\\) edges \\((k_1,k_2),(k_2,k_3),\\ldots,(k_{\\ell},k_{\\ell+1})\\in E\\) such that \\(k_1=1\\) and \\(k_{\\ell+1}= j\\) . The length of a directed path is the number of edges it traverses, namely \\(\\ell\\) . The above definitions can now be equivalently formulated as follows: Proposition. (Graph characterization) Let \\(A\\in\\RR^{n\\times n}\\) be a nonnegative matrix and \\(G(A)=(V,E)\\) be its associated graph. \\(A\\) is positive if, and only if, \\(G(A)\\) is complete, i.e. \\(E=\\{(i,j)\\mid \\forall i,j\\in V\\}\\) . \\(A\\) is irreducible if, and only if, \\(G(A)\\) is connected, i.e. for all \\(i,j\\in V\\) there exists a directed path from \\(i\\) to \\(j\\) (see Theorem \u2026). \\(A\\) is primitive if, and only if, \\(G(A)\\) is connected and the greatest common divisor of the lengths of all the directed paths starting and ending at the same node equals \\(1\\) (see Theorem 8.5.3 [Horn]). Note that the above characterizations highlight the fact that positivivity, irreducibility and primtivity are concepts related to the pattern of the positive entries rather than the magnitude of these entries. Another way to characterize nonnegative, positive, irreducible and primitive matrices, is to analyze their image on vectors with nonnegative entries. Note for instance that a matrix \\(A\\in\\RR^{n\\times n}\\) is nonnegative if, and only if, \\(Ax\\in C_+\\) for all \\(x\\in C_+\\) . Proposition. (operator characterization) Let \\(A\\in\\RR^{n\\times n}\\) be a nonnegative matrix. \\(A\\) is positive if, and only if, \\(Ax\\in C_{++}\\) for all \\(x\\in\\RR^n_+\\setminus\\{0\\}\\) . \\(A\\) is irreducible if, and only if, there exists an integer \\(m\\) such that \\(\\sum_{k= 0}^m A^{k}x\\in C_{++}\\) for all \\(x\\in\\RR^n_+\\setminus\\{0\\}\\) (see Theorem \u2026). \\(A\\) is primitive if, and only if, there exists an integer \\(m\\) such that \\(A^{m}x\\in C_{++}\\) for all \\(x\\in C_+\\setminus\\{0\\}\\) (see Theorem \u2026). From the above characterization and the matrices \\(A,B,C,D\\) of the Example, one can deduce the following inclusions: Lemma. Let \\(n>1\\) be an integer and consider the following sets of matrices: \\[\\begin{equation*} \\begin{array}{l c l} M_{\\text{nneg}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is nonnegative}\\}, && M_{\\text{irr}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is irreducible}\\},\\\\ M_{\\text{prim}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is primitive}\\}, && M_{\\text{pos}}=\\{A\\in\\RR^{n\\times n}\\colon A \\text{ is positive}\\}.\\end{array} \\end{equation*}\\] Then, it hold \\(M_{\\text{nneg}} \\subsetneq M_{\\text{irr}} \\subsetneq M_{\\text{prim}} \\subsetneq M_{\\text{pos}}\\) . We refer to [\u2026] for an extended discussion on nonnegative, irreducible, primitive and positive matrices.","title":"Nonnegative, irreducible, primitive and positive matrices"},{"location":"ch1/sec1/#the_perron-frobenius_theorem_and_the_collatz-wielandt_formula","text":"We can now state the celebrated theorem of Perron and Frobenius which provides conditions for the existence of a nonnegative eigenvector, i.e. an eigenvector in \\(C_+\\) , the existence and uniqueness of a positive eigenvector, i.e. an eigenvector in \\(C_{++}\\) , and the convergence of a sequence towards a unique positive eigenvector. Note that, by unique eigenvector, we mean unique up to scale so that \\(u\\) and \\(\\alpha u\\) are considered to be the same eigenvector. Theorem (Perron-Frobenius). Let \\(A\\in\\RR^{n\\times n}\\) be a matrix and \\(\\|\\cdot\\|\\) a norm on \\(\\RR^n\\) . If \\(A\\) is nonnegative, then there exists an eigenvector \\(u\\in C_+\\) such that \\(Au=\\rho(A)u\\) . If \\(A\\) is irreducible, then \\(u\\) is the unique up to scale eigenvector of \\(A\\) in \\(\\RR^n_+\\) . Furthermore \\(u\\in C_{++}\\) and \\(\\rho(A)\\) is a geometrically simple eigenvalue. If \\(A\\) is primitive, then \\(\\rho(A)\\) is algebraically simple and for every \\(x^0\\in C_+\\setminus\\{0\\}\\) , the sequence \\((x^k)_{k=0}^{\\infty}\\subset\\RR^n\\) defined as \\(x^k = A x^{k-1}\\) for all \\(k\\geq 1\\) satisfies \\[\\lim_{k\\to \\infty} \\frac{x^k}{\\|x^k\\|}= \\frac{u}{\\|u\\|}.\\] The matrices of the previous example shows that the assumptions in the Perron-Frobenius theorem are well calibrated. Remark. Consider the matrices \\(A,B,C,D\\) defined in \\(\\eqref{def:Example_ABCD}\\) . Then \\(B,C,D\\) have \\((1,1)^\\top\\) as positive eigenvector. The matrix \\(A\\) , is an example of nonnegative matrix which is not irreducible, and have no positive eigenvector. The matrix \\(B\\) , is an example of irreducible matrix which is not primitive, and for which the sequence \\((x^k)_{k=0}^{\\infty}\\) defined in the Perron-Frobenius theorem do not converge whenever \\(x^{0}\\in\\RR^2\\) satisfy \\(x^{0}_1\\neq x^{0}_2\\) . We mention one additional result from Collatz and Wielandt which provides a sup-min and an inf-max characterization of the spectral radius. The expressions in the Collatz-Wielandt formula is closely related to the Hilbert metric which is a key ingredient to generalize the Perron-Frobenius theorem. As illustrated by the applications below, the Collatz-Wielandt formula is useful to prove the maximality of an eigenvalue. Theorem (Collatz-Wielandt). Let \\(A\\in\\RR^{n\\times n}\\) be a nonnegative matrix. Then, it holds \\[\\begin{equation}\\label{def:cw_up} \\rho(A)\\quad =\\quad \\inf_{x\\in\\RR_{++}^n}\\quad \\max_{i=1,\\ldots,n}\\quad \\frac{(Ax)_i}{x_i}. \\end{equation}\\] If additionally, \\(A\\) has an eigenvector \\(u\\in\\RR^n_{++}\\) , then it holds \\[\\begin{equation}\\label{def:cw_down} \\rho(A)\\quad = \\quad\\sup_{\\substack{x\\in\\RR_{+}^n\\\\ x\\neq 0}} \\quad\\min_{\\substack{i=1,\\ldots,n\\\\ x_i >0}}\\quad \\frac{(Ax)_i}{x_i}. \\end{equation}\\] and both, the infimum and the supremum above, are attained at \\(u\\) . The Collatz-Wielandt formula is useful to discuss the maximality of eigenvalues, i.e. the egivenvalue of any positive eigenvector equals the spectral radius.","title":"The Perron-Frobenius theorem and the Collatz-Wielandt formula"},{"location":"ch1/sec1/#application_stochastic_matrices_and_homogeneous_markov_chains_on_n","text":"Let \\(S\\in\\RR^{n\\times n}\\) be a stochastic matrix, i.e. \\(S\\) is nonnegative and \\(S\\mathbf{1}=\\mathbf{1}\\) with \\(\\mathbf{1}=(1,\\ldots,1)^\\top \\in\\RR^n\\) . Let us connsider the set of probability vectors \\(\\Delta_+=\\{x\\in\\RR^n_+\\colon x_1+\\ldots+x_n=1\\}\\) . Note that \\(S\\) is a nonnegative matrix and \\(u=\\frac{1}{n}\\mathbf{1}\\) is a positive eigenvector of \\(S\\) corresponding to the eigenvalue \\(\\lambda=1\\) . We show the existence and uniqueness of a startionary distribution. First, note that by the Collatz-Wielandt formula, \\[\\rho(S) =\\inf_{x\\in\\RR_{++}^n} \\max_{i=1,\\ldots,n} \\frac{(Sx)_i}{x_i} = \\max_{i=1,\\ldots,n} \\frac{(Su)_i}{u_i} = 1. \\] As \\(S^\\top\\) is a nonnegative matrix, by the Perron-Frobenius theorem, we know that there exists \\(\\pi\\in \\RR^n_+\\) with \\(\\pi_1+\\ldots+\\pi_n=1\\) and \\(S^\\top \\pi = \\rho(S^\\top) \\pi\\) . As \\(\\rho(S^\\top)=\\rho(S)=1\\) , we have \\(S^\\top \\pi =\\pi\\) , i.e. \\(\\pi\\) is a stationary distribution of \\(S\\) . This proves the existence. If \\(S\\) is irreducible, then \\(S^T\\) is irreducible as well and by the Perron-Frobenius theorem, \\(\\pi\\) is the unique stationary distribution of \\(S\\) and \\(\\pi_1,\\ldots,\\pi_n>0\\) . Consider a sequence of random variables \\(X_0,X_1,\\ldots\\) with values in \\([n]=\\{1,\\ldots,n\\}\\) . Suppose that \\[\\operatorname{Pr}(X_k=j_k\\mid X_{k-1}=j_{k-1},\\ldots X_0=j_0)=\\operatorname{Pr}(X_1=j_k\\mid X_0=j_0)= S_{j_kj_0}\\] for all \\(k=1,2,\\ldots\\) \\(X_0,X_1,\\ldots\\) is a homogeneous Markov chain. Note that if \\(X_0\\) has distribution \\(x^0\\in\\Delta_+\\) , i.e. \\(\\operatorname{Pr}(X_0 = j)=x^0_j\\) for all \\(j\\in [n]\\) , then the distribution \\(x^k\\in\\Delta_+\\) of \\(X_k\\) satisfies \\(x^k=(S^\\top)^k x^{0}\\) for all \\(k\\geq 0\\) . So, if \\(S\\) is primitive, then so is \\(S^T\\) . Let \\(\\|\\cdot\\|_1\\) be the \\(1\\) -norm, i.e. \\(\\|x\\|_1=|x_1|+\\ldots+|x_n|\\) . Note that as \\(x^k\\in \\Delta_+\\) , it holds \\(\\|x^k\\|_1=1\\) for all \\(k\\geq 0\\) . Suppose, that \\(S\\) is primitive, then the Perron-Frobenius theorem, with \\(\\|\\cdot\\|=\\|\\cdot\\|_1\\) , implies that for every initial distribution \\(x^0\\in \\Delta_+\\) of \\(X_0\\) , it holds \\(\\lim_{k\\to \\infty}x^k = \\pi\\) .","title":"Application: Stochastic matrices and homogeneous Markov chains on \\([n]\\)"},{"location":"ch1/sec1/#application_spectral_norm_of_a_nonnegative_matrix","text":"Let \\(\\|\\cdot\\|_2\\) be the \\(2\\) -norm on \\(\\RR^n\\) , the induced matrix norm \\(\\|\\cdot\\|_{2,2}\\) on \\(\\RR^{n\\times n}\\) is the spectral norm defined as \\[ \\|A\\|_{2,2}=\\max_{x\\in\\RR^n}\\{\\|Ax\\|_2 \\colon \\|x\\|_2\\leq 1\\}=\\max_{x\\neq 0}\\frac{\\| Ax \\|_2}{\\| x \\|_2}. \\] The function \\(x\\mapsto \\|Ax\\|_2\\) is continuous on the compact set \\(\\{x\\colon \\|x\\|_2\\leq 1\\}\\) and therefore attains a maximum \\(x^*\\in\\RR^n, x^*\\neq 0\\) . Now, note that since \\(A\\) is nonnegative, we have \\(|Ax| \\leq A|x|\\) where the absolute value and the inequality are understood component-wise. In particular, this implies that \\[ \\|A\\|_{2,2}=\\frac{\\| Ax^* \\|_2}{\\| x^* \\|_2}=\\frac{\\| \\,|Ax^*|\\, \\|_2}{\\|\\,| x^*|\\, \\|_2}\\leq \\frac{\\| \\,|Ax^*|\\, \\|_2}{\\|\\,| x^*|\\, \\|_2}, \\] and therefore we may assume without loss of generality that \\(x^*\\in\\RR^n_+\\) . Now, note that if \\(v\\in\\RR^n_+\\) is a critical point of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) , the gradient of the later function vanishes at \\(x\\) and therefore it satisfies \\[A^TAv = \\lambda v \\qquad \\text{with} \\qquad \\lambda =\\Big(\\frac{\\| Av \\|_2}{\\| v \\|_2}\\Big)^2=\\|A\\|_{2,2}^2, \\] i.e. \\(v\\) is an eigenvector of \\(A^TA\\) . The converse of this statement holds true as well, that is, if \\(v\\) is an eigenvector of \\(A^TA\\) with eigenvalue \\(\\lambda\\) , then \\(v\\) is a critical point of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) and \\(\\lambda = (\\|Ax\\|_2/\\|x\\|_2)^2\\) . Therefore, the eigenvalues of \\(A^TA\\) and the square of the critical values of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) are equal. In particular, this implies that \\(\\rho(A^TA) =\\|A\\|_{2,2}^2\\) . Now, \\(A^TA\\) is a nonnegative matrix and if it is irreducible, by the Perron-Frobenius theorem, \\(A^TA\\) has a unique positive eigenvector \\(u\\in \\RR^n_{+}\\) such that \\(\\|u\\|_2=1\\) . Furthermore, \\(u\\in \\RR^n_{++}\\) and the corresponding eigenvalue is \\(\\rho(A^TA)\\) . From the above discussion: \\(x^*\\) is a global maximizer of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) and we may assume \\(x^*\\in\\RR^n_+\\) . Hence, \\(x^*\\) is a nonnegative eigenvector of \\(A^TA\\) and since \\(u\\) is the unique eigenvector of \\(A^TA\\) , we have \\(x^*/\\|x^*\\|_2 = u\\) . Finally, if \\(A^TA\\) is primitive, then for every \\(x^0\\in\\RR^n_+\\setminus \\{0\\}\\) , the sequence \\(x^k=A^TAx^{k-1}\\) satsifies \\(\\lim_{k\\to\\infty} x^k/\\|x^k\\| = x^*/\\|x^*\\|_2\\) , i.e. this sequence converges to a global maximizer of \\(x\\mapsto\\|Ax\\|_2/\\|x\\|_2\\) . We conclude by noting that, while the irreducibility of \\(A\\) implies the irreducibility of \\(A^TA\\) , the converse is not true in general. Indeed, the matrix \\(A\\) of \\(\\eqref{def:Example_ABCD}\\) is not irreducible, but \\(A^TA\\) is a positive matrix and is therefore irreducible and primitive.","title":"Application: Spectral norm of a nonnegative matrix"},{"location":"ch1/sec1/#the_perron-frobenius_theorem_for_multi-homogeneous_mappings","text":"","title":"The Perron-Frobenius theorem for multi-homogeneous mappings"},{"location":"ch1/sec1/#nonnegative_tensors_and_their_norm","text":"","title":"Nonnegative tensors and their norm"},{"location":"ch1/sec1/#product_of_cones_and_multi-homogeneous_mappings","text":"","title":"Product of cones and multi-homogeneous mappings"},{"location":"ch1/sec1/#eigenvalues_and_eigenvectors_of_multi-homogeneous_mappings","text":"","title":"Eigenvalues and eigenvectors of multi-homogeneous mappings"},{"location":"ch1/sec1/#the_perron-frobenius_theorem_and_the_collatz-wielandt_formula_1","text":"","title":"The Perron-Frobenius theorem and the Collatz-Wielandt formula"},{"location":"ch1/sec1/#application_computing_the_norm_of_a_nonnegative_tensor","text":"","title":"Application: Computing the norm of a nonnegative tensor"},{"location":"ch1/sec1/#application","text":"More generally, it can be shown that every closed cone in a finite dimensional space is normal (see Lemma 1.2.5 [NB]). \u21a9","title":"Application: ..."},{"location":"ch1/sec2/","text":"Homogeneous mappings Brief history: Birkhoff (1950): Hilbert metric, Nussbaum: homogeneous mappings, \u2026: strict contractivity. Hilbert projective distance and the Birkhoff-Hopf theorem A key observation for the development of the nonlinear Perron-Frobenius theory was made by Birkhoff, when he noticed that nonnegative matrices are non-expansive with respect to the Hilbert metric induced by the cone \\(C_+\\) . The Hilbert (projective) metric \\(d_H\\colon C_+\\times C_+\\to [0,\\infty]\\) is defined by \\(d_H(0,0)=0\\) , \\(d(x,y)=\\infty\\) if \\(x\\not\\sim y\\) , and \\[d_H(x,y) = \\ln\\Big(\\max_{i,j \\in \\mathcal I} \\frac{x_i}{y_i}\\frac{y_j}{x_j}\\Big) \\quad \\text{ where }\\quad \\mathcal I =\\{i\\colon x_i>0\\},\\] for all \\(x,y\\in C_+\\setminus \\{0\\}\\) such that \\(x\\sim y\\) . Technically, \\(d_H\\) is not a metric but a pseudometric since \\(d(x,y)=0\\) may not imply \\(x=y\\) . However, it is a complete metric on the set of unit vectors contained in the same part of \\(C_+\\) . Lemma. Let \\(\\|\\cdot\\|\\) be a norm on \\(\\RR^n\\) and \\(B=\\{x\\in \\RR^n \\colon \\|x\\|=1\\}\\) . For every \\(x,y\\in C_+\\) it holds \\(d_H(x,y)=0\\) if and only if there exists \\(\\lambda >0\\) such that \\(x= \\lambda y\\) . If \\(P\\subset C_+\\) is a part of \\(C_+\\) , then \\(x\\sim y\\) for all \\(x,y\\in P\\) , and \\((P\\cap B, d_H)\\) is a complete metric space. A direct consequence of the first property in the above Lemma is that for any mapping \\(f\\colon C_+\\to C_+\\) and any \\(x\\in C_+\\) , there exists \\(\\lambda >0\\) such that \\(f(x)=\\lambda x\\) if, and only if, \\(d_H(f(x),x)=0\\) . This observation suggest that the Hilbert metric is particularly appropriate to study eigenvectors of mappings leaving a cone invariant. We should put the picture of my talk here. If \\(M\\) is any nonnegative matrix in \\(\\RR^{n\\times n}\\) , then for every \\(x,y\\in C_{++}\\) it holds \\[\\begin{equation}\\label{eq:nonexpansive} Mx \\preceq M \\Big(\\max_{i=1,\\ldots,n}\\frac{x_i}{y_i}\\Big) y = \\Big(\\max_{i=1,\\ldots,n}\\frac{x_i}{y_i}\\Big) My \\end{equation}\\] and therefore, \\(d_H(Mx,My)\\leq d_H(x,y)\\) , i.e. \\(M\\) is nonnexpansive with respect to the Hilbert metric. The Birkhoff-Hopf theorem refines this observation by offering an explicit formula for the smallest Lipschitz constant of a positive matrix with respect to the Hilbert metric. Theorem. (Birkhoff-Hopf) Let \\(M\\in \\RR^{n\\times n}\\) be a positive matrix and let \\[\\kappa(M)=\\inf\\big\\{\\alpha \\geq 0 \\colon d_H(Mx,My)\\leq d_H(x,y), \\forall x,y\\in C_+, x\\sim y\\big\\},\\] then, it holds \\[\\kappa(M)=\\tanh(\\tfrac{1}{4}\\Delta(M)) \\quad \\text{with}\\quad \\Delta(M)=\\max_{i,j,k,\\ell = 1,\\ldots,n} \\frac{M_{ij}M_{k\\ell}}{M_{i\\ell}M_{kj}}\\] The Birkhoff-Hopf theorem holds for considerably more general settings such as linear mappings defined on general cones in finite or infinite dimensional spaces. We refer to [NBproof] for a very elegant proof in the general setting. Note that from the formula for \\(\\Delta(M)\\) one can see that the smallest Lipschitz constant of \\(M\\) with respect to \\(d_H\\) does not depend on the scale of \\(M\\) , i.e. \\(\\Delta(\\alpha M)=\\Delta(M)\\) for all \\(\\alpha >0\\) . Furthermore, it holds \\(\\Delta(M^\\top)=\\Delta(M)\\) . The quantity \\(\\Delta(M)\\) is the projective diameter of the set \\(M(C_+)\\) and can be characterized as \\[ \\Delta(M)=\\sup\\{d_H(Mx,My)\\colon x,y \\in C_+, x\\sim y\\}. \\] As discussed in the following remark, the Birkhoff-Hopf theorem can be used to prove results of the Perron-Frobenius theorem for positive matrices. Furthermore, \\(\\kappa(M)\\) provides a bound on the linear convergence rate of the sequence converging towards a unique positive eigenvector. Remark. Given a positive matrix \\(M\\in\\RR^{n\\times n}\\) , the Perron-Frobenius theorem implies that \\(M\\) has a unique eigenvector \\(u \\in C_{++}\\) with \\(\\|u\\|=1\\) , and the sequence \\(x^{k}=Ax^{k-1}\\) satisfies \\(\\lim_{k\\to \\infty} x^k/\\|x^k\\|=u\\) for every \\(x^0\\in C_{++}\\) . Thanks to the Birkhoff-Hopf theorem, these results can be obtained as a direct consequence of the Banach fixed point theorem. Indeed, let \\(B\\) be the unit ball in \\(\\RR^n\\) . By the precedent Lemma we know that \\((B\\cap C_++,d_H)\\) is a complete metric space. Since the hyperbolic tangent takes values in \\((0,1)\\) , the Birkhoff-Hopf theorem implies that \\(x\\mapsto Mx\\) is a strict contraction with respect to \\(d_H\\) . Finally, note that the scale invariance of the Hilbert metric implies that \\(x\\mapsto Mx\\) and \\(x\\mapsto Mx/\\|Mx\\|\\) have the same Lipschitz constants. Hence, by the Banach fixed point theorem, the latter mapping has a unique fixed point, \\(u\\) , and its iterates converge to \\(u\\) with rate \\(\\kappa(M)\\) . However, it should be noted that the linear convergence rate provided by the Birkhoff-Hopf theorem is not as tight as the classical ratio between the second and first eigenvalues of \\(M\\) . The Hilbert metric can be used to prove a generalization of the Perron-Frobenius theorem to eigenvectors of nonlinear mappings on cones. We discuss the class of such mappings in the next section. Homogeneous and order-preserving mappings To prove that nonnegative matrices are nonexpansive with respect to \\(d_H\\) in Equation \\(\\eqref{eq:nonexpansive}\\) , we have used two important properties of the nonnegative matrix \\(M\\) : First, we have used the fact that for every \\(x,y\\in C_+\\) satisfying \\(x\\preceq y\\) it holds \\(Mx\\preceq My\\) and second, we have used the homogeneity of \\(M\\) , i.e. if \\(x\\in \\RR^n\\) and \\(\\alpha\\geq 0\\) , then \\(M(\\alpha x)= \\alpha M(x)\\) . This motivates the following definition: Definition. Let \\(K\\subset C_+\\) be a convex cone, \\(f\\colon K \\to C_+\\) and \\(p\\in \\RR\\) . \\(f\\) is \\(\\textit{order-preserving}\\) if \\(f(x)\\preceq f(y)\\) for every \\(x,y\\in K\\) satisfying \\(x\\preceq y\\) . \\(f\\) is (positively) \\(\\textit{homogeneous of degree}\\) \\(p\\) , if for every \\(x\\in K\\) and every \\(\\alpha\\geq 0\\) , it holds \\(f(\\alpha\\,x)=\\alpha\\, f(x)\\) . If \\(f\\) is homogeneous of degree \\(1\\) , we simply say that \\(f\\) is homogeneous. If \\(f\\) is differentiable on an open set containing \\(C_+\\) , then Theorem 1.3.1 of [NB] implies that \\(f\\) is order-preserving if and only if \\(D f(x)\\in C_+\\) for all \\(x\\in C_+\\) . Similarly, the Euler theorem for homogeneous mappings shows that \\(f\\) is homogeneous of degree \\(p\\) if and only if \\(D f(x) x\\, = \\, p\\, f(x)\\) for all \\(x\\in C_+\\) . If \\(C_+\\subset \\RR^1\\) , then being order-preserving is precisely the same as being increasing. The concept reducing to that of deacreasing functions is termed order-reversing. A mapping \\(f\\colon K\\to C_+\\) is order-reversing, if \\(f(y)\\preceq f(x)\\) for every \\(x,y\\in K\\) satisfying \\(x\\preceq y\\) . We will however mainly discuss order-preserving functions. Example. Let \\(M\\) be a nonnegative matrix with at least one positive entry per row. Let \\(\\alpha\\in \\RR\\) and consider the mapping \\(f_{\\alpha}\\colon C_{++} \\to C_{++}\\) defined as \\[\\begin{equation}\\label{def:f_a} f_{\\alpha}(x) = (Mx)^{\\alpha} \\end{equation}\\] where the power \\(\\alpha\\) is applied component-wise, i.e. \\(y^{\\alpha}=(y_1^{\\alpha},\\ldots,y_n^{\\alpha})^\\top\\) for all \\(y\\in C_{++}\\) . Then, this mapping is homogeneous of degree \\(\\alpha\\) . Furthermore, \\(f_{\\alpha}\\) is order-preserving if \\(\\alpha > 0\\) , order-reversing if \\(\\alpha <0\\) and constant if \\(\\alpha =0\\) . 1 Let \\(T\\in\\RR^{n\\times n \\times n}\\) with \\(T_{ijk}\\geq 0\\) for all \\(i,j,k=1,\\ldots,n\\) and consider the mapping \\(f\\colon C_+\\to C_+\\) defined as \\[\\begin{equation}\\label{def:f_T3} f(x)_i = \\sum_{j,k} T_{ijk}x_jx_k \\qquad \\forall i = 1,\\ldots,n. \\end{equation}\\] Then, this mapping is homogeneous of degree \\(2\\) and order-preserving. Let \\(M\\) be a nonnegative matrix and \\(b\\in C_+\\) . Define \\(f\\colon C_+ \\to C_+\\) by \\(f(x)=Mx+b\\) . Then, \\(f\\) is order-preserving but not homogeneous. Nevertheless, \\(f\\) satisfies \\(\\alpha f(x)\\leq f(\\alpha x)\\) for all \\(\\alpha \\in (0,1)\\) and \\(x\\in C_+\\) . We refer to [\u2026] for further readings on subhomogeneous mappings. Let \\(C_+\\subset \\RR^2\\) and \\(f\\colon C_+\\to C_+\\) defined as \\[\\begin{equation}\\label{def:f_disco} f(x_1,x_2)=\\begin{cases} (x_2 , 0 )^\\top & \\text{if }x_2>0, x_1=0\\\\ (x_1 , x_1)^\\top & \\text{if }x_2=0, x_1>0\\\\ (0,x_2)^\\top & \\text{otherwise} \\end{cases} \\end{equation}\\] Then, \\(f\\) is homogeneous and order-preserving. Let \\(C_+ \\subset \\R^3\\) and \\(f\\colon C_+\\to C_+\\) defined as \\[\\begin{equation}\\label{def:f_max} f(x_1,x_2,x_3)= \\big(\\max\\{x_1,x_2,x_3\\}, \\max\\{\\tfrac{x_1}{2},x_2\\}, \\max\\{\\tfrac{x_2}{2},x_3\\}\\big)^\\top \\end{equation}\\] Then \\(f\\) is continuous, homogeneous and order-preserving. A similar argument as in Equation \\(\\eqref{eq:nonexpansive}\\) yields the following: Lemma. Let \\(K\\in\\{C_+,C_{++}\\}\\) and let \\(f\\colon K \\to C_+\\) be homogeneous of degree \\(p\\in \\RR\\) . Suppose that \\(f\\) is either order-preserving or order-reversing, then \\[ d_H(f(x),f(y))\\leq |p|\\,d_H(x,y)\\qquad \\forall x,y\\in C_{++}.\\] The classical Perron-Frobenius theorem is concerned with eigenvalues and eigenvectors of linear mappings and their spectral radius. The notions of eigenvalues and eigenvectors for a general mapping \\(f\\colon K\\to C_+\\) with \\(K\\subset C_+\\) can be extended as follows: We say that \\(x\\in C_+\\) is an eigenvector of if there exists \\(\\lambda\\geq 0\\) such that \\(f(x)=\\lambda x\\) , \\(\\lambda\\) is called an eigenvalue. However, the notion of spectral radius is more delicate to generalize. Indeed, without further assumptions on \\(f\\) , the spectrum of \\(f\\) (the set of its eigenvalues) can be empty or unbounded. We consider the following definition: Definition. Let \\(f\\colon C_+ \\to C_+\\) be continuous, order-preserving and homogeneous. The spectral radius of \\(f\\) is defined by \\[\\rho(f) = \\lim_{k\\to \\infty} \\|f^k\\|_{C_+}^{1/k},\\] where \\(f^k=f\\circ \\cdots \\circ f\\) is the \\(k\\) -th composition of \\(f\\) with itself and \\(\\|f\\|_{C_+}=\\max\\{\\|f(x)\\| \\colon x\\in C_+, \\|x\\|\\leq 1\\}\\) is the induced norm of \\(f\\) with respect to any norm \\(\\|\\cdot \\|\\) on \\(\\RR^n\\) . The above definition of \\(\\rho(f)\\) is usually referred to as the Bonsall spectral radius of \\(f\\) . Another definition of spectral radii have been proposed, namely the cone spectral radius of \\(f\\) . Nevertheless, it is shown in [specrad] that both definitions are equivalent for continuous homogeneous order-preserving mappings on \\(C_+\\) . Let us further observe that for mappings \\(f\\colon C_+\\to C_+\\) which are homogeneous of degree \\(p\\neq 1\\) , the concept of eigenvalue is less clear as they depend on the scaling of the eigenvector. However, as discussed in the next section, if \\(f\\colon C_{++}\\to C_{++}\\) is order-preserving and homogeneous of degree \\(p<1\\) , then \\(f\\) always have a unique eigenvector. The Perron-Frobenius theorem and the Collatz-Wielandt formula We split the statement of the Perron-Frobenius theorem for homogeneous mappings in several parts in order to facilitate the discussion of the assumptions. First, we state the weak form of the Perron-Frobenius theorem which guarantees that the spectral radius is an egeivalue. We refer to Corollary 5.4.2 in [NB] for a proof. Theorem 1. Let \\(f\\colon C_{+}\\to C_{+}\\) be continuous, homogeneous and order-preserving. There exists \\(u\\in C_{+}, u \\neq 0\\) such that \\(f(u) = \\rho(f) u\\) . ([NB] Corollary 5.4.2) Note that if \\(f\\colon \\RR^n\\to \\RR^n\\) is linear and saisfies \\(f(C_+)\\subset C_+\\) , then \\(f\\) is order-preserving and the matrix associated to \\(f\\) has nononegative entries (in the canonical basis). Hence, if \\(f\\) is linear, the assumption above matches the assumptiong of classical Perron-Frobenius theorem. There are two ways to prove the existence of a positive eigenvector of \\(f\\) . Each follows from a different generalization of the notion of irreducibility (see Section 1). The first is the following: Theorem 2. Let \\(f\\colon C_{+}\\to C_{+}\\) be homogeneous and order-preserving. If for all \\(x\\in C_+\\) , there exists \\(N>0\\) such that \\(\\sum_{k=0}^N f^k(x)\\in C_{++}\\) , then every eigenvector of \\(f\\) lies in \\(C_{++}\\) . The above result has the advantage to guarantee that there is no eigenvectors of \\(f\\) on the boundary of the cone \\(C_+\\) . On the other hand, in most applications involving tensors, it turns out to be more restrictive than the assumption of the following theorem which considers a generalization of the graph associated to a matrix. We recall the following definition from [GaubertGunnar]. Definition. Given \\(f\\colon C_{+}\\to C_{+}\\) homogeneous and order-preserving, consider the graph \\(G(f)=(V,E)\\) with \\(V=\\{1,\\ldots,n\\}\\) and \\((i,j)\\in E\\) if \\[\\lim_{t\\to \\infty} f(\\mathbf{1} + t e_j)_i =\\infty,\\] where \\(\\mathbf{1}\\) is the vector of all ones and \\(e_i\\) is the \\(i\\) -th vector of the canonical basis. Note that if \\(f(x)=Mx\\) for some nonnegative matrix \\(M\\) , then \\(G(f)\\) coincide with the directed graph induced by the positive entries of \\(M\\) (see Section 1). Theorem 3. Let \\(f\\colon C_{++}\\to C_{++}\\) be homogeneous and order-preserving. If \\(G(f)\\) is strongly connected then there exists \\(u\\in C_{++}\\) and \\(\\lambda >0\\) such that \\(f(u)=\\lambda u\\) . This result is a particular case of Theorem 1 in [CaubertGunar]. Example 5.4 of [ourPF] provides a mapping which satisfies the assumptions of Theorem 3 but not those of Theorem 2. For nonnegative tensors, the assumption of Theorem 1 implies that of Theorem 2, but not vice-versa (see [pqFriedland], Lemma \u2026). Regarding uniqueness, another generalization of irreducibility is needed: Theorem 4. Let \\(f\\colon C_{++}\\to C_{++}\\) be homogeneous and order-preserving. Suppose that there exists \\(u\\in C_{++}\\) and \\(\\lambda >0\\) such that \\(f(u)=\\lambda u\\) and \\(f\\) is differentiable at \\(u\\) . If \\(Df(u)\\in\\RR^{n\\times n}\\) is an irreducible matrix, then \\(u\\) is the unique positive eigenvector of \\(f\\) . The assumptions for the convergence of the iterates is similar to the above Theorem: Regarding uniqueness, another generalization of irreducibility is needed: Theorem 4. Let \\(f\\colon C_{++}\\to C_{++}\\) be homogeneous and order-preserving. Suppose that there exists \\(u\\in C_{++}\\) and \\(\\lambda >0\\) such that \\(f(u)=\\lambda u\\) and \\(f\\) is differentiable at \\(u\\) . If \\(Df(u)\\in\\RR^{n\\times n}\\) is a primitive matrix, then for every \\(x^0\\in C_{++}\\) , the sequence \\((x^k)_{k=0}^{\\infty}\\subset C_{+}\\) defined as \\(x^k = f(x^{k-1})\\) for all \\(k\\geq 1\\) satisfies \\[\\lim_{k\\to \\infty} \\frac{x^k}{\\|x^k\\|}= \\frac{u}{\\|u\\|}.\\] Finally, to guarantee the maximality of eigenvalues, one can use the following generalization of the Collatz-Wieland formula: Theorem. Let \\(f\\colon C_{+}\\to C_{+}\\) be order-preserving and homogeneous of degree \\(p\\) with \\(0< p<1\\) . Suppose that \\(f(C_{++})\\subset C_{++}\\) and let \\(\\|\\cdot\\|\\) be a monotonic norm on \\(\\RR^n\\) , i.e. \\(x,y\\in C_+\\) and \\(x\\preceq y\\) imply \\(\\|x\\|\\leq \\|y\\|\\) . Then, there exists \\(u\\in C_{++}\\) and \\(\\lambda>0\\) such that \\(f(u)=\\lambda u\\) , and \\[\\begin{align*} \\rho(A)\\quad &= \\quad\\sup_{x\\in C_{+}^n\\cap B} \\quad\\min_{\\substack{i=1,\\ldots,n\\\\ x_i >0}}\\quad \\frac{f(x)_i}{x_i}\\\\ & =\\quad \\inf_{x\\in C_{++}^n\\cap B}\\quad \\max_{i=1,\\ldots,n}\\quad \\frac{f(x)_i}{x_i},\\notag \\end{align*}\\] where \\(B=\\{x\\colon \\|x\\|=1\\}\\) is the unit ball in \\(\\RR^n\\) . Finally, we discuss the case of contractive mappings. The follwoing result is proved by Bushell in [Bus57] and can be obtained by combining both Lemmas of this section together with the Banach fixed point theorem. Theorem. Let \\(f\\colon C_{++}\\to C_{++}\\) be homogeneous of degree \\(p\\) with \\(|p|<1\\) . Suppose that \\(f\\) is either order-preserving or order-reversing. Let \\(\\|\\cdot\\|\\) be a norm on \\(\\RR^n\\) . There exists a unique eigenvector \\(u\\in C_{++}\\) of \\(f\\) . For every \\(x^0\\in C_{++}\\) , the sequence \\((x^k)_{k=0}^{\\infty}\\subset\\RR^n\\) defined as \\(x^k = f(x^{k-1})\\) for all \\(k\\geq 1\\) satisfies \\[\\lim_{k\\to \\infty} \\frac{x^k}{\\|x^k\\|}= \\frac{u}{\\|u\\|}.\\] Furthermore, for every \\(k\\geq 1\\) , it holds \\[\\begin{align*} \\mu(x^k, u) \\leq \\frac{|p|^k}{1-|p|}\\mu(x^1,x^0). \\end{align*}\\] It is worth remarking the simplicity in the assumptions of the above theorem. For instance, when \\(\\alpha\\in (-1,1)\\) , the mapping \\(f_{\\alpha}\\) of Equation \\(\\eqref{def:f_a}\\) is guaranteed to have a unique positive eigenvector with the only assumption on \\(M\\) that it needs to have at least one positive entry per row. This contrast strongly with the assumptions of the Perron-Frobenius theorem for nonnegative matrices which requires irreducibility. However, the above theorem does not improve the Perron-Frobenius theorem as the linear case would require \\(\\alpha = 1\\) . Another difference between the above theorem and the Perron-Frobenius theorem is that the former adresses mappings defined on \\(C_{++}\\) while the later adresses mappings defined on \\(C_+\\) . Thus, the Perron-Frobenius theorem is more informative in the sense that its uniqueness result imply that there is no nonnegative eigenvector. This may not be the case in the nonlinear case (take for example \\(M\\) to be identity matrix in the definition of \\(f_{\\alpha}\\) ). Furthermore, there is no maximality statement in the above theorem. As discussed above, eigenvalues of mappings which are homogeneous of degree \\(p\\) with \\(p\\neq 1\\) is delicate as the eigenvalue changes with respect to the scaling of the corresponding eigenvector. Therefore, in order to compare eigenvalues of such mappings, we fix the scale of the corresponding eigenvectors. With this observation, the following variant of the Collatz-Wielandt formula for order-preserving contractive mappings implies that if \\(u\\) is a positive eigenvector and \\(v\\) is a nonnegative eigenvector such that \\(\\|u\\|=\\|v\\|\\) and \\(\\|\\cdot\\|\\) is a monotonic norm, then the eigenvalue corresponding to \\(u\\) is at least as large as that corresponding to \\(v\\) . Theorem. Let \\(f\\colon C_{+}\\to C_{+}\\) be order-preserving and homogeneous of degree \\(p\\) with \\(0< p<1\\) . Suppose that \\(f(C_{++})\\subset C_{++}\\) and let \\(\\|\\cdot\\|\\) be a monotonic norm on \\(\\RR^n\\) , i.e. \\(x,y\\in C_+\\) and \\(x\\preceq y\\) imply \\(\\|x\\|\\leq \\|y\\|\\) . Then, there exists \\(u\\in C_{++}\\) and \\(\\lambda>0\\) such that \\(f(u)=\\lambda u\\) , and \\[\\begin{align}\\label{def:cw_down} \\rho(A)\\quad &= \\quad\\sup_{x\\in C_{+}^n\\cap B} \\quad\\min_{\\substack{i=1,\\ldots,n\\\\ x_i >0}}\\quad \\frac{f(x)_i}{x_i}\\\\ & =\\quad \\inf_{x\\in C_{++}^n\\cap B}\\quad \\max_{i=1,\\ldots,n}\\quad \\frac{f(x)_i}{x_i},\\notag \\end{align}\\] where \\(B=\\{x\\colon \\|x\\|=1\\}\\) is the unit ball in \\(\\RR^n\\) . Application: Computing the \\(q\\to p\\) norm of a nonnegative matrix Application: The Sinkhorn method We use the convention \\(t^{0}=1\\) for all \\(t>0\\) . \u21a9","title":"Homogeneous mappings"},{"location":"ch1/sec2/#homogeneous_mappings","text":"","title":"Homogeneous mappings"},{"location":"ch1/sec2/#brief_history_birkhoff_1950_hilbert_metric_nussbaum_homogeneous_mappings_strict_contractivity","text":"","title":"Brief history: Birkhoff (1950): Hilbert metric, Nussbaum: homogeneous mappings, ...: strict contractivity."},{"location":"ch1/sec2/#hilbert_projective_distance_and_the_birkhoff-hopf_theorem","text":"A key observation for the development of the nonlinear Perron-Frobenius theory was made by Birkhoff, when he noticed that nonnegative matrices are non-expansive with respect to the Hilbert metric induced by the cone \\(C_+\\) . The Hilbert (projective) metric \\(d_H\\colon C_+\\times C_+\\to [0,\\infty]\\) is defined by \\(d_H(0,0)=0\\) , \\(d(x,y)=\\infty\\) if \\(x\\not\\sim y\\) , and \\[d_H(x,y) = \\ln\\Big(\\max_{i,j \\in \\mathcal I} \\frac{x_i}{y_i}\\frac{y_j}{x_j}\\Big) \\quad \\text{ where }\\quad \\mathcal I =\\{i\\colon x_i>0\\},\\] for all \\(x,y\\in C_+\\setminus \\{0\\}\\) such that \\(x\\sim y\\) . Technically, \\(d_H\\) is not a metric but a pseudometric since \\(d(x,y)=0\\) may not imply \\(x=y\\) . However, it is a complete metric on the set of unit vectors contained in the same part of \\(C_+\\) . Lemma. Let \\(\\|\\cdot\\|\\) be a norm on \\(\\RR^n\\) and \\(B=\\{x\\in \\RR^n \\colon \\|x\\|=1\\}\\) . For every \\(x,y\\in C_+\\) it holds \\(d_H(x,y)=0\\) if and only if there exists \\(\\lambda >0\\) such that \\(x= \\lambda y\\) . If \\(P\\subset C_+\\) is a part of \\(C_+\\) , then \\(x\\sim y\\) for all \\(x,y\\in P\\) , and \\((P\\cap B, d_H)\\) is a complete metric space. A direct consequence of the first property in the above Lemma is that for any mapping \\(f\\colon C_+\\to C_+\\) and any \\(x\\in C_+\\) , there exists \\(\\lambda >0\\) such that \\(f(x)=\\lambda x\\) if, and only if, \\(d_H(f(x),x)=0\\) . This observation suggest that the Hilbert metric is particularly appropriate to study eigenvectors of mappings leaving a cone invariant. We should put the picture of my talk here. If \\(M\\) is any nonnegative matrix in \\(\\RR^{n\\times n}\\) , then for every \\(x,y\\in C_{++}\\) it holds \\[\\begin{equation}\\label{eq:nonexpansive} Mx \\preceq M \\Big(\\max_{i=1,\\ldots,n}\\frac{x_i}{y_i}\\Big) y = \\Big(\\max_{i=1,\\ldots,n}\\frac{x_i}{y_i}\\Big) My \\end{equation}\\] and therefore, \\(d_H(Mx,My)\\leq d_H(x,y)\\) , i.e. \\(M\\) is nonnexpansive with respect to the Hilbert metric. The Birkhoff-Hopf theorem refines this observation by offering an explicit formula for the smallest Lipschitz constant of a positive matrix with respect to the Hilbert metric. Theorem. (Birkhoff-Hopf) Let \\(M\\in \\RR^{n\\times n}\\) be a positive matrix and let \\[\\kappa(M)=\\inf\\big\\{\\alpha \\geq 0 \\colon d_H(Mx,My)\\leq d_H(x,y), \\forall x,y\\in C_+, x\\sim y\\big\\},\\] then, it holds \\[\\kappa(M)=\\tanh(\\tfrac{1}{4}\\Delta(M)) \\quad \\text{with}\\quad \\Delta(M)=\\max_{i,j,k,\\ell = 1,\\ldots,n} \\frac{M_{ij}M_{k\\ell}}{M_{i\\ell}M_{kj}}\\] The Birkhoff-Hopf theorem holds for considerably more general settings such as linear mappings defined on general cones in finite or infinite dimensional spaces. We refer to [NBproof] for a very elegant proof in the general setting. Note that from the formula for \\(\\Delta(M)\\) one can see that the smallest Lipschitz constant of \\(M\\) with respect to \\(d_H\\) does not depend on the scale of \\(M\\) , i.e. \\(\\Delta(\\alpha M)=\\Delta(M)\\) for all \\(\\alpha >0\\) . Furthermore, it holds \\(\\Delta(M^\\top)=\\Delta(M)\\) . The quantity \\(\\Delta(M)\\) is the projective diameter of the set \\(M(C_+)\\) and can be characterized as \\[ \\Delta(M)=\\sup\\{d_H(Mx,My)\\colon x,y \\in C_+, x\\sim y\\}. \\] As discussed in the following remark, the Birkhoff-Hopf theorem can be used to prove results of the Perron-Frobenius theorem for positive matrices. Furthermore, \\(\\kappa(M)\\) provides a bound on the linear convergence rate of the sequence converging towards a unique positive eigenvector. Remark. Given a positive matrix \\(M\\in\\RR^{n\\times n}\\) , the Perron-Frobenius theorem implies that \\(M\\) has a unique eigenvector \\(u \\in C_{++}\\) with \\(\\|u\\|=1\\) , and the sequence \\(x^{k}=Ax^{k-1}\\) satisfies \\(\\lim_{k\\to \\infty} x^k/\\|x^k\\|=u\\) for every \\(x^0\\in C_{++}\\) . Thanks to the Birkhoff-Hopf theorem, these results can be obtained as a direct consequence of the Banach fixed point theorem. Indeed, let \\(B\\) be the unit ball in \\(\\RR^n\\) . By the precedent Lemma we know that \\((B\\cap C_++,d_H)\\) is a complete metric space. Since the hyperbolic tangent takes values in \\((0,1)\\) , the Birkhoff-Hopf theorem implies that \\(x\\mapsto Mx\\) is a strict contraction with respect to \\(d_H\\) . Finally, note that the scale invariance of the Hilbert metric implies that \\(x\\mapsto Mx\\) and \\(x\\mapsto Mx/\\|Mx\\|\\) have the same Lipschitz constants. Hence, by the Banach fixed point theorem, the latter mapping has a unique fixed point, \\(u\\) , and its iterates converge to \\(u\\) with rate \\(\\kappa(M)\\) . However, it should be noted that the linear convergence rate provided by the Birkhoff-Hopf theorem is not as tight as the classical ratio between the second and first eigenvalues of \\(M\\) . The Hilbert metric can be used to prove a generalization of the Perron-Frobenius theorem to eigenvectors of nonlinear mappings on cones. We discuss the class of such mappings in the next section.","title":"Hilbert projective distance and the Birkhoff-Hopf theorem"},{"location":"ch1/sec2/#homogeneous_and_order-preserving_mappings","text":"To prove that nonnegative matrices are nonexpansive with respect to \\(d_H\\) in Equation \\(\\eqref{eq:nonexpansive}\\) , we have used two important properties of the nonnegative matrix \\(M\\) : First, we have used the fact that for every \\(x,y\\in C_+\\) satisfying \\(x\\preceq y\\) it holds \\(Mx\\preceq My\\) and second, we have used the homogeneity of \\(M\\) , i.e. if \\(x\\in \\RR^n\\) and \\(\\alpha\\geq 0\\) , then \\(M(\\alpha x)= \\alpha M(x)\\) . This motivates the following definition: Definition. Let \\(K\\subset C_+\\) be a convex cone, \\(f\\colon K \\to C_+\\) and \\(p\\in \\RR\\) . \\(f\\) is \\(\\textit{order-preserving}\\) if \\(f(x)\\preceq f(y)\\) for every \\(x,y\\in K\\) satisfying \\(x\\preceq y\\) . \\(f\\) is (positively) \\(\\textit{homogeneous of degree}\\) \\(p\\) , if for every \\(x\\in K\\) and every \\(\\alpha\\geq 0\\) , it holds \\(f(\\alpha\\,x)=\\alpha\\, f(x)\\) . If \\(f\\) is homogeneous of degree \\(1\\) , we simply say that \\(f\\) is homogeneous. If \\(f\\) is differentiable on an open set containing \\(C_+\\) , then Theorem 1.3.1 of [NB] implies that \\(f\\) is order-preserving if and only if \\(D f(x)\\in C_+\\) for all \\(x\\in C_+\\) . Similarly, the Euler theorem for homogeneous mappings shows that \\(f\\) is homogeneous of degree \\(p\\) if and only if \\(D f(x) x\\, = \\, p\\, f(x)\\) for all \\(x\\in C_+\\) . If \\(C_+\\subset \\RR^1\\) , then being order-preserving is precisely the same as being increasing. The concept reducing to that of deacreasing functions is termed order-reversing. A mapping \\(f\\colon K\\to C_+\\) is order-reversing, if \\(f(y)\\preceq f(x)\\) for every \\(x,y\\in K\\) satisfying \\(x\\preceq y\\) . We will however mainly discuss order-preserving functions. Example. Let \\(M\\) be a nonnegative matrix with at least one positive entry per row. Let \\(\\alpha\\in \\RR\\) and consider the mapping \\(f_{\\alpha}\\colon C_{++} \\to C_{++}\\) defined as \\[\\begin{equation}\\label{def:f_a} f_{\\alpha}(x) = (Mx)^{\\alpha} \\end{equation}\\] where the power \\(\\alpha\\) is applied component-wise, i.e. \\(y^{\\alpha}=(y_1^{\\alpha},\\ldots,y_n^{\\alpha})^\\top\\) for all \\(y\\in C_{++}\\) . Then, this mapping is homogeneous of degree \\(\\alpha\\) . Furthermore, \\(f_{\\alpha}\\) is order-preserving if \\(\\alpha > 0\\) , order-reversing if \\(\\alpha <0\\) and constant if \\(\\alpha =0\\) . 1 Let \\(T\\in\\RR^{n\\times n \\times n}\\) with \\(T_{ijk}\\geq 0\\) for all \\(i,j,k=1,\\ldots,n\\) and consider the mapping \\(f\\colon C_+\\to C_+\\) defined as \\[\\begin{equation}\\label{def:f_T3} f(x)_i = \\sum_{j,k} T_{ijk}x_jx_k \\qquad \\forall i = 1,\\ldots,n. \\end{equation}\\] Then, this mapping is homogeneous of degree \\(2\\) and order-preserving. Let \\(M\\) be a nonnegative matrix and \\(b\\in C_+\\) . Define \\(f\\colon C_+ \\to C_+\\) by \\(f(x)=Mx+b\\) . Then, \\(f\\) is order-preserving but not homogeneous. Nevertheless, \\(f\\) satisfies \\(\\alpha f(x)\\leq f(\\alpha x)\\) for all \\(\\alpha \\in (0,1)\\) and \\(x\\in C_+\\) . We refer to [\u2026] for further readings on subhomogeneous mappings. Let \\(C_+\\subset \\RR^2\\) and \\(f\\colon C_+\\to C_+\\) defined as \\[\\begin{equation}\\label{def:f_disco} f(x_1,x_2)=\\begin{cases} (x_2 , 0 )^\\top & \\text{if }x_2>0, x_1=0\\\\ (x_1 , x_1)^\\top & \\text{if }x_2=0, x_1>0\\\\ (0,x_2)^\\top & \\text{otherwise} \\end{cases} \\end{equation}\\] Then, \\(f\\) is homogeneous and order-preserving. Let \\(C_+ \\subset \\R^3\\) and \\(f\\colon C_+\\to C_+\\) defined as \\[\\begin{equation}\\label{def:f_max} f(x_1,x_2,x_3)= \\big(\\max\\{x_1,x_2,x_3\\}, \\max\\{\\tfrac{x_1}{2},x_2\\}, \\max\\{\\tfrac{x_2}{2},x_3\\}\\big)^\\top \\end{equation}\\] Then \\(f\\) is continuous, homogeneous and order-preserving. A similar argument as in Equation \\(\\eqref{eq:nonexpansive}\\) yields the following: Lemma. Let \\(K\\in\\{C_+,C_{++}\\}\\) and let \\(f\\colon K \\to C_+\\) be homogeneous of degree \\(p\\in \\RR\\) . Suppose that \\(f\\) is either order-preserving or order-reversing, then \\[ d_H(f(x),f(y))\\leq |p|\\,d_H(x,y)\\qquad \\forall x,y\\in C_{++}.\\] The classical Perron-Frobenius theorem is concerned with eigenvalues and eigenvectors of linear mappings and their spectral radius. The notions of eigenvalues and eigenvectors for a general mapping \\(f\\colon K\\to C_+\\) with \\(K\\subset C_+\\) can be extended as follows: We say that \\(x\\in C_+\\) is an eigenvector of if there exists \\(\\lambda\\geq 0\\) such that \\(f(x)=\\lambda x\\) , \\(\\lambda\\) is called an eigenvalue. However, the notion of spectral radius is more delicate to generalize. Indeed, without further assumptions on \\(f\\) , the spectrum of \\(f\\) (the set of its eigenvalues) can be empty or unbounded. We consider the following definition: Definition. Let \\(f\\colon C_+ \\to C_+\\) be continuous, order-preserving and homogeneous. The spectral radius of \\(f\\) is defined by \\[\\rho(f) = \\lim_{k\\to \\infty} \\|f^k\\|_{C_+}^{1/k},\\] where \\(f^k=f\\circ \\cdots \\circ f\\) is the \\(k\\) -th composition of \\(f\\) with itself and \\(\\|f\\|_{C_+}=\\max\\{\\|f(x)\\| \\colon x\\in C_+, \\|x\\|\\leq 1\\}\\) is the induced norm of \\(f\\) with respect to any norm \\(\\|\\cdot \\|\\) on \\(\\RR^n\\) . The above definition of \\(\\rho(f)\\) is usually referred to as the Bonsall spectral radius of \\(f\\) . Another definition of spectral radii have been proposed, namely the cone spectral radius of \\(f\\) . Nevertheless, it is shown in [specrad] that both definitions are equivalent for continuous homogeneous order-preserving mappings on \\(C_+\\) . Let us further observe that for mappings \\(f\\colon C_+\\to C_+\\) which are homogeneous of degree \\(p\\neq 1\\) , the concept of eigenvalue is less clear as they depend on the scaling of the eigenvector. However, as discussed in the next section, if \\(f\\colon C_{++}\\to C_{++}\\) is order-preserving and homogeneous of degree \\(p<1\\) , then \\(f\\) always have a unique eigenvector.","title":"Homogeneous and order-preserving mappings"},{"location":"ch1/sec2/#the_perron-frobenius_theorem_and_the_collatz-wielandt_formula","text":"We split the statement of the Perron-Frobenius theorem for homogeneous mappings in several parts in order to facilitate the discussion of the assumptions. First, we state the weak form of the Perron-Frobenius theorem which guarantees that the spectral radius is an egeivalue. We refer to Corollary 5.4.2 in [NB] for a proof. Theorem 1. Let \\(f\\colon C_{+}\\to C_{+}\\) be continuous, homogeneous and order-preserving. There exists \\(u\\in C_{+}, u \\neq 0\\) such that \\(f(u) = \\rho(f) u\\) . ([NB] Corollary 5.4.2) Note that if \\(f\\colon \\RR^n\\to \\RR^n\\) is linear and saisfies \\(f(C_+)\\subset C_+\\) , then \\(f\\) is order-preserving and the matrix associated to \\(f\\) has nononegative entries (in the canonical basis). Hence, if \\(f\\) is linear, the assumption above matches the assumptiong of classical Perron-Frobenius theorem. There are two ways to prove the existence of a positive eigenvector of \\(f\\) . Each follows from a different generalization of the notion of irreducibility (see Section 1). The first is the following: Theorem 2. Let \\(f\\colon C_{+}\\to C_{+}\\) be homogeneous and order-preserving. If for all \\(x\\in C_+\\) , there exists \\(N>0\\) such that \\(\\sum_{k=0}^N f^k(x)\\in C_{++}\\) , then every eigenvector of \\(f\\) lies in \\(C_{++}\\) . The above result has the advantage to guarantee that there is no eigenvectors of \\(f\\) on the boundary of the cone \\(C_+\\) . On the other hand, in most applications involving tensors, it turns out to be more restrictive than the assumption of the following theorem which considers a generalization of the graph associated to a matrix. We recall the following definition from [GaubertGunnar]. Definition. Given \\(f\\colon C_{+}\\to C_{+}\\) homogeneous and order-preserving, consider the graph \\(G(f)=(V,E)\\) with \\(V=\\{1,\\ldots,n\\}\\) and \\((i,j)\\in E\\) if \\[\\lim_{t\\to \\infty} f(\\mathbf{1} + t e_j)_i =\\infty,\\] where \\(\\mathbf{1}\\) is the vector of all ones and \\(e_i\\) is the \\(i\\) -th vector of the canonical basis. Note that if \\(f(x)=Mx\\) for some nonnegative matrix \\(M\\) , then \\(G(f)\\) coincide with the directed graph induced by the positive entries of \\(M\\) (see Section 1). Theorem 3. Let \\(f\\colon C_{++}\\to C_{++}\\) be homogeneous and order-preserving. If \\(G(f)\\) is strongly connected then there exists \\(u\\in C_{++}\\) and \\(\\lambda >0\\) such that \\(f(u)=\\lambda u\\) . This result is a particular case of Theorem 1 in [CaubertGunar]. Example 5.4 of [ourPF] provides a mapping which satisfies the assumptions of Theorem 3 but not those of Theorem 2. For nonnegative tensors, the assumption of Theorem 1 implies that of Theorem 2, but not vice-versa (see [pqFriedland], Lemma \u2026). Regarding uniqueness, another generalization of irreducibility is needed: Theorem 4. Let \\(f\\colon C_{++}\\to C_{++}\\) be homogeneous and order-preserving. Suppose that there exists \\(u\\in C_{++}\\) and \\(\\lambda >0\\) such that \\(f(u)=\\lambda u\\) and \\(f\\) is differentiable at \\(u\\) . If \\(Df(u)\\in\\RR^{n\\times n}\\) is an irreducible matrix, then \\(u\\) is the unique positive eigenvector of \\(f\\) . The assumptions for the convergence of the iterates is similar to the above Theorem: Regarding uniqueness, another generalization of irreducibility is needed: Theorem 4. Let \\(f\\colon C_{++}\\to C_{++}\\) be homogeneous and order-preserving. Suppose that there exists \\(u\\in C_{++}\\) and \\(\\lambda >0\\) such that \\(f(u)=\\lambda u\\) and \\(f\\) is differentiable at \\(u\\) . If \\(Df(u)\\in\\RR^{n\\times n}\\) is a primitive matrix, then for every \\(x^0\\in C_{++}\\) , the sequence \\((x^k)_{k=0}^{\\infty}\\subset C_{+}\\) defined as \\(x^k = f(x^{k-1})\\) for all \\(k\\geq 1\\) satisfies \\[\\lim_{k\\to \\infty} \\frac{x^k}{\\|x^k\\|}= \\frac{u}{\\|u\\|}.\\] Finally, to guarantee the maximality of eigenvalues, one can use the following generalization of the Collatz-Wieland formula: Theorem. Let \\(f\\colon C_{+}\\to C_{+}\\) be order-preserving and homogeneous of degree \\(p\\) with \\(0< p<1\\) . Suppose that \\(f(C_{++})\\subset C_{++}\\) and let \\(\\|\\cdot\\|\\) be a monotonic norm on \\(\\RR^n\\) , i.e. \\(x,y\\in C_+\\) and \\(x\\preceq y\\) imply \\(\\|x\\|\\leq \\|y\\|\\) . Then, there exists \\(u\\in C_{++}\\) and \\(\\lambda>0\\) such that \\(f(u)=\\lambda u\\) , and \\[\\begin{align*} \\rho(A)\\quad &= \\quad\\sup_{x\\in C_{+}^n\\cap B} \\quad\\min_{\\substack{i=1,\\ldots,n\\\\ x_i >0}}\\quad \\frac{f(x)_i}{x_i}\\\\ & =\\quad \\inf_{x\\in C_{++}^n\\cap B}\\quad \\max_{i=1,\\ldots,n}\\quad \\frac{f(x)_i}{x_i},\\notag \\end{align*}\\] where \\(B=\\{x\\colon \\|x\\|=1\\}\\) is the unit ball in \\(\\RR^n\\) . Finally, we discuss the case of contractive mappings. The follwoing result is proved by Bushell in [Bus57] and can be obtained by combining both Lemmas of this section together with the Banach fixed point theorem. Theorem. Let \\(f\\colon C_{++}\\to C_{++}\\) be homogeneous of degree \\(p\\) with \\(|p|<1\\) . Suppose that \\(f\\) is either order-preserving or order-reversing. Let \\(\\|\\cdot\\|\\) be a norm on \\(\\RR^n\\) . There exists a unique eigenvector \\(u\\in C_{++}\\) of \\(f\\) . For every \\(x^0\\in C_{++}\\) , the sequence \\((x^k)_{k=0}^{\\infty}\\subset\\RR^n\\) defined as \\(x^k = f(x^{k-1})\\) for all \\(k\\geq 1\\) satisfies \\[\\lim_{k\\to \\infty} \\frac{x^k}{\\|x^k\\|}= \\frac{u}{\\|u\\|}.\\] Furthermore, for every \\(k\\geq 1\\) , it holds \\[\\begin{align*} \\mu(x^k, u) \\leq \\frac{|p|^k}{1-|p|}\\mu(x^1,x^0). \\end{align*}\\] It is worth remarking the simplicity in the assumptions of the above theorem. For instance, when \\(\\alpha\\in (-1,1)\\) , the mapping \\(f_{\\alpha}\\) of Equation \\(\\eqref{def:f_a}\\) is guaranteed to have a unique positive eigenvector with the only assumption on \\(M\\) that it needs to have at least one positive entry per row. This contrast strongly with the assumptions of the Perron-Frobenius theorem for nonnegative matrices which requires irreducibility. However, the above theorem does not improve the Perron-Frobenius theorem as the linear case would require \\(\\alpha = 1\\) . Another difference between the above theorem and the Perron-Frobenius theorem is that the former adresses mappings defined on \\(C_{++}\\) while the later adresses mappings defined on \\(C_+\\) . Thus, the Perron-Frobenius theorem is more informative in the sense that its uniqueness result imply that there is no nonnegative eigenvector. This may not be the case in the nonlinear case (take for example \\(M\\) to be identity matrix in the definition of \\(f_{\\alpha}\\) ). Furthermore, there is no maximality statement in the above theorem. As discussed above, eigenvalues of mappings which are homogeneous of degree \\(p\\) with \\(p\\neq 1\\) is delicate as the eigenvalue changes with respect to the scaling of the corresponding eigenvector. Therefore, in order to compare eigenvalues of such mappings, we fix the scale of the corresponding eigenvectors. With this observation, the following variant of the Collatz-Wielandt formula for order-preserving contractive mappings implies that if \\(u\\) is a positive eigenvector and \\(v\\) is a nonnegative eigenvector such that \\(\\|u\\|=\\|v\\|\\) and \\(\\|\\cdot\\|\\) is a monotonic norm, then the eigenvalue corresponding to \\(u\\) is at least as large as that corresponding to \\(v\\) . Theorem. Let \\(f\\colon C_{+}\\to C_{+}\\) be order-preserving and homogeneous of degree \\(p\\) with \\(0< p<1\\) . Suppose that \\(f(C_{++})\\subset C_{++}\\) and let \\(\\|\\cdot\\|\\) be a monotonic norm on \\(\\RR^n\\) , i.e. \\(x,y\\in C_+\\) and \\(x\\preceq y\\) imply \\(\\|x\\|\\leq \\|y\\|\\) . Then, there exists \\(u\\in C_{++}\\) and \\(\\lambda>0\\) such that \\(f(u)=\\lambda u\\) , and \\[\\begin{align}\\label{def:cw_down} \\rho(A)\\quad &= \\quad\\sup_{x\\in C_{+}^n\\cap B} \\quad\\min_{\\substack{i=1,\\ldots,n\\\\ x_i >0}}\\quad \\frac{f(x)_i}{x_i}\\\\ & =\\quad \\inf_{x\\in C_{++}^n\\cap B}\\quad \\max_{i=1,\\ldots,n}\\quad \\frac{f(x)_i}{x_i},\\notag \\end{align}\\] where \\(B=\\{x\\colon \\|x\\|=1\\}\\) is the unit ball in \\(\\RR^n\\) .","title":"The Perron-Frobenius theorem and the Collatz-Wielandt formula"},{"location":"ch1/sec2/#application_computing_the_qto_p_norm_of_a_nonnegative_matrix","text":"","title":"Application: Computing the \\(q\\to p\\) norm of a nonnegative matrix"},{"location":"ch1/sec2/#application_the_sinkhorn_method","text":"We use the convention \\(t^{0}=1\\) for all \\(t>0\\) . \u21a9","title":"Application: The Sinkhorn method"},{"location":"ch1/sec3/","text":"Multihomogeneous mappings","title":"Multihomogeneous mappings"},{"location":"ch1/sec3/#multihomogeneous_mappings","text":"","title":"Multihomogeneous mappings"},{"location":"ch2/centrality/","text":"Network centrality Ranking the nodes of a network according to suitable \u201ccentrality measures\u201d is a recurring and fundamental question in network science and data mining. Among the various network centrality models, the class of eigenvector centrality is one of the most widely used and effective. This family of models dates back to the 19th Century when it was proposed as a mean to rank professional chess players by Edmund Landau 1 and was then popularized in the network science community starting from the late \u201980s with Bonacich 2 , PageRank 3 and HITS 4 models. This class of scores assigns importances to the nodes of a graph, based on the leading eigenvector \\(x\\) (or the leading singular vectors \\(x,y\\) ) of suitable network matrices and strongly rely on the matrix Perron-Frobenius theorem. One of the keys of the success of eigenvector centralities is that they naturally incorporate mutual reinforcement : important objects are those that interact with many other important objects. To clarify this idea, we review below two widely used centrality models (we consider the case of binary \u201cunweighted\u201d graphs for simplicity, but everything transfer with minor adjustments to the weighted setting.) Bonacich centrality Let \\(G=(V,E)\\) be a graph with adjacency matrix \\[ A_{ij} = \\begin{cases}1 & j\\to i \\\\ 0 & \\text{otherwise} \\end{cases} \\quad , \\] where \\(j\\to i\\) means that there is an edge in \\(G\\) going from \\(j\\) to \\(i\\) . Bonacich centrality model defines the importance \\(x_i\\) of node \\(i\\in V\\) as \\[ x_i\\propto \\sum_{j: \\, j\\to i} x_j = (Ax)_i \\, , \\] that is, the importance of node \\(x_i\\) is linearly proportional to the importances \\(x_j\\) of the nodes that points towards \\(i\\) . The Perron-Frobenius theory teaches us that, if the graph is strongly connected , only one such vector \\(x\\) exists, the Perron eigenvector of the adjacency matrix \\(A x=\\rho(A)x\\) , and it provides us with sufficient conditions to compute such \\(x\\) via the simple Power Method scheme. HITS centrality Another widely used approach defines two centralities indices, the hub score and the authority score , via the mutual reinforcing informal idea that \u201ca node is a good hub if it points to good authorities; and a node is a good authority if it is pointed by good hubs\u201d . If \\(x_i\\) and \\(y_i\\) are the hub and the authority scores of node \\(i\\) , respectively, then \\[\\begin{equation}\\label{eq:sing-vec} x_i \\propto \\sum_{j: i\\to j}y_j = (A^\\top y)_i \\qquad y_i \\propto \\sum_{j:j\\to i} x_j = (Ax)_i \\end{equation}\\] Again, by the Perron-Frobenius theorem, if the graph is strongly connected there is only one nonnegative solution, the dominant left and right singular vectors of the adjacency matrix. Nonlinear eigenvector centrality While powerful and useful, these models have two main drawbacks: they only allow for linear proportionality relations to define the importance model they may not be unique, even for simple graphs The nonlinear Perron-Frobenius theory allows us to overcome both these two drawbacks in a very natural way. This will be particularly important when moving to the case of higher-order networks , as we will discuss next. Here we discuss two simple illustrative examples. Suppose \\(A=I\\) is the identity matrix. This is certainly not irreducible and in fact \\(Ax = x\\) for all nonnegative vectors \\(x\\in C_+\\) . While from a linear algebra point of view there is no preferred nonnegative solution to \\(x=Ax\\) , from the graph centrality perspective this is not the case. The graph of \\(A\\) is a set of isolated nodes, each with a self-loop of weight exactly one. Thus, a centrality score for these nodes would assign same score to all the nodes. This corresponds to the eigenvector \\(x = \\one\\) of \\(A\\) . While this is only one of the nonnegative solutions of \\(x=Ax\\) , \\(\\one\\) is the only nonlinear Perron eigenvector of a \u201cnonlinear version\u201d of the eigenvalue problem for \\(A\\) . Precisely, consider the nonlinear eigenvalue equation \\[\\begin{equation}\\label{eq:Af(x)} \\lambda x = A x^{1-\\varepsilon} \\end{equation}\\] It is easy to verify that the mapping \\(F(x) := Ax^{1-\\varepsilon}\\) is (multi)homogeneous with homogeneity matrix the scalar \\(\\M=1-\\varepsilon\\) . Thus, for any \\(\\varepsilon \\in (0,1)\\) , \\(\\rho(\\M)<1\\) and by the nonlinear Perron-Frobenius theorem we have that \\(\\eqref{eq:Af(x)}\\) has a unique positive solution. It is easy to verify that such solution is entrywise constant, yielding the desired centrality assignment. A similar situation holds for the singular vector case. Consider the graph in the figure below: This is a well-known example where HITS centrality may fail to output a reasonable centrality. While this graph is not a strongly connected graph, we all most probably agree that node \\(1\\) is the most relevant hub while node \\(6\\) is the most relevant authority . Despite this simple setting, using the dominant singular vectors of \\(A\\) as in the HITS model may fail to identify these relevant hub and authority nodes. In fact, both the following pairs \\[\\begin{align*} x = (1,1,1,1,1, 0) \\qquad y = (0,1,1,1,1,4) \\\\ x = (4, 1, 1, 1, 1, 0) \\qquad y =(0,1,1,1,1,1) \\end{align*}\\] are (up to scaling) singular pairs of the dominant singular value of \\(A\\) . This shows that, in the first case, the hub vector fails to detect that node \\(1\\) is a better hub than nodes \\(2\\) \u2212 \\(5\\) . Similarly, in the second case, the authority vector fails to identify node \\(6\\) as a the best authority. Also in this case, a small nonlinear modification of \\(\\eqref{eq:sing-vec}\\) solves this issue. Consider the singular vector equation \\[\\begin{equation}\\label{eq:nonlin-sing-vec} \\lambda x = A^\\top y^\\alpha \\quad \\mu y = A x^\\beta\\, . \\end{equation}\\] It is easy to see that any such pair of vectors is an eigenvector of a multihomogeneous mapping with homogeneity matrix \\[ \\M = \\begin{bmatrix}0 & \\alpha \\\\ \\beta & 0 \\end{bmatrix}, \\qquad \\rho(H) = \\sqrt{|\\alpha\\beta|} \\] Thus for any \\(\\alpha\\beta<1\\) the nonlinear eigenvector equation \\(\\eqref{eq:nonlin-sing-vec}\\) has a unique solution. Below we show the value of the two solution vectors \\(x,y\\) for different choices of \\(\\alpha\\) and \\(\\beta\\) , showing that these two vectors capture the actual roles of nodes in this example graph. 1 2 3 4 5 6 7 8 9 \u2502 \u2502 \u03b1 = 0.5 \u2502 \u03b1 = 0.9 \u2502 \u03b1 = 0.5 \u2502 \u03b1 = 0.9 \u2502 \u03b2 = 0.5 \u2502 \u03b2 = 0.9 \u2502 \u03b2 = 0.9 \u2502 \u03b2 = 0.5 \u2502 \u2502entry\u2502 x1 \u2502 x2 \u2502 x3 \u2502 x4 \u2502 y1 \u2502 y2 \u2502 y3 \u2502 y4 \u2502 \u251c-----\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502 \u2502 1 \u2502 0.386488 \u2502 0.341489 \u2502 0.468535 \u2502 0.243379 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 \u2502 2 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 3 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 4 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 5 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 6 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 0.386488 \u2502 0.341489 \u2502 0.243379 \u2502 0.468535 \u2502 Higher-order network models While graph and networks are ubiquitous in the natural sciences, in many real-world applications we are confronted with higher-order interaction data. Relational data is full of interactions that happen in groups. For example, friendship relations very often happen in groups that are strictly larger than two individuals. Moreover, interactions naturally occur on multiple layers, for example work relations, sport relations, friendship relations, etc. To model higher-order interactions we need higher-order network models, which include multilayer networks , where we have a set of networks (so-called layers) with connections internally and across the layers, and non-dyadic networks , such as hypergraphs or simplicial complexes, where we have access to interactions involving multiple nodes. For simplicity, here we consider the following two settings: Multiplex : This is a set \\(\\{G_i\\}\\) of \\(m\\) graphs \\(G_i=(V,E_i)\\) \\(i=1,\\dots,m\\) , the layers , each defined on the same set of nodes but with possibly different edge sets Hypergraph : Just like a standard graph, this is a pair \\(H=(V,\\mathcal E)\\) where the set of hyperedges \\(\\mathcal E\\) is such that each \\(e\\in \\mathcal E\\) can involve an arbitrary number of nodes, rather than just two nodes as in the standard graph case. Eigenvector centrality for multiplex A multiplex network \\(\\{G_k\\}\\) can be naturally described by means of an adjacency tensor \\(T\\) with three modes \\[ T_{ijk} = \\begin{cases} 1 & i\\to j \\text{ on layer }k \\\\ 0 & \\text{otherwise} \\end{cases} \\] where \\(i\\to j\\) means that there is an edge from node \\(i\\) to node \\(j\\) , i.e. \\((i,j)\\in E_k\\) . How can we define a mutual reinforcing centrality score for nodes (and layers) in \\(\\{G_k\\}\\) so that a larger centrality is assigned to nodes that form links with other central nodes in highly influential layers? Here we discuss the model proposed in 5 based on \\(T\\) and a multihomogeneous order-preserving mapping associated to it. Other approaches are discussed in #Related work . In this model, mutual reinforcement happens at both layer and node levels, as layers are more influential if highly central nodes are active in them. Thus, if \\(x_i\\) denotes the centrality of node \\(i\\) and \\(y_k\\) the influence of layer \\(k\\) , we require that \\[\\begin{equation}\\label{eq:tensor_singvec} \\left\\{ \\begin{array}{l} \\sum_{j,k}T_{i,j,k}x_jy_k = \\lambda \\, |x_i|^{p} \\mathrm{sign}(x_i)\\\\[.5em] \\sum_{i,j}T_{i,j,k}x_ix_j = \\mu\\, |y_k|^{q} \\mathrm{sign}(y_k) \\end{array} \\right. \\end{equation}\\] which imposes that the \\(p\\) -power of the importance of node \\(i\\) is proportional to the sum of the importances of the nodes that point at \\(i\\) , times the influence of the layer where such connections take place and, similarly, defines the \\(q\\) -power of the influence of the layer \\(k\\) as being proportional the product of the centrality of the nodes that are connected in that layer. Since the centrality score is a positive number it is natural to add the constraint \\(\\lambda,\\mu>0\\) and \\(x,y\\succ 0\\) in \\(\\eqref{eq:tensor_singvec}\\) . Thus, our centrality problem boils down to a constrained nonlinear system of equations. When \\(p=q=1\\) , the equations are homogeneous polynomials and \\(\\eqref{eq:tensor_singvec}\\) is directly reminiscent of a singular vector equation. However, unlike the matrix case, even positive tensors may admit multiple solutions here, as shown by the following example: Consider the positive adjacency tensor \\(T\\) with entries \\[ \\left\\{ \\begin{array}{llll} T_{1,1,1} = 6 & T_{1,2,1} = 199/7 & T_{2,1,1} = 16/7 & T_{2,2,1} = 11 \\\\ T_{1,1,2} = 61/7 & T_{1,2,2} = 6 & T_{2,1,2} = 29 & T_{2,2,2} = 16/7 \\end{array}\\right. \\] then both the pair \\(x = \\frac 1 3(2,1)\\) , \\(y = \\frac 1 3(1,2)\\) and the pair \\(x = \\frac 1 4(1,3)\\) , \\(y = \\frac 1 4(3,1)\\) are positive solutions to \\(\\eqref{eq:tensor_singvec}\\) . In order to practically use the centrality model \\(\\eqref{eq:tensor_singvec}\\) we need it to define a unique score. To this end, we recast the vectors \\(x,y\\) solutions of \\(\\eqref{eq:tensor_singvec}\\) as eigenvectors of the multihomogeneous mapping \\[ F(x,y) := \\begin{bmatrix} F_1(x,y) \\\\ F_2(x,y) \\end{bmatrix} = \\begin{bmatrix} (T_{(1)}xy)^{1/p} \\\\ (T_{(3)}xx)^{1/q} \\end{bmatrix} = \\begin{bmatrix} (\\sum_{jk} T_{ijk} x_jy_k)^{1/p} \\\\ (\\sum_{ij}T_{ijk}x_ix_j)^{1/q} \\end{bmatrix}\\, . \\] In fact, it is easy to see that \\(\\eqref{eq:tensor_singvec}\\) holds iff \\(F(x,y)= (\\lambda,\\mu)\\krog (x,y)\\) . Moreover, one easily realizes that \\[ \\M = \\begin{bmatrix} 1/p & 1/p \\\\ 2/q & 0 \\end{bmatrix} \\] is the homogeneity matrix of \\(F\\) . Thus, using a slightly different cone than \\(C_+\\) , one obtains the following: Theorem. Let \\(n\\) and \\(\\ell\\) be the number of nodes and the number of layers in \\(\\{G_k\\}\\) , respectively. Consider the following set of pairs of vectors \\[ C_+(T) =\\left\\{(x,y)\\succeq 0 \\quad : \\quad \\begin{array}{c} x_i \\propto \\textstyle{\\sum_{j,k} T_{ijk}}, \\quad \\forall i = 1, \\dots, n\\\\ y_k \\propto \\textstyle{\\sum_{i,j} T_{ijk}}, \\quad \\forall k =1,\\dots, \\ell \\end{array}\\right\\}\\, . \\] This is a cone of vectors that depends on the nonzero pattern of \\(T\\) : for \\((x,y)\\in C_+(T)\\) , \\(x_i\\) is zero if and only if \\(T_{ijk}=0\\) for all \\(j,k\\) , that is node \\(i\\) is isolated in all the layers and, similarly, \\(y_k=0\\) if and only if layer \\(k\\) is empty (there is no edge in that layer). Then \\(\\rho(\\M) = \\frac{\\sqrt{8p+q}+\\sqrt{q}}{2p\\sqrt{q}}\\) and if \\(\\rho(\\M)<1\\) we have: 1. The system of nonlinear equations \\(\\eqref{eq:tensor_singvec}\\) has a unique solution \\((x^*,y^*)\\in C_+(T)\\) such that \\(\\|x^*\\|=\\|y^*\\|=1\\) . 2. The power method iteration x = ones ( n , 1 ) y = ones ( l , 1 ) for r = 0 , 1 , 2 , 3 , .. x = F_1 ( x , y ) x = x / norm ( x ) y = F_2 ( x , y ) y = y / norm ( y ) converges to \\((x^*,y^*)\\) and after \\(m\\) steps we have \\[ \\|(x,y)-(x^*,y^*)\\|_{\\infty} \\leq \\rho(\\M)^m \\Big\\{ \\rho(\\M) \\, p\\, \\frac{\\max_{i\\in\\mathcal I} x_i^*}{\\min_{i'\\in\\mathcal I} x_{i'}^*}+\\, \\frac{\\max_{k\\in\\mathcal J} y_k^*}{\\min_{k'\\in\\mathcal J} y_{k'}^*}\\Big\\} \\] with \\(\\mathcal I=\\{i \\, : \\, x^{*}_i>0\\}\\) and \\(\\mathcal J =\\{k \\, :\\, y_k^{*}>0\\}\\) . Related work Other models for eigenvector centrality on multiplex networks are available. Many of them are based on a \u201cflattening\u201d or \u201cprojection\u201d approach which essentially transforms the multiplex into a graph and uses matrix eigenvectors on that graph to model node importances. These include, the aggregated graph of a multiplex graph 6 7 8 9 , where multiple layers with adjacency matrices \\(A_1,\\dots, A_m\\) are aggregated via a linear (possibly weighted) combination into a single denser adjacency matrix \\(A_{\\mathrm{agg}} = w_1A_1 + \\dots + w_mA_m\\) ; the supra-adjacency graph of a multilayer graph 10 11 12 , where a new graph of larger size is built by taking the Cartesian product of all the layers with weighted layer couplings obtaining a large adjacency matrix of the form \\[A_{\\mathrm{supra}}= \\begin{bmatrix} A_1 & I & \\dots & I \\\\ I & \\ddots & \\ddots &\\vdots \\\\ \\vdots & \\ddots & \\ddots & I \\\\ I & \\dots & I & A_m \\end{bmatrix} \\] Once a standard graph is obtained from the multiplex data, standard techniques can be applied to define and compute mutual-reinforcing centralities. In particular, the classical Perron\u2013Frobenius theory for matrices can be directly applied to the flattened graphs. To this end, it is interesting to notice that \\[ A_{\\mathrm{supra}} \\text{ is irreducible } \\iff A_{\\mathrm{agg}} \\text{ is irreducbile} \\] A different approach is proposed in 13 where the centrality for nodes and layers is computed by summing up powers of entries of the incidence matrix of the multiplex. Nonlinear eigenvector centrality for hypergraphs A hypergraph \\(H=(V,E)\\) consists of a set of nodes \\(V\\) and a set of hyperedges \\(E\\) , but, unlike graphs, an hyperedge \\(e\\in E\\) can contain an arbitrary number of nodes. In the weighted setting, we assume a weight function \\(w:E\\to \\RR_+\\) that assigns the weight \\(w(e)>0\\) to each hyperedge. Also in this setting, a relatively standard way to extend graph mappings and their eigenvectors is via a \u201cflattening\u201d or a \u201cprojection\u201d. These are forms of linearizations where the whole hypergraph is flattened into a standard graph to which standard centrality models are applied. There are many approaches that follow this line, including linear-weighted clique expansions 14 15 16 17 18 19 where hyperedges are replaced by cliques in the flattened graph, whose adjacency matrix becomes \\[\\begin{equation}\\label{eq:clique-expansion-adjacency} A_{ij} = \\sum_{e: \\, i,j\\in e}w(e) \\end{equation}\\] with \\(w(e)\\) the weights of the original hypergraph; clique averaging 20 , where the weights \\(w(e)\\) in the sum \\(\\eqref{eq:clique-expansion-adjacency}\\) are averaged with generalized mean functions; connectivity graph expansion 21 22 , where the weights in the clique expansion are based on hyperedge degrees, for example replacing \\(w(e)\\) with \\(1/(|e|-1)\\) in \\(\\eqref{eq:clique-expansion-adjacency}\\) ; the star expansion 23 , where the flattened graph is obtained by introducing new vertices for each hyperedge, which are then connected according to the hypergraph structure. Another popular approach for centrality on hypergraphs uses a tensor representation of the data and tensor eigenvectors. This is a particularly natural approach in the case of uniform hypergraphs. A \\(m\\) -uniform hypergraph is a hypergraph \\(H=(V,E)\\) such that each hyperedge \\(e\\in E\\) contains exactly \\(m\\) nodes. Thus, a \\(2\\) -uniform hypergraph is a standard graph. As every hyperedge contains exactly \\(m\\) nodes, we can associate to \\(H\\) the adjacency tensor \\(T\\) such that \\(T_{i_1,\\dots,i_m} = w(e)\\) if the hyperedge \\(e = \\{i_1,\\dots,i_m\\}\\in E\\) , and \\(T_{i_1,\\dots,i_m}=0\\) otherwise. Clearly, \\(T\\) coincides with the adjacency matrix of the graph when \\(m=2\\) . A centrality score \\(x_i\\) for the node \\(i\\in V\\) of a \\(m\\) -uniform hypergraph is defined in 24 as being linearly proportional to the product of the centrality scores of the nodes in each hyperedge that involves \\(i\\) . This mutual reinforcing relation boils down to the constrained eigenvector equation \\[\\begin{equation}\\label{eq:tensor_eig} \\sum_{i_2,\\dots,i_k}T_{i,i_2,\\dots,i_m}x_{i_2}x_{i_3}\\cdots x_{i_m} = \\lambda \\, |x_{i}|^{p-2}x_{i} \\end{equation}\\] with \\(x>0\\) , \\(\\lambda>0\\) and \\(p>1\\) . The special cases \\(p=2\\) and \\(p={m}\\) correspond to so-called \\(Z\\) - and \\(H\\) -eigenvectors for \\(T\\) . Beyond matrices and tensors Matrix and tensor eigenvector approaches are constrained to model the interaction of nodes at higher-order and across layers as either an additive or a multiplicative function. For example, in \\(\\eqref{eq:tensor_eig}\\) the importance \\(x_i\\) of node \\(i\\) is inherited by the product of the importances of the nodes on each hyperedge node \\(i\\) belongs to. Moreover, tensor representations seem inadequate to model general hypergraphs as they require a constant number of nodes in the hyperedges. We discuss below a model introduced in 25 , based on the incidence matrix of the hypergraph and a general nonlinear multihomogeneous mapping. If \\(n=|V|\\) and \\(m = |E|\\) , the incidence matrix and the diagonal weight matrix of \\(H\\) are \\(n\\times m\\) and the \\(m\\times m\\) matrices defined respectively as \\[ B_{i,e} = \\begin{cases} 1 & i\\in e \\\\ 0 & \\text{otherwise } \\end{cases} \\qquad W = \\begin{bmatrix} w(e_1) & & \\\\ & \\ddots & \\\\ & & w(e_m)\\end{bmatrix}\\, . \\] These matrices fully describe the hypergraph. For example, when each \\(e\\) has size exactly 2, i.e. we are considering a standard graph, then \\(BWB^\\top = A + D\\) where \\(A\\) is the adjacency matrix of \\(H\\) and \\(D = \\mathrm{Diag}(d_1, \\dots, d_n)\\) is the digonal matrix of the weighted node degrees \\(d_i = \\sum_{j}A_{ij}\\) . Similarly, for a general hypergraph \\(H\\) , we have \\(BWB^\\top = A+D\\) where \\(A\\) and \\(D\\) this time are the adjacency and degree matrices of the clique-expansion graph associated with \\(H\\) , as defined in \\(\\eqref{eq:clique-expansion-adjacency}\\) . However, unlike the clique-expanded adjacency matrix, \\(B\\) allows us to model the structure of \\(H\\) \u201cbefore\u201d the flattening step. This is the basis of the model below where we describe a spectral (thus mutually reinforcing) model for both nodes and edges of a hypergraph. Let \\(x\\) and \\(y\\) be nonnegative vectors whose entries will provide centrality scores for the nodes and hyperedges of \\(H\\) , respectively. We would like the importance \\(y_e\\) for an edge \\(e\\in E\\) to be a nonnegative number proportional to a function of the importances of the nodes in \\(e\\) , for example \\(y_e \\propto \\sum_{i\\in e} x_i\\) . Similarly, we require that the centrality \\(x_i\\) of node \\(i\\in V\\) is a nonnegative number proportional to a function of the importances of the edges it participates in, for example \\(x_i \\propto \\sum_{e: i\\in e}w(e)y_e\\) . As the centralities \\(x_i\\) and \\(y_e\\) are all nonnegative, these sums coincide with the weighted \\(\\ell^1\\) norm of specific sets of centrality scores. Thus, we can generalize this idea by considering the weighted \\(\\ell^p\\) norm of node and edge importances. This leads to \\[ x_i \\propto \\Big(\\sum_{e: i\\in e}w(e)y_e^p\\Big)^{1/p},\\qquad y_e \\propto \\Big(\\sum_{i\\in e} x_i^q\\Big)^{1/q}, \\] for some \\(p,q\\geq 1\\) . More generally, we can consider four functions \\(f,g,\\varphi,\\psi:\\RR_+\\to\\RR_+\\) of the nonnegative real line and require that \\[ x_i \\propto g\\Big(\\sum_{e: i\\in e}w(e)f(y_e)\\Big),\\qquad y_e \\propto \\psi\\Big(\\sum_{i\\in e}\\nu(i)\\varphi(x_i)\\Big) \\, . \\] If we extend real functions on vectors by defining them as mappings that act in a componentwise fashion, the previous relations can be compactly written as the following constrained nonlinear equations \\[\\begin{equation}\\label{eq:NEP} \\begin{cases} \\lambda x = g\\big(BW f(y)\\big) & \\\\ \\mu y = \\psi\\big(B^\\top \\varphi(x)\\big) \\end{cases}\\qquad x, y \\succ 0, \\quad \\lambda, \\mu > 0 \\, . \\end{equation}\\] If \\(f,g,\\psi\\) and \\(\\varphi\\) are all identity functions, then \\(\\eqref{eq:NEP}\\) boils down to a linear system of equations which is structurally reminiscent of the HITS centrality model for directed graphs, briefly reviewed above: the importance of a node is proportional to the sum of the importances of the hyperedges it belongs to and, vice-versa, the importancesof a hyperdge is proportional to the sum of the importances of the nodes it involves. As for HITS centrality, when \\(f=g=\\varphi=\\psi=\\text{id}\\) and we have no edge nor node weights (i.e. \\(W,N\\) are identity matrices), then \\(x, y\\) in \\(\\eqref{eq:NEP}\\) are the left and right singular vectors of a graph matrix, in this case \\(B\\) , and the matrix Perron-Frobenius theory tells us that if the bipartite graph with adjacency matrix \\[\\begin{equation}\\label{eq:bipartite} \\begin{bmatrix} 0 & B\\\\ B^\\top & 0 \\end{bmatrix} \\end{equation}\\] is connected, then \\(\\eqref{eq:NEP}\\) has a unique solution. Instead, when either \\(f,g,\\varphi\\) or \\(\\psi\\) is not linear, even the most basic question of existence of a solution to \\(\\eqref{eq:NEP}\\) may be not straightforward. However, for homogeneous functions \\(f,g,\\varphi\\) and \\(\\psi\\) , the nonlinear Perron-Frobenius theory for multihomogeneous operators allows us to give guarantees on existence, uniqueness and computability for the nonlinear singular-vector centrality model in \\(\\eqref{eq:NEP}\\) . Theorem. Let \\(f,g,\\varphi,\\psi\\) be order preserving and homogeneous of degrees \\(\\alpha,\\beta,\\gamma,\\delta\\) , respectively. Define the coefficient \\(\\rho = |\\alpha\\beta\\gamma\\delta|\\) . If either ( 1 ) \\(\\rho<1\\) , or ( 2 ) \\(\\rho=1\\) , \\(f,g,\\varphi,\\psi\\) are differentiable and the bipartite graph with adjacency matrix as in \\(\\eqref{eq:bipartite}\\) is connected, then there exist unique \\(x^*,y^* \\succ 0\\) (up to scaling) and unique \\(\\lambda, \\mu >0\\) solution of \\(\\eqref{eq:NEP}\\) and the nonlinear power iteration x = ones ( n , 1 ) y = ones ( l , 1 ) for r = 0 , 1 , 2 , 3 , ... x = sqrt . ( x .* g ( B * W * f ( y ))) x = x / norm ( x ) y = sqrt . ( y .* \u03c8 ( B '* N * \u03c6 ( x ))) y = y / norm ( y ) converges to such \\(x^*,y^*\\) . You can find here the julia code that implements this algorithm and that runs it on a number of example datasets. The hypergraph sunflower A sunflower is a hypergraph whose hyperedges all have one common intersection in one single node, called the core . Let \\(u\\in V\\) be that intersection. Also let \\(r\\) be the number of petals (the hyperedges) each containing \\(|e_i|\\) nodes, for \\(i=1,\\dots,r\\) . By definition \\(u\\) is the only element in all the edges \\(\\cap_i e_i = \\{u\\}\\) . If \\(|e_i|=k+1\\) for all \\(i\\) , we say that the hypergraph is a uniform sunflower. The tensor eigenvector centrality of a uniform sunflower is studied for example in 24 . In this case we can assume that all the hyperedges have the same centrality score and that the same holds for all the nodes, besides the core, which is assigned a specific value. Assuming no weights, by symmetry we may impose the constraints \\(x_{v_i}=x_v\\) for all \\(v_i\\neq u\\) and \\(y_e = y\\) for all \\(e\\in E\\) in \\(\\eqref{eq:NEP}\\) to obtain \\[ x_v \\propto g(f(y)),\\qquad x_u \\propto g(rf(y)), \\qquad y \\propto \\psi(\\varphi(x_u) + k\\varphi(x_v)). \\] So, for example, with the choices of Theorem \\(\\ref{thm:tensor-eig} we get \\(x_u/x_v = g(r) = r^{1/(p+1)}\\) which coincides with the value computed in \\cite{benson2019three}, for the two choices \\(p = 1\\) and \\(p=m-1\\) , \\new{i.e., the tensor \\(Z\\) -eigenvector and \\(H\\) -eigenvector based centralities, respectively}. More generally, if \\(g\\) is homogeneous of degree \\(\\beta\\) we have \\begin{equation}\\label{eq:sunflower-centrality-ratio} \\frac{x_u}{x_v} \\propto r^\\beta\\, . \\end{equation} This shows that the node centrality assignment in the case of a uniform sunflower hypergraph only depends on the homogeneity degree of \\(g\\) and, in particular, when \\(\\beta\\to 0\\) all the centralities tend to coincide, while \\(x_u > x_v\\) for all \\(\\beta>0\\) , confirming and extending the observation in \\cite{benson2019three} for the setting of uniform hypergraph centralities based on tensor eigenvectors. Figure \\ref{fig:uniform-sunflower} illustrates this behaviour on an example uniform sunflower hypergraph with eight petals (\\) r=8 \\() each having three nodes (\\) k=3$). The figure shows the nodes of the hypergraph with a blue dot whose size is proportional to its centrality value computed according to the three singular vector hypergraph centrality models defined in Section \\ref{sec:comp}. The value of \\(\\beta\\) for these three centralities is \\(1\\) for both the max' and the linear\u2019 centrality\u2019, and \\(1/2\\) for `log-exp\u2019 centrality\u2019. Thus, all the three models assign essentially the same centrality score: the core node \\(u\\) has strictly larger centrality, while all other nodes have same centrality score. \\new{Similarly, the computed edge centrality is constant across all models and all petals.} \\subsubsection*{Generic sunflower} The situation is different for the case of a nonuniform hypergraph sunflower where we have \\(r\\) petals each containing an arbitrary number of nodes. The computational results in Figure~\\ref{fig:non-uniform-sunflower} indicate that the three models in Section \\ref{sec:comp} capture significantly different centrality properties: All three models recognize the core node as the most central one, however while the linear' model favours nodes that belong to large hyperedges, the multiplicative log-exp\u2019 model behaves in the opposite way assigning a larger centrality score to nodes being part of small hyperedges. Finally, the max' model behaves like in the uniform case, assigning the same centrality value to all the nodes in the petals (core node excluded). \\new{For this hypergraph, we observe that the edge centrality follows directly from the node one: for the linear\u2019 model the edge centrality is proportional to the number of nodes in the edge, for the log-exp' model it is inversely proportional to the number of nodes, while for the max\u2019 model all edges have the same centrality.} It would be of interest to pursue these differences analytically and hence gain further insights into the effect of \\(f,g,\\varphi\\) and \\(\\psi\\) . J. P. Sch\u00e4fermeyer. On Edmund Landau\u2019s contribution to the ranking of chess players. Technical Report, Unpublished manuscript, 2019. \u21a9 P. Bonacich. Power and centrality: a family of measures. American Journal of Sociology , 92:1170\u20131182, 1987. \u21a9 Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank citation ranking: Bringing order to the web. Technical Report, Stanford InfoLab, 1999. \u21a9 Jon M Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM \\(JACM\\) , 46 \\(5\\) :604\u2013632, 1999. \u21a9 Francesco Tudisco, Francesca Arrigo, and Antoine Gautier. Node and layer eigenvector centralities for multiplex networks. SIAM Journal on Applied Mathematics , 78 \\(2\\) :853\u2013876, 2018. \u21a9 Luis Sol\u00e1, Miguel Romance, Regino Criado, Julio Flores, Alejandro Garc\u00eda del Amo, and Stefano Boccaletti. Eigenvector centrality of nodes in multiplex networks. Chaos: An Interdisciplinary Journal of Nonlinear Science , 23 \\(3\\) :033131, 2013. \u21a9 Federico Battiston, Vincenzo Nicosia, and Vito Latora. Structural measures for multiplex networks. Physical Review E , 89 \\(3\\) :032804, 2014. \u21a9 Dengyong Zhou and Christopher JC Burges. Spectral clustering and transductive learning with multiple views. In International Conference on Machine Learning \\(ICML\\) , 1159\u20131166. 2007. \u21a9 Koji Tsuda, Hyunjung Shin, and Bernhard Sch\u00f6lkopf. Fast protein classification with multiple networks. Bioinformatics , 21 \\(suppl\\_2\\) :ii59\u2013ii65, 2005. \u21a9 Manlio De Domenico, Elisa Omodei, Sergio G\u00f3mez, Alex Arenas, and Albert Sol\u00e9-Ribalta. Centrality in interconnected multilayer networks. Nature Communications , 6 \\(arXiv: 1311\\.2906\\) :6868, 2013. \u21a9 Dane Taylor, Sean A Myers, Aaron Clauset, Mason A Porter, and Peter J Mucha. Eigenvector-based centrality measures for temporal networks. Multiscale Modeling & Simulation \\(SIAM\\) , 15 \\(1\\) :537\u2013574, 2017. \u21a9 Dane Taylor, Mason A Porter, and Peter J Mucha. Tunable eigenvector-based centralities for multiplex and temporal networks. Multiscale Modeling & Simulation \\(SIAM\\) , 19 \\(1\\) :113\u2013147, 2021. \u21a9 Christoph Rahmede, Jacopo Iacovacci, Alex Arenas, and Ginestra Bianconi. Centralities of nodes and influences of layers in large multiplex networks. Journal of Complex Networks , 6 \\(5\\) :733\u2013752, 2018. \u21a9 Timoteo Carletti, Federico Battiston, Giulia Cencetti, and Duccio Fanelli. Random walks on hypergraphs. Physical Review E , 101 \\(2\\) :022308, 2020. \u21a9 Juan Alberto Rodriguez. On the Laplacian eigenvalues and metric parameters of hypergraphs. Linear and Multilinear Algebra , 50 \\(1\\) :1\u201314, 2002. \u21a9 Juan Alberto Rodriguez. On the Laplacian spectrum and walk-regular hypergraphs. Linear and Multilinear Algebra , 51 \\(3\\) :285\u2013297, 2003. \u21a9 Juan Alberto Rodriguez. Laplacian eigenvalues and partition problems in hypergraphs. Applied Mathematics Letters , 22 \\(6\\) :916\u2013921, 2009. \u21a9 Sameer Agarwal, Kristin Branson, and Serge Belongie. Higher order learning with graphs. In International Conference on Machine Learning \\(ICML\\) , 17\u201324. 2006. \u21a9 Dengyong Zhou, Jiayuan Huang, and Bernhard Sch\u00f6lkopf. Learning with hypergraphs: clustering, classification, and embedding. In Advances in Neural Information Processing Systems \\(NeurIPS\\) , 1601\u20131608. 2007. \u21a9 Sameer Agarwal, Jongwoo Lim, Lihi Zelnik-Manor, Pietro Perona, David Kriegman, and Serge Belongie. Beyond pairwise clustering. In Conference on Computer Vision and Pattern Recognition \\(CVPR\\) , volume 2, 838\u2013845. IEEE, 2005. \u21a9 Anirban Banerjee. On the spectrum of hypergraphs. Linear Algebra and its Applications , 614:82\u2013110, 2021. \u21a9 Guilherme Ferraz de Arruda, Michele Tizzani, and Yamir Moreno. Phase transitions and stability of dynamical processes on hypergraphs. Communications Physics , 4 \\(1\\) :1\u20139, 2021. \u21a9 Jason Y Zien, Martine DF Schlag, and Pak K Chan. Multilevel spectral hypergraph partitioning with arbitrary vertex sizes. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 18 \\(9\\) :1389\u20131399, 1999. \u21a9 Austin R Benson. Three hypergraph eigenvector centralities. SIAM Journal on Mathematics of Data Science , 1 \\(2\\) :293\u2013312, 2019. \u21a9 \u21a9 Francesco Tudisco and Desmond J. Higham. Node and edge eigenvector centrality for hypergraphs. arXiv:2101.06215 , 2021. \u21a9","title":"Network centrality"},{"location":"ch2/centrality/#network_centrality","text":"Ranking the nodes of a network according to suitable \u201ccentrality measures\u201d is a recurring and fundamental question in network science and data mining. Among the various network centrality models, the class of eigenvector centrality is one of the most widely used and effective. This family of models dates back to the 19th Century when it was proposed as a mean to rank professional chess players by Edmund Landau 1 and was then popularized in the network science community starting from the late \u201980s with Bonacich 2 , PageRank 3 and HITS 4 models. This class of scores assigns importances to the nodes of a graph, based on the leading eigenvector \\(x\\) (or the leading singular vectors \\(x,y\\) ) of suitable network matrices and strongly rely on the matrix Perron-Frobenius theorem. One of the keys of the success of eigenvector centralities is that they naturally incorporate mutual reinforcement : important objects are those that interact with many other important objects. To clarify this idea, we review below two widely used centrality models (we consider the case of binary \u201cunweighted\u201d graphs for simplicity, but everything transfer with minor adjustments to the weighted setting.) Bonacich centrality Let \\(G=(V,E)\\) be a graph with adjacency matrix \\[ A_{ij} = \\begin{cases}1 & j\\to i \\\\ 0 & \\text{otherwise} \\end{cases} \\quad , \\] where \\(j\\to i\\) means that there is an edge in \\(G\\) going from \\(j\\) to \\(i\\) . Bonacich centrality model defines the importance \\(x_i\\) of node \\(i\\in V\\) as \\[ x_i\\propto \\sum_{j: \\, j\\to i} x_j = (Ax)_i \\, , \\] that is, the importance of node \\(x_i\\) is linearly proportional to the importances \\(x_j\\) of the nodes that points towards \\(i\\) . The Perron-Frobenius theory teaches us that, if the graph is strongly connected , only one such vector \\(x\\) exists, the Perron eigenvector of the adjacency matrix \\(A x=\\rho(A)x\\) , and it provides us with sufficient conditions to compute such \\(x\\) via the simple Power Method scheme. HITS centrality Another widely used approach defines two centralities indices, the hub score and the authority score , via the mutual reinforcing informal idea that \u201ca node is a good hub if it points to good authorities; and a node is a good authority if it is pointed by good hubs\u201d . If \\(x_i\\) and \\(y_i\\) are the hub and the authority scores of node \\(i\\) , respectively, then \\[\\begin{equation}\\label{eq:sing-vec} x_i \\propto \\sum_{j: i\\to j}y_j = (A^\\top y)_i \\qquad y_i \\propto \\sum_{j:j\\to i} x_j = (Ax)_i \\end{equation}\\] Again, by the Perron-Frobenius theorem, if the graph is strongly connected there is only one nonnegative solution, the dominant left and right singular vectors of the adjacency matrix.","title":"Network centrality"},{"location":"ch2/centrality/#nonlinear_eigenvector_centrality","text":"While powerful and useful, these models have two main drawbacks: they only allow for linear proportionality relations to define the importance model they may not be unique, even for simple graphs The nonlinear Perron-Frobenius theory allows us to overcome both these two drawbacks in a very natural way. This will be particularly important when moving to the case of higher-order networks , as we will discuss next. Here we discuss two simple illustrative examples. Suppose \\(A=I\\) is the identity matrix. This is certainly not irreducible and in fact \\(Ax = x\\) for all nonnegative vectors \\(x\\in C_+\\) . While from a linear algebra point of view there is no preferred nonnegative solution to \\(x=Ax\\) , from the graph centrality perspective this is not the case. The graph of \\(A\\) is a set of isolated nodes, each with a self-loop of weight exactly one. Thus, a centrality score for these nodes would assign same score to all the nodes. This corresponds to the eigenvector \\(x = \\one\\) of \\(A\\) . While this is only one of the nonnegative solutions of \\(x=Ax\\) , \\(\\one\\) is the only nonlinear Perron eigenvector of a \u201cnonlinear version\u201d of the eigenvalue problem for \\(A\\) . Precisely, consider the nonlinear eigenvalue equation \\[\\begin{equation}\\label{eq:Af(x)} \\lambda x = A x^{1-\\varepsilon} \\end{equation}\\] It is easy to verify that the mapping \\(F(x) := Ax^{1-\\varepsilon}\\) is (multi)homogeneous with homogeneity matrix the scalar \\(\\M=1-\\varepsilon\\) . Thus, for any \\(\\varepsilon \\in (0,1)\\) , \\(\\rho(\\M)<1\\) and by the nonlinear Perron-Frobenius theorem we have that \\(\\eqref{eq:Af(x)}\\) has a unique positive solution. It is easy to verify that such solution is entrywise constant, yielding the desired centrality assignment. A similar situation holds for the singular vector case. Consider the graph in the figure below: This is a well-known example where HITS centrality may fail to output a reasonable centrality. While this graph is not a strongly connected graph, we all most probably agree that node \\(1\\) is the most relevant hub while node \\(6\\) is the most relevant authority . Despite this simple setting, using the dominant singular vectors of \\(A\\) as in the HITS model may fail to identify these relevant hub and authority nodes. In fact, both the following pairs \\[\\begin{align*} x = (1,1,1,1,1, 0) \\qquad y = (0,1,1,1,1,4) \\\\ x = (4, 1, 1, 1, 1, 0) \\qquad y =(0,1,1,1,1,1) \\end{align*}\\] are (up to scaling) singular pairs of the dominant singular value of \\(A\\) . This shows that, in the first case, the hub vector fails to detect that node \\(1\\) is a better hub than nodes \\(2\\) \u2212 \\(5\\) . Similarly, in the second case, the authority vector fails to identify node \\(6\\) as a the best authority. Also in this case, a small nonlinear modification of \\(\\eqref{eq:sing-vec}\\) solves this issue. Consider the singular vector equation \\[\\begin{equation}\\label{eq:nonlin-sing-vec} \\lambda x = A^\\top y^\\alpha \\quad \\mu y = A x^\\beta\\, . \\end{equation}\\] It is easy to see that any such pair of vectors is an eigenvector of a multihomogeneous mapping with homogeneity matrix \\[ \\M = \\begin{bmatrix}0 & \\alpha \\\\ \\beta & 0 \\end{bmatrix}, \\qquad \\rho(H) = \\sqrt{|\\alpha\\beta|} \\] Thus for any \\(\\alpha\\beta<1\\) the nonlinear eigenvector equation \\(\\eqref{eq:nonlin-sing-vec}\\) has a unique solution. Below we show the value of the two solution vectors \\(x,y\\) for different choices of \\(\\alpha\\) and \\(\\beta\\) , showing that these two vectors capture the actual roles of nodes in this example graph. 1 2 3 4 5 6 7 8 9 \u2502 \u2502 \u03b1 = 0.5 \u2502 \u03b1 = 0.9 \u2502 \u03b1 = 0.5 \u2502 \u03b1 = 0.9 \u2502 \u03b2 = 0.5 \u2502 \u03b2 = 0.9 \u2502 \u03b2 = 0.9 \u2502 \u03b2 = 0.5 \u2502 \u2502entry\u2502 x1 \u2502 x2 \u2502 x3 \u2502 x4 \u2502 y1 \u2502 y2 \u2502 y3 \u2502 y4 \u2502 \u251c-----\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502 \u2502 1 \u2502 0.386488 \u2502 0.341489 \u2502 0.468535 \u2502 0.243379 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 \u2502 2 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 3 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 4 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 5 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 6 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 0.386488 \u2502 0.341489 \u2502 0.243379 \u2502 0.468535 \u2502","title":"Nonlinear eigenvector centrality"},{"location":"ch2/centrality/#higher-order_network_models","text":"While graph and networks are ubiquitous in the natural sciences, in many real-world applications we are confronted with higher-order interaction data. Relational data is full of interactions that happen in groups. For example, friendship relations very often happen in groups that are strictly larger than two individuals. Moreover, interactions naturally occur on multiple layers, for example work relations, sport relations, friendship relations, etc. To model higher-order interactions we need higher-order network models, which include multilayer networks , where we have a set of networks (so-called layers) with connections internally and across the layers, and non-dyadic networks , such as hypergraphs or simplicial complexes, where we have access to interactions involving multiple nodes. For simplicity, here we consider the following two settings: Multiplex : This is a set \\(\\{G_i\\}\\) of \\(m\\) graphs \\(G_i=(V,E_i)\\) \\(i=1,\\dots,m\\) , the layers , each defined on the same set of nodes but with possibly different edge sets Hypergraph : Just like a standard graph, this is a pair \\(H=(V,\\mathcal E)\\) where the set of hyperedges \\(\\mathcal E\\) is such that each \\(e\\in \\mathcal E\\) can involve an arbitrary number of nodes, rather than just two nodes as in the standard graph case.","title":"Higher-order network models"},{"location":"ch2/centrality/#eigenvector_centrality_for_multiplex","text":"A multiplex network \\(\\{G_k\\}\\) can be naturally described by means of an adjacency tensor \\(T\\) with three modes \\[ T_{ijk} = \\begin{cases} 1 & i\\to j \\text{ on layer }k \\\\ 0 & \\text{otherwise} \\end{cases} \\] where \\(i\\to j\\) means that there is an edge from node \\(i\\) to node \\(j\\) , i.e. \\((i,j)\\in E_k\\) . How can we define a mutual reinforcing centrality score for nodes (and layers) in \\(\\{G_k\\}\\) so that a larger centrality is assigned to nodes that form links with other central nodes in highly influential layers? Here we discuss the model proposed in 5 based on \\(T\\) and a multihomogeneous order-preserving mapping associated to it. Other approaches are discussed in #Related work . In this model, mutual reinforcement happens at both layer and node levels, as layers are more influential if highly central nodes are active in them. Thus, if \\(x_i\\) denotes the centrality of node \\(i\\) and \\(y_k\\) the influence of layer \\(k\\) , we require that \\[\\begin{equation}\\label{eq:tensor_singvec} \\left\\{ \\begin{array}{l} \\sum_{j,k}T_{i,j,k}x_jy_k = \\lambda \\, |x_i|^{p} \\mathrm{sign}(x_i)\\\\[.5em] \\sum_{i,j}T_{i,j,k}x_ix_j = \\mu\\, |y_k|^{q} \\mathrm{sign}(y_k) \\end{array} \\right. \\end{equation}\\] which imposes that the \\(p\\) -power of the importance of node \\(i\\) is proportional to the sum of the importances of the nodes that point at \\(i\\) , times the influence of the layer where such connections take place and, similarly, defines the \\(q\\) -power of the influence of the layer \\(k\\) as being proportional the product of the centrality of the nodes that are connected in that layer. Since the centrality score is a positive number it is natural to add the constraint \\(\\lambda,\\mu>0\\) and \\(x,y\\succ 0\\) in \\(\\eqref{eq:tensor_singvec}\\) . Thus, our centrality problem boils down to a constrained nonlinear system of equations. When \\(p=q=1\\) , the equations are homogeneous polynomials and \\(\\eqref{eq:tensor_singvec}\\) is directly reminiscent of a singular vector equation. However, unlike the matrix case, even positive tensors may admit multiple solutions here, as shown by the following example: Consider the positive adjacency tensor \\(T\\) with entries \\[ \\left\\{ \\begin{array}{llll} T_{1,1,1} = 6 & T_{1,2,1} = 199/7 & T_{2,1,1} = 16/7 & T_{2,2,1} = 11 \\\\ T_{1,1,2} = 61/7 & T_{1,2,2} = 6 & T_{2,1,2} = 29 & T_{2,2,2} = 16/7 \\end{array}\\right. \\] then both the pair \\(x = \\frac 1 3(2,1)\\) , \\(y = \\frac 1 3(1,2)\\) and the pair \\(x = \\frac 1 4(1,3)\\) , \\(y = \\frac 1 4(3,1)\\) are positive solutions to \\(\\eqref{eq:tensor_singvec}\\) . In order to practically use the centrality model \\(\\eqref{eq:tensor_singvec}\\) we need it to define a unique score. To this end, we recast the vectors \\(x,y\\) solutions of \\(\\eqref{eq:tensor_singvec}\\) as eigenvectors of the multihomogeneous mapping \\[ F(x,y) := \\begin{bmatrix} F_1(x,y) \\\\ F_2(x,y) \\end{bmatrix} = \\begin{bmatrix} (T_{(1)}xy)^{1/p} \\\\ (T_{(3)}xx)^{1/q} \\end{bmatrix} = \\begin{bmatrix} (\\sum_{jk} T_{ijk} x_jy_k)^{1/p} \\\\ (\\sum_{ij}T_{ijk}x_ix_j)^{1/q} \\end{bmatrix}\\, . \\] In fact, it is easy to see that \\(\\eqref{eq:tensor_singvec}\\) holds iff \\(F(x,y)= (\\lambda,\\mu)\\krog (x,y)\\) . Moreover, one easily realizes that \\[ \\M = \\begin{bmatrix} 1/p & 1/p \\\\ 2/q & 0 \\end{bmatrix} \\] is the homogeneity matrix of \\(F\\) . Thus, using a slightly different cone than \\(C_+\\) , one obtains the following: Theorem. Let \\(n\\) and \\(\\ell\\) be the number of nodes and the number of layers in \\(\\{G_k\\}\\) , respectively. Consider the following set of pairs of vectors \\[ C_+(T) =\\left\\{(x,y)\\succeq 0 \\quad : \\quad \\begin{array}{c} x_i \\propto \\textstyle{\\sum_{j,k} T_{ijk}}, \\quad \\forall i = 1, \\dots, n\\\\ y_k \\propto \\textstyle{\\sum_{i,j} T_{ijk}}, \\quad \\forall k =1,\\dots, \\ell \\end{array}\\right\\}\\, . \\] This is a cone of vectors that depends on the nonzero pattern of \\(T\\) : for \\((x,y)\\in C_+(T)\\) , \\(x_i\\) is zero if and only if \\(T_{ijk}=0\\) for all \\(j,k\\) , that is node \\(i\\) is isolated in all the layers and, similarly, \\(y_k=0\\) if and only if layer \\(k\\) is empty (there is no edge in that layer). Then \\(\\rho(\\M) = \\frac{\\sqrt{8p+q}+\\sqrt{q}}{2p\\sqrt{q}}\\) and if \\(\\rho(\\M)<1\\) we have: 1. The system of nonlinear equations \\(\\eqref{eq:tensor_singvec}\\) has a unique solution \\((x^*,y^*)\\in C_+(T)\\) such that \\(\\|x^*\\|=\\|y^*\\|=1\\) . 2. The power method iteration x = ones ( n , 1 ) y = ones ( l , 1 ) for r = 0 , 1 , 2 , 3 , .. x = F_1 ( x , y ) x = x / norm ( x ) y = F_2 ( x , y ) y = y / norm ( y ) converges to \\((x^*,y^*)\\) and after \\(m\\) steps we have \\[ \\|(x,y)-(x^*,y^*)\\|_{\\infty} \\leq \\rho(\\M)^m \\Big\\{ \\rho(\\M) \\, p\\, \\frac{\\max_{i\\in\\mathcal I} x_i^*}{\\min_{i'\\in\\mathcal I} x_{i'}^*}+\\, \\frac{\\max_{k\\in\\mathcal J} y_k^*}{\\min_{k'\\in\\mathcal J} y_{k'}^*}\\Big\\} \\] with \\(\\mathcal I=\\{i \\, : \\, x^{*}_i>0\\}\\) and \\(\\mathcal J =\\{k \\, :\\, y_k^{*}>0\\}\\) .","title":"Eigenvector centrality for multiplex"},{"location":"ch2/centrality/#related_work","text":"Other models for eigenvector centrality on multiplex networks are available. Many of them are based on a \u201cflattening\u201d or \u201cprojection\u201d approach which essentially transforms the multiplex into a graph and uses matrix eigenvectors on that graph to model node importances. These include, the aggregated graph of a multiplex graph 6 7 8 9 , where multiple layers with adjacency matrices \\(A_1,\\dots, A_m\\) are aggregated via a linear (possibly weighted) combination into a single denser adjacency matrix \\(A_{\\mathrm{agg}} = w_1A_1 + \\dots + w_mA_m\\) ; the supra-adjacency graph of a multilayer graph 10 11 12 , where a new graph of larger size is built by taking the Cartesian product of all the layers with weighted layer couplings obtaining a large adjacency matrix of the form \\[A_{\\mathrm{supra}}= \\begin{bmatrix} A_1 & I & \\dots & I \\\\ I & \\ddots & \\ddots &\\vdots \\\\ \\vdots & \\ddots & \\ddots & I \\\\ I & \\dots & I & A_m \\end{bmatrix} \\] Once a standard graph is obtained from the multiplex data, standard techniques can be applied to define and compute mutual-reinforcing centralities. In particular, the classical Perron\u2013Frobenius theory for matrices can be directly applied to the flattened graphs. To this end, it is interesting to notice that \\[ A_{\\mathrm{supra}} \\text{ is irreducible } \\iff A_{\\mathrm{agg}} \\text{ is irreducbile} \\] A different approach is proposed in 13 where the centrality for nodes and layers is computed by summing up powers of entries of the incidence matrix of the multiplex.","title":"Related work"},{"location":"ch2/centrality/#nonlinear_eigenvector_centrality_for_hypergraphs","text":"A hypergraph \\(H=(V,E)\\) consists of a set of nodes \\(V\\) and a set of hyperedges \\(E\\) , but, unlike graphs, an hyperedge \\(e\\in E\\) can contain an arbitrary number of nodes. In the weighted setting, we assume a weight function \\(w:E\\to \\RR_+\\) that assigns the weight \\(w(e)>0\\) to each hyperedge. Also in this setting, a relatively standard way to extend graph mappings and their eigenvectors is via a \u201cflattening\u201d or a \u201cprojection\u201d. These are forms of linearizations where the whole hypergraph is flattened into a standard graph to which standard centrality models are applied. There are many approaches that follow this line, including linear-weighted clique expansions 14 15 16 17 18 19 where hyperedges are replaced by cliques in the flattened graph, whose adjacency matrix becomes \\[\\begin{equation}\\label{eq:clique-expansion-adjacency} A_{ij} = \\sum_{e: \\, i,j\\in e}w(e) \\end{equation}\\] with \\(w(e)\\) the weights of the original hypergraph; clique averaging 20 , where the weights \\(w(e)\\) in the sum \\(\\eqref{eq:clique-expansion-adjacency}\\) are averaged with generalized mean functions; connectivity graph expansion 21 22 , where the weights in the clique expansion are based on hyperedge degrees, for example replacing \\(w(e)\\) with \\(1/(|e|-1)\\) in \\(\\eqref{eq:clique-expansion-adjacency}\\) ; the star expansion 23 , where the flattened graph is obtained by introducing new vertices for each hyperedge, which are then connected according to the hypergraph structure. Another popular approach for centrality on hypergraphs uses a tensor representation of the data and tensor eigenvectors. This is a particularly natural approach in the case of uniform hypergraphs. A \\(m\\) -uniform hypergraph is a hypergraph \\(H=(V,E)\\) such that each hyperedge \\(e\\in E\\) contains exactly \\(m\\) nodes. Thus, a \\(2\\) -uniform hypergraph is a standard graph. As every hyperedge contains exactly \\(m\\) nodes, we can associate to \\(H\\) the adjacency tensor \\(T\\) such that \\(T_{i_1,\\dots,i_m} = w(e)\\) if the hyperedge \\(e = \\{i_1,\\dots,i_m\\}\\in E\\) , and \\(T_{i_1,\\dots,i_m}=0\\) otherwise. Clearly, \\(T\\) coincides with the adjacency matrix of the graph when \\(m=2\\) . A centrality score \\(x_i\\) for the node \\(i\\in V\\) of a \\(m\\) -uniform hypergraph is defined in 24 as being linearly proportional to the product of the centrality scores of the nodes in each hyperedge that involves \\(i\\) . This mutual reinforcing relation boils down to the constrained eigenvector equation \\[\\begin{equation}\\label{eq:tensor_eig} \\sum_{i_2,\\dots,i_k}T_{i,i_2,\\dots,i_m}x_{i_2}x_{i_3}\\cdots x_{i_m} = \\lambda \\, |x_{i}|^{p-2}x_{i} \\end{equation}\\] with \\(x>0\\) , \\(\\lambda>0\\) and \\(p>1\\) . The special cases \\(p=2\\) and \\(p={m}\\) correspond to so-called \\(Z\\) - and \\(H\\) -eigenvectors for \\(T\\) .","title":"Nonlinear eigenvector centrality for hypergraphs"},{"location":"ch2/centrality/#beyond_matrices_and_tensors","text":"Matrix and tensor eigenvector approaches are constrained to model the interaction of nodes at higher-order and across layers as either an additive or a multiplicative function. For example, in \\(\\eqref{eq:tensor_eig}\\) the importance \\(x_i\\) of node \\(i\\) is inherited by the product of the importances of the nodes on each hyperedge node \\(i\\) belongs to. Moreover, tensor representations seem inadequate to model general hypergraphs as they require a constant number of nodes in the hyperedges. We discuss below a model introduced in 25 , based on the incidence matrix of the hypergraph and a general nonlinear multihomogeneous mapping. If \\(n=|V|\\) and \\(m = |E|\\) , the incidence matrix and the diagonal weight matrix of \\(H\\) are \\(n\\times m\\) and the \\(m\\times m\\) matrices defined respectively as \\[ B_{i,e} = \\begin{cases} 1 & i\\in e \\\\ 0 & \\text{otherwise } \\end{cases} \\qquad W = \\begin{bmatrix} w(e_1) & & \\\\ & \\ddots & \\\\ & & w(e_m)\\end{bmatrix}\\, . \\] These matrices fully describe the hypergraph. For example, when each \\(e\\) has size exactly 2, i.e. we are considering a standard graph, then \\(BWB^\\top = A + D\\) where \\(A\\) is the adjacency matrix of \\(H\\) and \\(D = \\mathrm{Diag}(d_1, \\dots, d_n)\\) is the digonal matrix of the weighted node degrees \\(d_i = \\sum_{j}A_{ij}\\) . Similarly, for a general hypergraph \\(H\\) , we have \\(BWB^\\top = A+D\\) where \\(A\\) and \\(D\\) this time are the adjacency and degree matrices of the clique-expansion graph associated with \\(H\\) , as defined in \\(\\eqref{eq:clique-expansion-adjacency}\\) . However, unlike the clique-expanded adjacency matrix, \\(B\\) allows us to model the structure of \\(H\\) \u201cbefore\u201d the flattening step. This is the basis of the model below where we describe a spectral (thus mutually reinforcing) model for both nodes and edges of a hypergraph. Let \\(x\\) and \\(y\\) be nonnegative vectors whose entries will provide centrality scores for the nodes and hyperedges of \\(H\\) , respectively. We would like the importance \\(y_e\\) for an edge \\(e\\in E\\) to be a nonnegative number proportional to a function of the importances of the nodes in \\(e\\) , for example \\(y_e \\propto \\sum_{i\\in e} x_i\\) . Similarly, we require that the centrality \\(x_i\\) of node \\(i\\in V\\) is a nonnegative number proportional to a function of the importances of the edges it participates in, for example \\(x_i \\propto \\sum_{e: i\\in e}w(e)y_e\\) . As the centralities \\(x_i\\) and \\(y_e\\) are all nonnegative, these sums coincide with the weighted \\(\\ell^1\\) norm of specific sets of centrality scores. Thus, we can generalize this idea by considering the weighted \\(\\ell^p\\) norm of node and edge importances. This leads to \\[ x_i \\propto \\Big(\\sum_{e: i\\in e}w(e)y_e^p\\Big)^{1/p},\\qquad y_e \\propto \\Big(\\sum_{i\\in e} x_i^q\\Big)^{1/q}, \\] for some \\(p,q\\geq 1\\) . More generally, we can consider four functions \\(f,g,\\varphi,\\psi:\\RR_+\\to\\RR_+\\) of the nonnegative real line and require that \\[ x_i \\propto g\\Big(\\sum_{e: i\\in e}w(e)f(y_e)\\Big),\\qquad y_e \\propto \\psi\\Big(\\sum_{i\\in e}\\nu(i)\\varphi(x_i)\\Big) \\, . \\] If we extend real functions on vectors by defining them as mappings that act in a componentwise fashion, the previous relations can be compactly written as the following constrained nonlinear equations \\[\\begin{equation}\\label{eq:NEP} \\begin{cases} \\lambda x = g\\big(BW f(y)\\big) & \\\\ \\mu y = \\psi\\big(B^\\top \\varphi(x)\\big) \\end{cases}\\qquad x, y \\succ 0, \\quad \\lambda, \\mu > 0 \\, . \\end{equation}\\] If \\(f,g,\\psi\\) and \\(\\varphi\\) are all identity functions, then \\(\\eqref{eq:NEP}\\) boils down to a linear system of equations which is structurally reminiscent of the HITS centrality model for directed graphs, briefly reviewed above: the importance of a node is proportional to the sum of the importances of the hyperedges it belongs to and, vice-versa, the importancesof a hyperdge is proportional to the sum of the importances of the nodes it involves. As for HITS centrality, when \\(f=g=\\varphi=\\psi=\\text{id}\\) and we have no edge nor node weights (i.e. \\(W,N\\) are identity matrices), then \\(x, y\\) in \\(\\eqref{eq:NEP}\\) are the left and right singular vectors of a graph matrix, in this case \\(B\\) , and the matrix Perron-Frobenius theory tells us that if the bipartite graph with adjacency matrix \\[\\begin{equation}\\label{eq:bipartite} \\begin{bmatrix} 0 & B\\\\ B^\\top & 0 \\end{bmatrix} \\end{equation}\\] is connected, then \\(\\eqref{eq:NEP}\\) has a unique solution. Instead, when either \\(f,g,\\varphi\\) or \\(\\psi\\) is not linear, even the most basic question of existence of a solution to \\(\\eqref{eq:NEP}\\) may be not straightforward. However, for homogeneous functions \\(f,g,\\varphi\\) and \\(\\psi\\) , the nonlinear Perron-Frobenius theory for multihomogeneous operators allows us to give guarantees on existence, uniqueness and computability for the nonlinear singular-vector centrality model in \\(\\eqref{eq:NEP}\\) . Theorem. Let \\(f,g,\\varphi,\\psi\\) be order preserving and homogeneous of degrees \\(\\alpha,\\beta,\\gamma,\\delta\\) , respectively. Define the coefficient \\(\\rho = |\\alpha\\beta\\gamma\\delta|\\) . If either ( 1 ) \\(\\rho<1\\) , or ( 2 ) \\(\\rho=1\\) , \\(f,g,\\varphi,\\psi\\) are differentiable and the bipartite graph with adjacency matrix as in \\(\\eqref{eq:bipartite}\\) is connected, then there exist unique \\(x^*,y^* \\succ 0\\) (up to scaling) and unique \\(\\lambda, \\mu >0\\) solution of \\(\\eqref{eq:NEP}\\) and the nonlinear power iteration x = ones ( n , 1 ) y = ones ( l , 1 ) for r = 0 , 1 , 2 , 3 , ... x = sqrt . ( x .* g ( B * W * f ( y ))) x = x / norm ( x ) y = sqrt . ( y .* \u03c8 ( B '* N * \u03c6 ( x ))) y = y / norm ( y ) converges to such \\(x^*,y^*\\) . You can find here the julia code that implements this algorithm and that runs it on a number of example datasets.","title":"Beyond matrices and tensors"},{"location":"ch2/centrality/#the_hypergraph_sunflower","text":"A sunflower is a hypergraph whose hyperedges all have one common intersection in one single node, called the core . Let \\(u\\in V\\) be that intersection. Also let \\(r\\) be the number of petals (the hyperedges) each containing \\(|e_i|\\) nodes, for \\(i=1,\\dots,r\\) . By definition \\(u\\) is the only element in all the edges \\(\\cap_i e_i = \\{u\\}\\) . If \\(|e_i|=k+1\\) for all \\(i\\) , we say that the hypergraph is a uniform sunflower. The tensor eigenvector centrality of a uniform sunflower is studied for example in 24 . In this case we can assume that all the hyperedges have the same centrality score and that the same holds for all the nodes, besides the core, which is assigned a specific value. Assuming no weights, by symmetry we may impose the constraints \\(x_{v_i}=x_v\\) for all \\(v_i\\neq u\\) and \\(y_e = y\\) for all \\(e\\in E\\) in \\(\\eqref{eq:NEP}\\) to obtain \\[ x_v \\propto g(f(y)),\\qquad x_u \\propto g(rf(y)), \\qquad y \\propto \\psi(\\varphi(x_u) + k\\varphi(x_v)). \\] So, for example, with the choices of Theorem \\(\\ref{thm:tensor-eig} we get \\(x_u/x_v = g(r) = r^{1/(p+1)}\\) which coincides with the value computed in \\cite{benson2019three}, for the two choices \\(p = 1\\) and \\(p=m-1\\) , \\new{i.e., the tensor \\(Z\\) -eigenvector and \\(H\\) -eigenvector based centralities, respectively}. More generally, if \\(g\\) is homogeneous of degree \\(\\beta\\) we have \\begin{equation}\\label{eq:sunflower-centrality-ratio} \\frac{x_u}{x_v} \\propto r^\\beta\\, . \\end{equation} This shows that the node centrality assignment in the case of a uniform sunflower hypergraph only depends on the homogeneity degree of \\(g\\) and, in particular, when \\(\\beta\\to 0\\) all the centralities tend to coincide, while \\(x_u > x_v\\) for all \\(\\beta>0\\) , confirming and extending the observation in \\cite{benson2019three} for the setting of uniform hypergraph centralities based on tensor eigenvectors. Figure \\ref{fig:uniform-sunflower} illustrates this behaviour on an example uniform sunflower hypergraph with eight petals (\\) r=8 \\() each having three nodes (\\) k=3$). The figure shows the nodes of the hypergraph with a blue dot whose size is proportional to its centrality value computed according to the three singular vector hypergraph centrality models defined in Section \\ref{sec:comp}. The value of \\(\\beta\\) for these three centralities is \\(1\\) for both the max' and the linear\u2019 centrality\u2019, and \\(1/2\\) for `log-exp\u2019 centrality\u2019. Thus, all the three models assign essentially the same centrality score: the core node \\(u\\) has strictly larger centrality, while all other nodes have same centrality score. \\new{Similarly, the computed edge centrality is constant across all models and all petals.} \\subsubsection*{Generic sunflower} The situation is different for the case of a nonuniform hypergraph sunflower where we have \\(r\\) petals each containing an arbitrary number of nodes. The computational results in Figure~\\ref{fig:non-uniform-sunflower} indicate that the three models in Section \\ref{sec:comp} capture significantly different centrality properties: All three models recognize the core node as the most central one, however while the linear' model favours nodes that belong to large hyperedges, the multiplicative log-exp\u2019 model behaves in the opposite way assigning a larger centrality score to nodes being part of small hyperedges. Finally, the max' model behaves like in the uniform case, assigning the same centrality value to all the nodes in the petals (core node excluded). \\new{For this hypergraph, we observe that the edge centrality follows directly from the node one: for the linear\u2019 model the edge centrality is proportional to the number of nodes in the edge, for the log-exp' model it is inversely proportional to the number of nodes, while for the max\u2019 model all edges have the same centrality.} It would be of interest to pursue these differences analytically and hence gain further insights into the effect of \\(f,g,\\varphi\\) and \\(\\psi\\) . J. P. Sch\u00e4fermeyer. On Edmund Landau\u2019s contribution to the ranking of chess players. Technical Report, Unpublished manuscript, 2019. \u21a9 P. Bonacich. Power and centrality: a family of measures. American Journal of Sociology , 92:1170\u20131182, 1987. \u21a9 Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank citation ranking: Bringing order to the web. Technical Report, Stanford InfoLab, 1999. \u21a9 Jon M Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM \\(JACM\\) , 46 \\(5\\) :604\u2013632, 1999. \u21a9 Francesco Tudisco, Francesca Arrigo, and Antoine Gautier. Node and layer eigenvector centralities for multiplex networks. SIAM Journal on Applied Mathematics , 78 \\(2\\) :853\u2013876, 2018. \u21a9 Luis Sol\u00e1, Miguel Romance, Regino Criado, Julio Flores, Alejandro Garc\u00eda del Amo, and Stefano Boccaletti. Eigenvector centrality of nodes in multiplex networks. Chaos: An Interdisciplinary Journal of Nonlinear Science , 23 \\(3\\) :033131, 2013. \u21a9 Federico Battiston, Vincenzo Nicosia, and Vito Latora. Structural measures for multiplex networks. Physical Review E , 89 \\(3\\) :032804, 2014. \u21a9 Dengyong Zhou and Christopher JC Burges. Spectral clustering and transductive learning with multiple views. In International Conference on Machine Learning \\(ICML\\) , 1159\u20131166. 2007. \u21a9 Koji Tsuda, Hyunjung Shin, and Bernhard Sch\u00f6lkopf. Fast protein classification with multiple networks. Bioinformatics , 21 \\(suppl\\_2\\) :ii59\u2013ii65, 2005. \u21a9 Manlio De Domenico, Elisa Omodei, Sergio G\u00f3mez, Alex Arenas, and Albert Sol\u00e9-Ribalta. Centrality in interconnected multilayer networks. Nature Communications , 6 \\(arXiv: 1311\\.2906\\) :6868, 2013. \u21a9 Dane Taylor, Sean A Myers, Aaron Clauset, Mason A Porter, and Peter J Mucha. Eigenvector-based centrality measures for temporal networks. Multiscale Modeling & Simulation \\(SIAM\\) , 15 \\(1\\) :537\u2013574, 2017. \u21a9 Dane Taylor, Mason A Porter, and Peter J Mucha. Tunable eigenvector-based centralities for multiplex and temporal networks. Multiscale Modeling & Simulation \\(SIAM\\) , 19 \\(1\\) :113\u2013147, 2021. \u21a9 Christoph Rahmede, Jacopo Iacovacci, Alex Arenas, and Ginestra Bianconi. Centralities of nodes and influences of layers in large multiplex networks. Journal of Complex Networks , 6 \\(5\\) :733\u2013752, 2018. \u21a9 Timoteo Carletti, Federico Battiston, Giulia Cencetti, and Duccio Fanelli. Random walks on hypergraphs. Physical Review E , 101 \\(2\\) :022308, 2020. \u21a9 Juan Alberto Rodriguez. On the Laplacian eigenvalues and metric parameters of hypergraphs. Linear and Multilinear Algebra , 50 \\(1\\) :1\u201314, 2002. \u21a9 Juan Alberto Rodriguez. On the Laplacian spectrum and walk-regular hypergraphs. Linear and Multilinear Algebra , 51 \\(3\\) :285\u2013297, 2003. \u21a9 Juan Alberto Rodriguez. Laplacian eigenvalues and partition problems in hypergraphs. Applied Mathematics Letters , 22 \\(6\\) :916\u2013921, 2009. \u21a9 Sameer Agarwal, Kristin Branson, and Serge Belongie. Higher order learning with graphs. In International Conference on Machine Learning \\(ICML\\) , 17\u201324. 2006. \u21a9 Dengyong Zhou, Jiayuan Huang, and Bernhard Sch\u00f6lkopf. Learning with hypergraphs: clustering, classification, and embedding. In Advances in Neural Information Processing Systems \\(NeurIPS\\) , 1601\u20131608. 2007. \u21a9 Sameer Agarwal, Jongwoo Lim, Lihi Zelnik-Manor, Pietro Perona, David Kriegman, and Serge Belongie. Beyond pairwise clustering. In Conference on Computer Vision and Pattern Recognition \\(CVPR\\) , volume 2, 838\u2013845. IEEE, 2005. \u21a9 Anirban Banerjee. On the spectrum of hypergraphs. Linear Algebra and its Applications , 614:82\u2013110, 2021. \u21a9 Guilherme Ferraz de Arruda, Michele Tizzani, and Yamir Moreno. Phase transitions and stability of dynamical processes on hypergraphs. Communications Physics , 4 \\(1\\) :1\u20139, 2021. \u21a9 Jason Y Zien, Martine DF Schlag, and Pak K Chan. Multilevel spectral hypergraph partitioning with arbitrary vertex sizes. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 18 \\(9\\) :1389\u20131399, 1999. \u21a9 Austin R Benson. Three hypergraph eigenvector centralities. SIAM Journal on Mathematics of Data Science , 1 \\(2\\) :293\u2013312, 2019. \u21a9 \u21a9 Francesco Tudisco and Desmond J. Higham. Node and edge eigenvector centrality for hypergraphs. arXiv:2101.06215 , 2021. \u21a9","title":"The hypergraph sunflower"},{"location":"ch2/sec1/","text":"Eigenvector centrality Complex networks is a common name for various real networks which are usually presented by graphs with a large number of nodes: Internet graphs, collaboration graphs, e-mail graphs, social networks, transport networks, protein-protein interaction networks, and many other. The term network analysis refers to a wealth of mathematical techniques aiming at describing the structure, function, and evolution of complex networks. One of the main tasks in network analysis is the localization of nodes that, in some sense, are the \u201cmost important\u201d in a given graph. The main tool to quantify the relevance of nodes in a graph is through the computation of suitably defined centrality indices. Many centrality indices have been invented during time. Each one of them refers to a particular definition of \u201cimportance\u201d or \u201crelevance\u201d that is most useful in a given context..... Eigenvector centrality 101 Ranking the nodes of a network according to suitable \u201ccentrality measures\u201d is a recurring and fundamental question in network science and data mining. Among the various network centrality models, the class of eigenvector centrality is one of the most widely used and effective. This family of models dates back to the 19th Century when it was proposed as a mean to rank professional chess players by Edmund Landau 1 and was then popularized in the network science community starting from the late \u201980s with Bonacich 2 , PageRank 3 and HITS 4 models. This class of scores assigns importances to the nodes of a graph, based on the leading eigenvector \\(x\\) (or the leading singular vectors \\(x,y\\) ) of suitable network matrices and strongly rely on the matrix Perron-Frobenius theorem. One of the keys of the success of eigenvector centralities is that they naturally incorporate mutual reinforcement : important objects are those that interact with many other important objects. To clarify this idea, we review below two widely used centrality models (we consider the case of binary \u201cunweighted\u201d graphs for simplicity, but everything transfer with minor adjustments to the weighted setting.) Bonacich centrality Let \\(G=(V,E)\\) be a graph with adjacency matrix \\[ A_{ij} = \\begin{cases}1 & j\\to i \\\\ 0 & \\text{otherwise} \\end{cases} \\quad , \\] where \\(j\\to i\\) means that there is an edge in \\(G\\) going from \\(j\\) to \\(i\\) . Bonacich centrality model defines the importance \\(x_i\\) of node \\(i\\in V\\) as \\[ x_i\\propto \\sum_{j: \\, j\\to i} x_j = (Ax)_i \\, , \\] that is, the importance of node \\(x_i\\) is linearly proportional to the importances \\(x_j\\) of the nodes that points towards \\(i\\) . The Perron-Frobenius theory teaches us that, if the graph is strongly connected , only one such vector \\(x\\) exists, the Perron eigenvector of the adjacency matrix \\(A x=\\rho(A)x\\) , and it provides us with sufficient conditions to compute such \\(x\\) via the simple Power Method scheme. HITS centrality Another widely used approach defines two centralities indices, the hub score and the authority score , via the mutual reinforcing informal idea that \u201ca node is a good hub if it points to good authorities; and a node is a good authority if it is pointed by good hubs\u201d . If \\(x_i\\) and \\(y_i\\) are the hub and the authority scores of node \\(i\\) , respectively, then \\[\\begin{equation}\\label{eq:sing-vec} x_i \\propto \\sum_{j: i\\to j}y_j = (A^\\top y)_i \\qquad y_i \\propto \\sum_{j:j\\to i} x_j = (Ax)_i \\end{equation}\\] Again, by the Perron-Frobenius theorem, if the graph is strongly connected there is only one nonnegative solution, the dominant left and right singular vectors of the adjacency matrix. Nonlinear eigenvector centrality While powerful and useful, these models have two main drawbacks: they only allow for linear proportionality relations to define the importance model they may not be unique, even for simple graphs The nonlinear Perron-Frobenius theory allows us to overcome both these two drawbacks in a very natural way. This will be particularly important when moving to the case of higher-order networks , as we will discuss next. Here we discuss two simple illustrative examples. Suppose \\(A=I\\) is the identity matrix. This is certainly not irreducible and in fact \\(Ax = x\\) for all nonnegative vectors \\(x\\in C_+\\) . While from a linear algebra point of view there is no preferred nonnegative solution to \\(x=Ax\\) , from the graph centrality perspective this is not the case. The graph of \\(A\\) is a set of isolated nodes, each with a self-loop of weight exactly one. Thus, a centrality score for these nodes would assign same score to all the nodes. This corresponds to the eigenvector \\(x = \\one\\) of \\(A\\) . While this is only one of the nonnegative solutions of \\(x=Ax\\) , \\(\\one\\) is the only nonlinear Perron eigenvector of a \u201cnonlinear version\u201d of the eigenvalue problem for \\(A\\) . Precisely, consider the nonlinear eigenvalue equation \\[\\begin{equation}\\label{eq:Af(x)} \\lambda x = A x^{1-\\varepsilon} \\end{equation}\\] It is easy to verify that the mapping \\(F(x) := Ax^{1-\\varepsilon}\\) is (multi)homogeneous with homogeneity matrix the scalar \\(\\M=1-\\varepsilon\\) . Thus, for any \\(\\varepsilon \\in (0,1)\\) , \\(\\rho(\\M)<1\\) and by the nonlinear Perron-Frobenius theorem we have that \\(\\eqref{eq:Af(x)}\\) has a unique positive solution. It is easy to verify that such solution is entrywise constant, yielding the desired centrality assignment. A similar situation holds for the singular vector case. Consider the graph in the figure below: This is a well-known example where HITS centrality may fail to output a reasonable centrality. While this graph is not a strongly connected graph, we all most probably agree that node \\(1\\) is the most relevant hub while node \\(6\\) is the most relevant authority . Despite this simple setting, using the dominant singular vectors of \\(A\\) as in the HITS model may fail to identify these relevant hub and authority nodes. In fact, for example, both the following pairs \\[\\begin{align*} x = (1,1,1,1,1, 0) \\qquad y = (0,1,1,1,1,4) \\\\ x = (4, 1, 1, 1, 1, 0) \\qquad y =(0,1,1,1,1,1) \\end{align*}\\] are (up to scaling) singular pairs of the dominant singular value of \\(A\\) . This shows that, in the first case, the hub vector fails to detect that node \\(1\\) is a better hub than nodes \\(2\\) \u2212 \\(5\\) . Similarly, in the second case, the authority vector fails to identify node \\(6\\) as a the best authority. Also in this case, a small nonlinear modification of \\(\\eqref{eq:sing-vec}\\) solves this issue. Consider the singular vector equation \\[\\begin{equation}\\label{eq:nonlin-sing-vec} \\lambda x = A^\\top y^\\alpha \\quad \\mu y = A x^\\beta\\, . \\end{equation}\\] It is easy to see that any such pair of vectors is an eigenvector of a multihomogeneous mapping with homogeneity matrix \\[ \\M = \\begin{bmatrix}0 & \\alpha \\\\ \\beta & 0 \\end{bmatrix}, \\qquad \\rho(H) = \\sqrt{|\\alpha\\beta|} \\] Thus for any \\(\\alpha\\beta<1\\) the nonlinear eigenvector equation \\(\\eqref{eq:nonlin-sing-vec}\\) has a unique solution. Below we show the value of the two solution vectors \\(x,y\\) for different choices of \\(\\alpha\\) and \\(\\beta\\) , showing that these two vectors capture the actual roles of nodes in this example graph. 1 2 3 4 5 6 7 8 9 \u2502 \u2502 \u03b1 = 0.5 \u2502 \u03b1 = 0.9 \u2502 \u03b1 = 0.5 \u2502 \u03b1 = 0.9 \u2502 \u03b2 = 0.5 \u2502 \u03b2 = 0.9 \u2502 \u03b2 = 0.9 \u2502 \u03b2 = 0.5 \u2502 \u2502entry\u2502 x1 \u2502 x2 \u2502 x3 \u2502 x4 \u2502 y1 \u2502 y2 \u2502 y3 \u2502 y4 \u2502 \u251c-----\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502 \u2502 1 \u2502 0.386488 \u2502 0.341489 \u2502 0.468535 \u2502 0.243379 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 \u2502 2 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 3 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 4 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 5 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 6 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 0.386488 \u2502 0.341489 \u2502 0.243379 \u2502 0.468535 \u2502 J. P. Sch\u00e4fermeyer. On Edmund Landau\u2019s contribution to the ranking of chess players. Technical Report, Unpublished manuscript, 2019. \u21a9 P. Bonacich. Power and centrality: a family of measures. American Journal of Sociology , 92:1170\u20131182, 1987. \u21a9 Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank citation ranking: Bringing order to the web. Technical Report, Stanford InfoLab, 1999. \u21a9 Jon M Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM \\(JACM\\) , 46 \\(5\\) :604\u2013632, 1999. \u21a9","title":"Eigenvector centrality"},{"location":"ch2/sec1/#eigenvector_centrality","text":"Complex networks is a common name for various real networks which are usually presented by graphs with a large number of nodes: Internet graphs, collaboration graphs, e-mail graphs, social networks, transport networks, protein-protein interaction networks, and many other. The term network analysis refers to a wealth of mathematical techniques aiming at describing the structure, function, and evolution of complex networks. One of the main tasks in network analysis is the localization of nodes that, in some sense, are the \u201cmost important\u201d in a given graph. The main tool to quantify the relevance of nodes in a graph is through the computation of suitably defined centrality indices. Many centrality indices have been invented during time. Each one of them refers to a particular definition of \u201cimportance\u201d or \u201crelevance\u201d that is most useful in a given context.....","title":"Eigenvector centrality"},{"location":"ch2/sec1/#eigenvector_centrality_101","text":"Ranking the nodes of a network according to suitable \u201ccentrality measures\u201d is a recurring and fundamental question in network science and data mining. Among the various network centrality models, the class of eigenvector centrality is one of the most widely used and effective. This family of models dates back to the 19th Century when it was proposed as a mean to rank professional chess players by Edmund Landau 1 and was then popularized in the network science community starting from the late \u201980s with Bonacich 2 , PageRank 3 and HITS 4 models. This class of scores assigns importances to the nodes of a graph, based on the leading eigenvector \\(x\\) (or the leading singular vectors \\(x,y\\) ) of suitable network matrices and strongly rely on the matrix Perron-Frobenius theorem. One of the keys of the success of eigenvector centralities is that they naturally incorporate mutual reinforcement : important objects are those that interact with many other important objects. To clarify this idea, we review below two widely used centrality models (we consider the case of binary \u201cunweighted\u201d graphs for simplicity, but everything transfer with minor adjustments to the weighted setting.) Bonacich centrality Let \\(G=(V,E)\\) be a graph with adjacency matrix \\[ A_{ij} = \\begin{cases}1 & j\\to i \\\\ 0 & \\text{otherwise} \\end{cases} \\quad , \\] where \\(j\\to i\\) means that there is an edge in \\(G\\) going from \\(j\\) to \\(i\\) . Bonacich centrality model defines the importance \\(x_i\\) of node \\(i\\in V\\) as \\[ x_i\\propto \\sum_{j: \\, j\\to i} x_j = (Ax)_i \\, , \\] that is, the importance of node \\(x_i\\) is linearly proportional to the importances \\(x_j\\) of the nodes that points towards \\(i\\) . The Perron-Frobenius theory teaches us that, if the graph is strongly connected , only one such vector \\(x\\) exists, the Perron eigenvector of the adjacency matrix \\(A x=\\rho(A)x\\) , and it provides us with sufficient conditions to compute such \\(x\\) via the simple Power Method scheme. HITS centrality Another widely used approach defines two centralities indices, the hub score and the authority score , via the mutual reinforcing informal idea that \u201ca node is a good hub if it points to good authorities; and a node is a good authority if it is pointed by good hubs\u201d . If \\(x_i\\) and \\(y_i\\) are the hub and the authority scores of node \\(i\\) , respectively, then \\[\\begin{equation}\\label{eq:sing-vec} x_i \\propto \\sum_{j: i\\to j}y_j = (A^\\top y)_i \\qquad y_i \\propto \\sum_{j:j\\to i} x_j = (Ax)_i \\end{equation}\\] Again, by the Perron-Frobenius theorem, if the graph is strongly connected there is only one nonnegative solution, the dominant left and right singular vectors of the adjacency matrix.","title":"Eigenvector centrality 101"},{"location":"ch2/sec1/#nonlinear_eigenvector_centrality","text":"While powerful and useful, these models have two main drawbacks: they only allow for linear proportionality relations to define the importance model they may not be unique, even for simple graphs The nonlinear Perron-Frobenius theory allows us to overcome both these two drawbacks in a very natural way. This will be particularly important when moving to the case of higher-order networks , as we will discuss next. Here we discuss two simple illustrative examples. Suppose \\(A=I\\) is the identity matrix. This is certainly not irreducible and in fact \\(Ax = x\\) for all nonnegative vectors \\(x\\in C_+\\) . While from a linear algebra point of view there is no preferred nonnegative solution to \\(x=Ax\\) , from the graph centrality perspective this is not the case. The graph of \\(A\\) is a set of isolated nodes, each with a self-loop of weight exactly one. Thus, a centrality score for these nodes would assign same score to all the nodes. This corresponds to the eigenvector \\(x = \\one\\) of \\(A\\) . While this is only one of the nonnegative solutions of \\(x=Ax\\) , \\(\\one\\) is the only nonlinear Perron eigenvector of a \u201cnonlinear version\u201d of the eigenvalue problem for \\(A\\) . Precisely, consider the nonlinear eigenvalue equation \\[\\begin{equation}\\label{eq:Af(x)} \\lambda x = A x^{1-\\varepsilon} \\end{equation}\\] It is easy to verify that the mapping \\(F(x) := Ax^{1-\\varepsilon}\\) is (multi)homogeneous with homogeneity matrix the scalar \\(\\M=1-\\varepsilon\\) . Thus, for any \\(\\varepsilon \\in (0,1)\\) , \\(\\rho(\\M)<1\\) and by the nonlinear Perron-Frobenius theorem we have that \\(\\eqref{eq:Af(x)}\\) has a unique positive solution. It is easy to verify that such solution is entrywise constant, yielding the desired centrality assignment. A similar situation holds for the singular vector case. Consider the graph in the figure below: This is a well-known example where HITS centrality may fail to output a reasonable centrality. While this graph is not a strongly connected graph, we all most probably agree that node \\(1\\) is the most relevant hub while node \\(6\\) is the most relevant authority . Despite this simple setting, using the dominant singular vectors of \\(A\\) as in the HITS model may fail to identify these relevant hub and authority nodes. In fact, for example, both the following pairs \\[\\begin{align*} x = (1,1,1,1,1, 0) \\qquad y = (0,1,1,1,1,4) \\\\ x = (4, 1, 1, 1, 1, 0) \\qquad y =(0,1,1,1,1,1) \\end{align*}\\] are (up to scaling) singular pairs of the dominant singular value of \\(A\\) . This shows that, in the first case, the hub vector fails to detect that node \\(1\\) is a better hub than nodes \\(2\\) \u2212 \\(5\\) . Similarly, in the second case, the authority vector fails to identify node \\(6\\) as a the best authority. Also in this case, a small nonlinear modification of \\(\\eqref{eq:sing-vec}\\) solves this issue. Consider the singular vector equation \\[\\begin{equation}\\label{eq:nonlin-sing-vec} \\lambda x = A^\\top y^\\alpha \\quad \\mu y = A x^\\beta\\, . \\end{equation}\\] It is easy to see that any such pair of vectors is an eigenvector of a multihomogeneous mapping with homogeneity matrix \\[ \\M = \\begin{bmatrix}0 & \\alpha \\\\ \\beta & 0 \\end{bmatrix}, \\qquad \\rho(H) = \\sqrt{|\\alpha\\beta|} \\] Thus for any \\(\\alpha\\beta<1\\) the nonlinear eigenvector equation \\(\\eqref{eq:nonlin-sing-vec}\\) has a unique solution. Below we show the value of the two solution vectors \\(x,y\\) for different choices of \\(\\alpha\\) and \\(\\beta\\) , showing that these two vectors capture the actual roles of nodes in this example graph. 1 2 3 4 5 6 7 8 9 \u2502 \u2502 \u03b1 = 0.5 \u2502 \u03b1 = 0.9 \u2502 \u03b1 = 0.5 \u2502 \u03b1 = 0.9 \u2502 \u03b2 = 0.5 \u2502 \u03b2 = 0.9 \u2502 \u03b2 = 0.9 \u2502 \u03b2 = 0.5 \u2502 \u2502entry\u2502 x1 \u2502 x2 \u2502 x3 \u2502 x4 \u2502 y1 \u2502 y2 \u2502 y3 \u2502 y4 \u2502 \u251c-----\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502----------\u2502 \u2502 1 \u2502 0.386488 \u2502 0.341489 \u2502 0.468535 \u2502 0.243379 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 \u2502 2 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 3 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 4 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 5 \u2502 0.153378 \u2502 0.164628 \u2502 0.132866 \u2502 0.189155 \u2502 0.153378 \u2502 0.164628 \u2502 0.189155 \u2502 0.132866 \u2502 \u2502 6 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 0.0 \u2502 0.386488 \u2502 0.341489 \u2502 0.243379 \u2502 0.468535 \u2502 J. P. Sch\u00e4fermeyer. On Edmund Landau\u2019s contribution to the ranking of chess players. Technical Report, Unpublished manuscript, 2019. \u21a9 P. Bonacich. Power and centrality: a family of measures. American Journal of Sociology , 92:1170\u20131182, 1987. \u21a9 Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The PageRank citation ranking: Bringing order to the web. Technical Report, Stanford InfoLab, 1999. \u21a9 Jon M Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM \\(JACM\\) , 46 \\(5\\) :604\u2013632, 1999. \u21a9","title":"Nonlinear eigenvector centrality"},{"location":"ch2/sec2/","text":"Higher-order network models While graph and networks are ubiquitous in the natural sciences, in many real-world applications we are confronted with higher-order interaction data. Relational data is full of interactions that happen in groups. For example, friendship relations very often happen in groups that are strictly larger than two individuals. Moreover, interactions naturally occur on multiple layers, for example work relations, sport relations, friendship relations, etc. To model higher-order interactions we need higher-order network models, which include multilayer networks , where we have a set of networks (so-called layers) with connections internally and across the layers, and non-dyadic networks , such as hypergraphs or simplicial complexes, where we have access to interactions involving multiple nodes. For simplicity, here we consider the following two settings: Multiplex : This is a set \\(\\{G_i\\}\\) of \\(m\\) graphs \\(G_i=(V,E_i)\\) \\(i=1,\\dots,m\\) , the layers , each defined on the same set of nodes but with possibly different edge sets Hypergraph : Just like a standard graph, this is a pair \\(H=(V,\\mathcal E)\\) where the set of hyperedges \\(\\mathcal E\\) is such that each \\(e\\in \\mathcal E\\) can involve an arbitrary number of nodes, rather than just two nodes as in the standard graph case. Eigenvector centrality for multiplex A multiplex network \\(\\{G_k\\}\\) can be naturally described by means of an adjacency tensor \\(T\\) with three modes \\[ T_{ijk} = \\begin{cases} 1 & i\\to j \\text{ on layer }k \\\\ 0 & \\text{otherwise} \\end{cases} \\] where \\(i\\to j\\) means that there is an edge from node \\(i\\) to node \\(j\\) , i.e. \\((i,j)\\in E_k\\) . How can we define a mutual reinforcing centrality score for nodes (and layers) in \\(\\{G_k\\}\\) so that a larger centrality is assigned to nodes that form links with other central nodes in highly influential layers? Here we discuss the model proposed in 1 based on \\(T\\) and a multihomogeneous order-preserving mapping associated to it. Other approaches are discussed in #Related work . In this model, mutual reinforcement happens at both layer and node levels, as layers are more influential if highly central nodes are active in them. Thus, if \\(x_i\\) denotes the centrality of node \\(i\\) and \\(y_k\\) the influence of layer \\(k\\) , we require that \\[\\begin{equation}\\label{eq:tensor_singvec} \\left\\{ \\begin{array}{l} \\sum_{j,k}T_{i,j,k}x_jy_k = \\lambda \\, |x_i|^{p} \\mathrm{sign}(x_i)\\\\[.5em] \\sum_{i,j}T_{i,j,k}x_ix_j = \\mu\\, |y_k|^{q} \\mathrm{sign}(y_k) \\end{array} \\right. \\end{equation}\\] which imposes that the \\(p\\) -power of the importance of node \\(i\\) is proportional to the sum of the importances of the nodes that point at \\(i\\) , times the influence of the layer where such connections take place and, similarly, defines the \\(q\\) -power of the influence of the layer \\(k\\) as being proportional the product of the centrality of the nodes that are connected in that layer. Since the centrality score is a positive number it is natural to add the constraint \\(\\lambda,\\mu>0\\) and \\(x,y\\succ 0\\) in \\(\\eqref{eq:tensor_singvec}\\) . Thus, our centrality problem boils down to a constrained nonlinear system of equations. When \\(p=q=1\\) , the equations are homogeneous polynomials and \\(\\eqref{eq:tensor_singvec}\\) is directly reminiscent of a singular vector equation. However, unlike the matrix case, even positive tensors may admit multiple solutions here, as shown by the following example: Consider the positive adjacency tensor \\(T\\) with entries \\[ \\left\\{ \\begin{array}{llll} T_{1,1,1} = 6 & T_{1,2,1} = 199/7 & T_{2,1,1} = 16/7 & T_{2,2,1} = 11 \\\\ T_{1,1,2} = 61/7 & T_{1,2,2} = 6 & T_{2,1,2} = 29 & T_{2,2,2} = 16/7 \\end{array}\\right. \\] then both the pair \\(x = \\frac 1 3(2,1)\\) , \\(y = \\frac 1 3(1,2)\\) and the pair \\(x = \\frac 1 4(1,3)\\) , \\(y = \\frac 1 4(3,1)\\) are positive solutions to \\(\\eqref{eq:tensor_singvec}\\) . In order to practically use the centrality model \\(\\eqref{eq:tensor_singvec}\\) we need it to define a unique score. To this end, we recast the vectors \\(x,y\\) solutions of \\(\\eqref{eq:tensor_singvec}\\) as eigenvectors of the multihomogeneous mapping \\[ F(x,y) := \\begin{bmatrix} F_1(x,y) \\\\ F_2(x,y) \\end{bmatrix} = \\begin{bmatrix} (T_{(1)}xy)^{1/p} \\\\ (T_{(3)}xx)^{1/q} \\end{bmatrix} = \\begin{bmatrix} (\\sum_{jk} T_{ijk} x_jy_k)^{1/p} \\\\ (\\sum_{ij}T_{ijk}x_ix_j)^{1/q} \\end{bmatrix}\\, . \\] In fact, it is easy to see that \\(\\eqref{eq:tensor_singvec}\\) holds iff \\(F(x,y)= (\\lambda,\\mu)\\krog (x,y)\\) . Moreover, one easily realizes that \\[ \\M = \\begin{bmatrix} 1/p & 1/p \\\\ 2/q & 0 \\end{bmatrix} \\] is the homogeneity matrix of \\(F\\) . Thus, using a slightly different cone than \\(C_+\\) , one obtains the following: Theorem. Let \\(n\\) and \\(\\ell\\) be the number of nodes and the number of layers in \\(\\{G_k\\}\\) , respectively. Consider the following set of pairs of vectors \\[ C_+(T) =\\left\\{(x,y)\\succeq 0 \\quad : \\quad \\begin{array}{c} x_i \\propto \\textstyle{\\sum_{j,k} T_{ijk}}, \\quad \\forall i = 1, \\dots, n\\\\ y_k \\propto \\textstyle{\\sum_{i,j} T_{ijk}}, \\quad \\forall k =1,\\dots, \\ell \\end{array}\\right\\}\\, . \\] This is a cone of vectors that depends on the nonzero pattern of \\(T\\) : for \\((x,y)\\in C_+(T)\\) , \\(x_i\\) is zero if and only if \\(T_{ijk}=0\\) for all \\(j,k\\) , that is node \\(i\\) is isolated in all the layers and, similarly, \\(y_k=0\\) if and only if layer \\(k\\) is empty (there is no edge in that layer). Then \\(\\rho(\\M) = ( \\sqrt{8p+q}+\\sqrt{q}) / (2p\\sqrt{q})\\) and if \\(\\rho(\\M)<1\\) we have: 1. The system of nonlinear equations \\(\\eqref{eq:tensor_singvec}\\) has a unique solution \\((x^*,y^*)\\in C_+(T)\\) such that \\(\\|x^*\\|=\\|y^*\\|=1\\) . 2. The power method iteration x = ones ( n , 1 ) y = ones ( l , 1 ) for r = 0 , 1 , 2 , 3 , .. x = F_1 ( x , y ) x = x / norm ( x ) y = F_2 ( x , y ) y = y / norm ( y ) converges to \\((x^*,y^*)\\) and after \\(m\\) steps we have \\[ \\|(x,y)-(x^*,y^*)\\|_{\\infty} \\leq \\rho(\\M)^m \\Big\\{ \\rho(\\M) \\, p\\, \\frac{\\max_{i\\in\\mathcal I} x_i^*}{\\min_{i'\\in\\mathcal I} x_{i'}^*}+\\, \\frac{\\max_{k\\in\\mathcal J} y_k^*}{\\min_{k'\\in\\mathcal J} y_{k'}^*}\\Big\\} \\] with \\(\\mathcal I=\\{i \\, : \\, x^{*}_i>0\\}\\) and \\(\\mathcal J =\\{k \\, :\\, y_k^{*}>0\\}\\) . Related work Other models for eigenvector centrality on multiplex networks are available. Many of them are based on a \u201cflattening\u201d or \u201cprojection\u201d approach which essentially transforms the multiplex into a graph and uses matrix eigenvectors on that graph to model node importances. These include, the aggregated graph of a multiplex graph 2 3 4 5 , where multiple layers with adjacency matrices \\(A_1,\\dots, A_m\\) are aggregated via a linear (possibly weighted) combination into a single denser adjacency matrix \\(A_{\\mathrm{agg}} = w_1A_1 + \\dots + w_mA_m\\) ; the supra-adjacency graph of a multilayer graph 6 7 8 , where a new graph of larger size is built by taking the Cartesian product of all the layers with weighted layer couplings obtaining a large adjacency matrix of the form \\[A_{\\mathrm{supra}}= \\begin{bmatrix} A_1 & I & \\dots & I \\\\ I & \\ddots & \\ddots &\\vdots \\\\ \\vdots & \\ddots & \\ddots & I \\\\ I & \\dots & I & A_m \\end{bmatrix} \\] Once a standard graph is obtained from the multiplex data, standard techniques can be applied to define and compute mutual-reinforcing centralities. In particular, the classical Perron\u2013Frobenius theory for matrices can be directly applied to the flattened graphs. To this end, it is interesting to notice that \\[ A_{\\mathrm{supra}} \\text{ is irreducible } \\iff A_{\\mathrm{agg}} \\text{ is irreducbile} \\] A different approach is proposed in 9 where the centrality for nodes and layers is computed by summing up powers of entries of the incidence matrix of the multiplex. Nonlinear eigenvector centrality for hypergraphs A hypergraph \\(H=(V,E)\\) consists of a set of nodes \\(V\\) and a set of hyperedges \\(E\\) , but, unlike graphs, an hyperedge \\(e\\in E\\) can contain an arbitrary number of nodes. In the weighted setting, we assume a weight function \\(w:E\\to \\RR_+\\) that assigns the weight \\(w(e)>0\\) to each hyperedge. Also in this setting, a relatively standard way to extend graph mappings and their eigenvectors is via a \u201cflattening\u201d or a \u201cprojection\u201d. These are forms of linearizations where the whole hypergraph is flattened into a standard graph to which standard centrality models are applied. There are many approaches that follow this line, including linear-weighted clique expansions 10 11 12 13 14 15 where hyperedges are replaced by cliques in the flattened graph, whose adjacency matrix becomes \\[\\begin{equation}\\label{eq:clique-expansion-adjacency} A_{ij} = \\sum_{e: \\, i,j\\in e}w(e) \\end{equation}\\] with \\(w(e)\\) the weights of the original hypergraph; clique averaging 16 , where the weights \\(w(e)\\) in the sum \\(\\eqref{eq:clique-expansion-adjacency}\\) are averaged with generalized mean functions; connectivity graph expansion 17 18 , where the weights in the clique expansion are based on hyperedge degrees, for example replacing \\(w(e)\\) with \\(1/(|e|-1)\\) in \\(\\eqref{eq:clique-expansion-adjacency}\\) ; the star expansion 19 , where the flattened graph is obtained by introducing new vertices for each hyperedge, which are then connected according to the hypergraph structure. Another popular approach for centrality on hypergraphs uses a tensor representation of the data and tensor eigenvectors. This is a particularly natural approach in the case of uniform hypergraphs. A \\(m\\) -uniform hypergraph is a hypergraph \\(H=(V,E)\\) such that each hyperedge \\(e\\in E\\) contains exactly \\(m\\) nodes. Thus, a \\(2\\) -uniform hypergraph is a standard graph. As every hyperedge contains exactly \\(m\\) nodes, we can associate to \\(H\\) the adjacency tensor \\(T\\) such that \\(T_{i_1,\\dots,i_m} = w(e)\\) if the hyperedge \\(e = \\{i_1,\\dots,i_m\\}\\in E\\) , and \\(T_{i_1,\\dots,i_m}=0\\) otherwise. Clearly, \\(T\\) coincides with the adjacency matrix of the graph when \\(m=2\\) . A centrality score \\(x_i\\) for the node \\(i\\in V\\) of a \\(m\\) -uniform hypergraph is defined in 20 as being linearly proportional to the product of the centrality scores of the nodes in each hyperedge that involves \\(i\\) . This mutual reinforcing relation boils down to the constrained eigenvector equation \\[\\begin{equation}\\label{eq:tensor_eig} \\sum_{i_2,\\dots,i_k}T_{i,i_2,\\dots,i_m}x_{i_2}x_{i_3}\\cdots x_{i_m} = \\lambda \\, |x_{i}|^{p-2}x_{i} \\end{equation}\\] with \\(x>0\\) , \\(\\lambda>0\\) and \\(p>1\\) . The special cases \\(p=2\\) and \\(p={m}\\) correspond to so-called \\(Z\\) - and \\(H\\) -eigenvectors for \\(T\\) . Beyond matrices and tensors Matrix and tensor eigenvector approaches are constrained to model the interaction of nodes at higher-order and across layers as either an additive or a multiplicative function. For example, in \\(\\eqref{eq:tensor_eig}\\) the importance \\(x_i\\) of node \\(i\\) is inherited by the product of the importances of the nodes on each hyperedge node \\(i\\) belongs to. Moreover, tensor representations seem inadequate to model general hypergraphs as they require a constant number of nodes in the hyperedges. We discuss below a model introduced in 21 , based on the incidence matrix of the hypergraph and a general nonlinear multihomogeneous mapping. If \\(n=|V|\\) and \\(m = |E|\\) , the incidence matrix and the diagonal weight matrix of \\(H\\) are \\(n\\times m\\) and the \\(m\\times m\\) matrices defined respectively as \\[ B_{i,e} = \\begin{cases} 1 & i\\in e \\\\ 0 & \\text{otherwise } \\end{cases} \\qquad W = \\begin{bmatrix} w(e_1) & & \\\\ & \\ddots & \\\\ & & w(e_m)\\end{bmatrix}\\, . \\] These matrices fully describe the hypergraph. For example, when each \\(e\\) has size exactly 2, i.e. we are considering a standard graph, then \\(BWB^\\top = A + D\\) where \\(A\\) is the adjacency matrix of \\(H\\) and \\(D = \\mathrm{Diag}(d_1, \\dots, d_n)\\) is the digonal matrix of the weighted node degrees \\(d_i = \\sum_{j}A_{ij}\\) . Similarly, for a general hypergraph \\(H\\) , we have \\(BWB^\\top = A+D\\) where \\(A\\) and \\(D\\) this time are the adjacency and degree matrices of the clique-expansion graph associated with \\(H\\) , as defined in \\(\\eqref{eq:clique-expansion-adjacency}\\) . However, unlike the clique-expanded adjacency matrix, \\(B\\) allows us to model the structure of \\(H\\) \u201cbefore\u201d the flattening step. This is the basis of the model below where we describe a spectral (thus mutually reinforcing) model for both nodes and edges of a hypergraph. Let \\(x\\) and \\(y\\) be nonnegative vectors whose entries will provide centrality scores for the nodes and hyperedges of \\(H\\) , respectively. We would like the importance \\(y_e\\) for an edge \\(e\\in E\\) to be a nonnegative number proportional to a function of the importances of the nodes in \\(e\\) , for example \\(y_e \\propto \\sum_{i\\in e} x_i\\) . Similarly, we require that the centrality \\(x_i\\) of node \\(i\\in V\\) is a nonnegative number proportional to a function of the importances of the edges it participates in, for example \\(x_i \\propto \\sum_{e: i\\in e}w(e)y_e\\) . As the centralities \\(x_i\\) and \\(y_e\\) are all nonnegative, these sums coincide with the weighted \\(\\ell^1\\) norm of specific sets of centrality scores. Thus, we can generalize this idea by considering the weighted \\(\\ell^p\\) norm of node and edge importances. This leads to \\[ x_i \\propto \\Big(\\sum_{e: i\\in e}w(e)y_e^p\\Big)^{1/p},\\qquad y_e \\propto \\Big(\\sum_{i\\in e} x_i^q\\Big)^{1/q}, \\] for some \\(p,q\\geq 1\\) . More generally, we can consider four functions \\(f,g,\\varphi,\\psi:\\RR_+\\to\\RR_+\\) of the nonnegative real line and require that \\[ x_i \\propto g\\Big(\\sum_{e: i\\in e}w(e)f(y_e)\\Big),\\qquad y_e \\propto \\psi\\Big(\\sum_{i\\in e}\\nu(i)\\varphi(x_i)\\Big) \\, . \\] If we extend real functions on vectors by defining them as mappings that act in a componentwise fashion, the previous relations can be compactly written as the following constrained nonlinear equations \\[\\begin{equation}\\label{eq:NEP} \\begin{cases} \\lambda x = g\\big(BW f(y)\\big) & \\\\ \\mu y = \\psi\\big(B^\\top \\varphi(x)\\big) \\end{cases}\\qquad x, y \\succ 0, \\quad \\lambda, \\mu > 0 \\, . \\end{equation}\\] Particular choices of the functions \\(f,g,\\psi\\) and \\(\\varphi\\) allow us to retrieve different models. If \\(f,g,\\psi\\) and \\(\\varphi\\) are all identity functions, then \\(\\eqref{eq:NEP}\\) boils down to a linear system of equations which is structurally reminiscent of the HITS centrality model for directed graphs, briefly reviewed above: the importance of a node is proportional to the sum of the importances of the hyperedges it belongs to and, vice-versa, the importancesof a hyperdge is proportional to the sum of the importances of the nodes it involves. When \\(f,g,\\psi\\) and \\(\\varphi\\) are logarithmic- and exponential-based, instead, the nonlinear eigenvector equation \\(\\eqref{eq:NEP}\\) allow us to extend the tensor eigenvector centrality model to the non-uniform hypergraph setting. In fact, the theorem below shows that, for uniform hypergraphs, the tensor-based eigenvector centrality \\(\\eqref{eq:tensor_eig}\\) is a particular case of \\(\\eqref{eq:NEP}\\) for logarithmic- and exponential-based nonlinear functions. Thus, when used on non-uniform hypergraphs, these choices of functions in \\(\\eqref{eq:NEP}\\) yield a tensor eigenvector like centrality for general hypergraphs. Theorem. Let \\(H\\) be a \\(k\\) -uniform hypergraph. If \\(x\\) is a positive solution of \\(\\eqref{eq:NEP}\\) with \\(f(x) = x\\) , \\(g(x) = x^{1/(p+1)}\\) , \\(\\psi(x) = e^{x}\\) and \\(\\varphi(x) = \\ln(x)\\) , then \\(x\\succ 0\\) is an eigenvector centrality solution of the tensor eigenvalue problem \\(\\eqref{eq:tensor_eig}\\) . As for HITS centrality, when \\(f=g=\\varphi=\\psi=\\text{id}\\) and we have no edge nor node weights (i.e. \\(W,N\\) are identity matrices), then \\(x, y\\) in \\(\\eqref{eq:NEP}\\) are the left and right singular vectors of a graph matrix, in this case \\(B\\) , and the matrix Perron-Frobenius theory tells us that if the bipartite graph with adjacency matrix \\[\\begin{equation}\\label{eq:bipartite} \\begin{bmatrix} 0 & B\\\\ B^\\top & 0 \\end{bmatrix} \\end{equation}\\] is connected, then \\(\\eqref{eq:NEP}\\) has a unique solution. Instead, when either \\(f,g,\\varphi\\) or \\(\\psi\\) is not linear, even the most basic question of existence of a solution to \\(\\eqref{eq:NEP}\\) may be not straightforward. However, for homogeneous functions \\(f,g,\\varphi\\) and \\(\\psi\\) , the nonlinear Perron-Frobenius theory for multihomogeneous operators allows us to give guarantees on existence, uniqueness and computability for the nonlinear singular-vector centrality model in \\(\\eqref{eq:NEP}\\) . Theorem. Let \\(f,g,\\varphi,\\psi\\) be order preserving and homogeneous of degrees \\(\\alpha,\\beta,\\gamma,\\delta\\) , respectively. Define the coefficient \\(\\rho = |\\alpha\\beta\\gamma\\delta|\\) . If either \\(\\rho<1\\) or \\(\\rho=1\\) , \\(f,g,\\varphi,\\psi\\) are differentiable and the bipartite graph with adjacency matrix as in \\(\\eqref{eq:bipartite}\\) is connected, then there exist unique \\(x^*,y^* \\succ 0\\) (up to scaling) and unique \\(\\lambda, \\mu >0\\) solution of \\(\\eqref{eq:NEP}\\) and the nonlinear power iteration x = ones ( n , 1 ) y = ones ( l , 1 ) for r = 0 , 1 , 2 , 3 , ... x = sqrt . ( x .* g ( B * W * f ( y ))) x = x / norm ( x ) y = sqrt . ( y .* \u03c8 ( B '* N * \u03c6 ( x ))) y = y / norm ( y ) converges to such \\(x^*,y^*\\) . You can find here the julia code that implements this algorithm and that runs it on a number of example datasets. The hypergraph sunflower A sunflower is a hypergraph whose hyperedges all have one common intersection in one single node, called the core . Let \\(u\\in V\\) be that intersection. Also let \\(r\\) be the number of petals (the hyperedges) each containing \\(|e_i|\\) nodes, for \\(i=1,\\dots,r\\) . By definition \\(u\\) is the only element in all the edges \\(\\cap_i e_i = \\{u\\}\\) . If \\(|e_i|=k+1\\) for all \\(i\\) , we say that the hypergraph is a uniform sunflower. The tensor eigenvector centrality of a uniform sunflower is studied for example in 20 . In this case we can assume that all the hyperedges have the same centrality score and that the same holds for all the nodes, besides the core, which is assigned a specific value. Assuming no weights, by symmetry we may impose the constraints \\(x_{v_i}=x_v\\) for all \\(v_i\\neq u\\) and \\(y_e = y\\) for all \\(e\\in E\\) in \\(\\eqref{eq:NEP}\\) to obtain \\[ x_v \\propto g(f(y)),\\qquad x_u \\propto g(rf(y)), \\qquad y \\propto \\psi(\\varphi(x_u) + k\\varphi(x_v)). \\] So, if \\(g\\) is homogeneous of degree \\(\\beta\\) we have \\(x_u/ x_v \\propto r^\\beta\\) . This shows that the node centrality assignment in the case of a uniform sunflower hypergraph only depends on the homogeneity degree of \\(g\\) and, in particular, when \\(\\beta\\to 0\\) all the centralities tend to coincide, while \\(x_u > x_v\\) for all \\(\\beta>0\\) , confirming and extending the observation in 20 for the setting of uniform hypergraph centralities based on tensor eigenvectors. The figure below illustrates this behaviour on an example uniform sunflower hypergraph with eight petals ( \\(r=8\\) ), each having three nodes ( \\(k=3\\) ). The figure shows the nodes of the hypergraph with a blue dot whose size is proportional to its centrality value computed according to three choices of the mappings in \\(\\eqref{eq:NEP}\\) : Linear : \\(f(x) = g(x) = \\psi(x) = \\varphi(x) = x\\) Log-exp : \\(f(x) = x\\) , \\(g(x) = x^{1/2}\\) , \\(\\psi(x) = e^{x}\\) and \\(\\varphi(x) = \\ln(x)\\) Max : \\(f(x) = g(x) = x\\) , \\(\\varphi(x) = x^{10}\\) , \\(\\psi(x) = x^{1/10}\\) The value of \\(\\beta\\) is \\(1\\) for both the max and the linear centrality, and \\(1/2\\) for log-exp centrality. Thus, all the three models assign the same centrality ranking: the core node \\(u\\) has strictly larger centrality, while all other nodes have same centrality score. The situation is different for the case of a nonuniform hypergraph sunflower where we have \\(r\\) petals each containing an arbitrary number of nodes. The figure below shows the computed centrality on an example sunflower and indicates that the three models capture significantly different centrality properties: All three models recognize the core node as the most central one, however while the linear model favours nodes that belong to large hyperedges, the multiplicative log-exp model behaves in the opposite way assigning a larger centrality score to nodes being part of small hyperedges. Finally, the max model behaves like in the uniform case, assigning the same centrality value to all the nodes in the petals (core node excluded). For this hypergraph, we observe that the edge centrality follows directly from the node one: for the linear model the edge centrality is proportional to the number of nodes in the edge, for the log-exp model it is inversely proportional to the number of nodes, while for the max model all edges have the same centrality. Francesco Tudisco, Francesca Arrigo, and Antoine Gautier. Node and layer eigenvector centralities for multiplex networks. SIAM Journal on Applied Mathematics , 78 \\(2\\) :853\u2013876, 2018. \u21a9 Luis Sol\u00e1, Miguel Romance, Regino Criado, Julio Flores, Alejandro Garc\u00eda del Amo, and Stefano Boccaletti. Eigenvector centrality of nodes in multiplex networks. Chaos: An Interdisciplinary Journal of Nonlinear Science , 23 \\(3\\) :033131, 2013. \u21a9 Federico Battiston, Vincenzo Nicosia, and Vito Latora. Structural measures for multiplex networks. Physical Review E , 89 \\(3\\) :032804, 2014. \u21a9 Dengyong Zhou and Christopher JC Burges. Spectral clustering and transductive learning with multiple views. In International Conference on Machine Learning \\(ICML\\) , 1159\u20131166. 2007. \u21a9 Koji Tsuda, Hyunjung Shin, and Bernhard Sch\u00f6lkopf. Fast protein classification with multiple networks. Bioinformatics , 21 \\(suppl\\_2\\) :ii59\u2013ii65, 2005. \u21a9 Manlio De Domenico, Elisa Omodei, Sergio G\u00f3mez, Alex Arenas, and Albert Sol\u00e9-Ribalta. Centrality in interconnected multilayer networks. Nature Communications , 6 \\(arXiv: 1311\\.2906\\) :6868, 2013. \u21a9 Dane Taylor, Sean A Myers, Aaron Clauset, Mason A Porter, and Peter J Mucha. Eigenvector-based centrality measures for temporal networks. Multiscale Modeling & Simulation \\(SIAM\\) , 15 \\(1\\) :537\u2013574, 2017. \u21a9 Dane Taylor, Mason A Porter, and Peter J Mucha. Tunable eigenvector-based centralities for multiplex and temporal networks. Multiscale Modeling & Simulation \\(SIAM\\) , 19 \\(1\\) :113\u2013147, 2021. \u21a9 Christoph Rahmede, Jacopo Iacovacci, Alex Arenas, and Ginestra Bianconi. Centralities of nodes and influences of layers in large multiplex networks. Journal of Complex Networks , 6 \\(5\\) :733\u2013752, 2018. \u21a9 Timoteo Carletti, Federico Battiston, Giulia Cencetti, and Duccio Fanelli. Random walks on hypergraphs. Physical Review E , 101 \\(2\\) :022308, 2020. \u21a9 Juan Alberto Rodriguez. On the Laplacian eigenvalues and metric parameters of hypergraphs. Linear and Multilinear Algebra , 50 \\(1\\) :1\u201314, 2002. \u21a9 Juan Alberto Rodriguez. On the Laplacian spectrum and walk-regular hypergraphs. Linear and Multilinear Algebra , 51 \\(3\\) :285\u2013297, 2003. \u21a9 Juan Alberto Rodriguez. Laplacian eigenvalues and partition problems in hypergraphs. Applied Mathematics Letters , 22 \\(6\\) :916\u2013921, 2009. \u21a9 Sameer Agarwal, Kristin Branson, and Serge Belongie. Higher order learning with graphs. In International Conference on Machine Learning \\(ICML\\) , 17\u201324. 2006. \u21a9 Dengyong Zhou, Jiayuan Huang, and Bernhard Sch\u00f6lkopf. Learning with hypergraphs: clustering, classification, and embedding. In Advances in Neural Information Processing Systems \\(NeurIPS\\) , 1601\u20131608. 2007. \u21a9 Sameer Agarwal, Jongwoo Lim, Lihi Zelnik-Manor, Pietro Perona, David Kriegman, and Serge Belongie. Beyond pairwise clustering. In Conference on Computer Vision and Pattern Recognition \\(CVPR\\) , volume 2, 838\u2013845. IEEE, 2005. \u21a9 Anirban Banerjee. On the spectrum of hypergraphs. Linear Algebra and its Applications , 614:82\u2013110, 2021. \u21a9 Guilherme Ferraz de Arruda, Michele Tizzani, and Yamir Moreno. Phase transitions and stability of dynamical processes on hypergraphs. Communications Physics , 4 \\(1\\) :1\u20139, 2021. \u21a9 Jason Y Zien, Martine DF Schlag, and Pak K Chan. Multilevel spectral hypergraph partitioning with arbitrary vertex sizes. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 18 \\(9\\) :1389\u20131399, 1999. \u21a9 Austin R Benson. Three hypergraph eigenvector centralities. SIAM Journal on Mathematics of Data Science , 1 \\(2\\) :293\u2013312, 2019. \u21a9 \u21a9 \u21a9 \u21a9 Francesco Tudisco and Desmond J. Higham. Node and edge eigenvector centrality for hypergraphs. arXiv:2101.06215 , 2021. \u21a9","title":"Higher-order network models"},{"location":"ch2/sec2/#higher-order_network_models","text":"While graph and networks are ubiquitous in the natural sciences, in many real-world applications we are confronted with higher-order interaction data. Relational data is full of interactions that happen in groups. For example, friendship relations very often happen in groups that are strictly larger than two individuals. Moreover, interactions naturally occur on multiple layers, for example work relations, sport relations, friendship relations, etc. To model higher-order interactions we need higher-order network models, which include multilayer networks , where we have a set of networks (so-called layers) with connections internally and across the layers, and non-dyadic networks , such as hypergraphs or simplicial complexes, where we have access to interactions involving multiple nodes. For simplicity, here we consider the following two settings: Multiplex : This is a set \\(\\{G_i\\}\\) of \\(m\\) graphs \\(G_i=(V,E_i)\\) \\(i=1,\\dots,m\\) , the layers , each defined on the same set of nodes but with possibly different edge sets Hypergraph : Just like a standard graph, this is a pair \\(H=(V,\\mathcal E)\\) where the set of hyperedges \\(\\mathcal E\\) is such that each \\(e\\in \\mathcal E\\) can involve an arbitrary number of nodes, rather than just two nodes as in the standard graph case.","title":"Higher-order network models"},{"location":"ch2/sec2/#eigenvector_centrality_for_multiplex","text":"A multiplex network \\(\\{G_k\\}\\) can be naturally described by means of an adjacency tensor \\(T\\) with three modes \\[ T_{ijk} = \\begin{cases} 1 & i\\to j \\text{ on layer }k \\\\ 0 & \\text{otherwise} \\end{cases} \\] where \\(i\\to j\\) means that there is an edge from node \\(i\\) to node \\(j\\) , i.e. \\((i,j)\\in E_k\\) . How can we define a mutual reinforcing centrality score for nodes (and layers) in \\(\\{G_k\\}\\) so that a larger centrality is assigned to nodes that form links with other central nodes in highly influential layers? Here we discuss the model proposed in 1 based on \\(T\\) and a multihomogeneous order-preserving mapping associated to it. Other approaches are discussed in #Related work . In this model, mutual reinforcement happens at both layer and node levels, as layers are more influential if highly central nodes are active in them. Thus, if \\(x_i\\) denotes the centrality of node \\(i\\) and \\(y_k\\) the influence of layer \\(k\\) , we require that \\[\\begin{equation}\\label{eq:tensor_singvec} \\left\\{ \\begin{array}{l} \\sum_{j,k}T_{i,j,k}x_jy_k = \\lambda \\, |x_i|^{p} \\mathrm{sign}(x_i)\\\\[.5em] \\sum_{i,j}T_{i,j,k}x_ix_j = \\mu\\, |y_k|^{q} \\mathrm{sign}(y_k) \\end{array} \\right. \\end{equation}\\] which imposes that the \\(p\\) -power of the importance of node \\(i\\) is proportional to the sum of the importances of the nodes that point at \\(i\\) , times the influence of the layer where such connections take place and, similarly, defines the \\(q\\) -power of the influence of the layer \\(k\\) as being proportional the product of the centrality of the nodes that are connected in that layer. Since the centrality score is a positive number it is natural to add the constraint \\(\\lambda,\\mu>0\\) and \\(x,y\\succ 0\\) in \\(\\eqref{eq:tensor_singvec}\\) . Thus, our centrality problem boils down to a constrained nonlinear system of equations. When \\(p=q=1\\) , the equations are homogeneous polynomials and \\(\\eqref{eq:tensor_singvec}\\) is directly reminiscent of a singular vector equation. However, unlike the matrix case, even positive tensors may admit multiple solutions here, as shown by the following example: Consider the positive adjacency tensor \\(T\\) with entries \\[ \\left\\{ \\begin{array}{llll} T_{1,1,1} = 6 & T_{1,2,1} = 199/7 & T_{2,1,1} = 16/7 & T_{2,2,1} = 11 \\\\ T_{1,1,2} = 61/7 & T_{1,2,2} = 6 & T_{2,1,2} = 29 & T_{2,2,2} = 16/7 \\end{array}\\right. \\] then both the pair \\(x = \\frac 1 3(2,1)\\) , \\(y = \\frac 1 3(1,2)\\) and the pair \\(x = \\frac 1 4(1,3)\\) , \\(y = \\frac 1 4(3,1)\\) are positive solutions to \\(\\eqref{eq:tensor_singvec}\\) . In order to practically use the centrality model \\(\\eqref{eq:tensor_singvec}\\) we need it to define a unique score. To this end, we recast the vectors \\(x,y\\) solutions of \\(\\eqref{eq:tensor_singvec}\\) as eigenvectors of the multihomogeneous mapping \\[ F(x,y) := \\begin{bmatrix} F_1(x,y) \\\\ F_2(x,y) \\end{bmatrix} = \\begin{bmatrix} (T_{(1)}xy)^{1/p} \\\\ (T_{(3)}xx)^{1/q} \\end{bmatrix} = \\begin{bmatrix} (\\sum_{jk} T_{ijk} x_jy_k)^{1/p} \\\\ (\\sum_{ij}T_{ijk}x_ix_j)^{1/q} \\end{bmatrix}\\, . \\] In fact, it is easy to see that \\(\\eqref{eq:tensor_singvec}\\) holds iff \\(F(x,y)= (\\lambda,\\mu)\\krog (x,y)\\) . Moreover, one easily realizes that \\[ \\M = \\begin{bmatrix} 1/p & 1/p \\\\ 2/q & 0 \\end{bmatrix} \\] is the homogeneity matrix of \\(F\\) . Thus, using a slightly different cone than \\(C_+\\) , one obtains the following: Theorem. Let \\(n\\) and \\(\\ell\\) be the number of nodes and the number of layers in \\(\\{G_k\\}\\) , respectively. Consider the following set of pairs of vectors \\[ C_+(T) =\\left\\{(x,y)\\succeq 0 \\quad : \\quad \\begin{array}{c} x_i \\propto \\textstyle{\\sum_{j,k} T_{ijk}}, \\quad \\forall i = 1, \\dots, n\\\\ y_k \\propto \\textstyle{\\sum_{i,j} T_{ijk}}, \\quad \\forall k =1,\\dots, \\ell \\end{array}\\right\\}\\, . \\] This is a cone of vectors that depends on the nonzero pattern of \\(T\\) : for \\((x,y)\\in C_+(T)\\) , \\(x_i\\) is zero if and only if \\(T_{ijk}=0\\) for all \\(j,k\\) , that is node \\(i\\) is isolated in all the layers and, similarly, \\(y_k=0\\) if and only if layer \\(k\\) is empty (there is no edge in that layer). Then \\(\\rho(\\M) = ( \\sqrt{8p+q}+\\sqrt{q}) / (2p\\sqrt{q})\\) and if \\(\\rho(\\M)<1\\) we have: 1. The system of nonlinear equations \\(\\eqref{eq:tensor_singvec}\\) has a unique solution \\((x^*,y^*)\\in C_+(T)\\) such that \\(\\|x^*\\|=\\|y^*\\|=1\\) . 2. The power method iteration x = ones ( n , 1 ) y = ones ( l , 1 ) for r = 0 , 1 , 2 , 3 , .. x = F_1 ( x , y ) x = x / norm ( x ) y = F_2 ( x , y ) y = y / norm ( y ) converges to \\((x^*,y^*)\\) and after \\(m\\) steps we have \\[ \\|(x,y)-(x^*,y^*)\\|_{\\infty} \\leq \\rho(\\M)^m \\Big\\{ \\rho(\\M) \\, p\\, \\frac{\\max_{i\\in\\mathcal I} x_i^*}{\\min_{i'\\in\\mathcal I} x_{i'}^*}+\\, \\frac{\\max_{k\\in\\mathcal J} y_k^*}{\\min_{k'\\in\\mathcal J} y_{k'}^*}\\Big\\} \\] with \\(\\mathcal I=\\{i \\, : \\, x^{*}_i>0\\}\\) and \\(\\mathcal J =\\{k \\, :\\, y_k^{*}>0\\}\\) .","title":"Eigenvector centrality for multiplex"},{"location":"ch2/sec2/#related_work","text":"Other models for eigenvector centrality on multiplex networks are available. Many of them are based on a \u201cflattening\u201d or \u201cprojection\u201d approach which essentially transforms the multiplex into a graph and uses matrix eigenvectors on that graph to model node importances. These include, the aggregated graph of a multiplex graph 2 3 4 5 , where multiple layers with adjacency matrices \\(A_1,\\dots, A_m\\) are aggregated via a linear (possibly weighted) combination into a single denser adjacency matrix \\(A_{\\mathrm{agg}} = w_1A_1 + \\dots + w_mA_m\\) ; the supra-adjacency graph of a multilayer graph 6 7 8 , where a new graph of larger size is built by taking the Cartesian product of all the layers with weighted layer couplings obtaining a large adjacency matrix of the form \\[A_{\\mathrm{supra}}= \\begin{bmatrix} A_1 & I & \\dots & I \\\\ I & \\ddots & \\ddots &\\vdots \\\\ \\vdots & \\ddots & \\ddots & I \\\\ I & \\dots & I & A_m \\end{bmatrix} \\] Once a standard graph is obtained from the multiplex data, standard techniques can be applied to define and compute mutual-reinforcing centralities. In particular, the classical Perron\u2013Frobenius theory for matrices can be directly applied to the flattened graphs. To this end, it is interesting to notice that \\[ A_{\\mathrm{supra}} \\text{ is irreducible } \\iff A_{\\mathrm{agg}} \\text{ is irreducbile} \\] A different approach is proposed in 9 where the centrality for nodes and layers is computed by summing up powers of entries of the incidence matrix of the multiplex.","title":"Related work"},{"location":"ch2/sec2/#nonlinear_eigenvector_centrality_for_hypergraphs","text":"A hypergraph \\(H=(V,E)\\) consists of a set of nodes \\(V\\) and a set of hyperedges \\(E\\) , but, unlike graphs, an hyperedge \\(e\\in E\\) can contain an arbitrary number of nodes. In the weighted setting, we assume a weight function \\(w:E\\to \\RR_+\\) that assigns the weight \\(w(e)>0\\) to each hyperedge. Also in this setting, a relatively standard way to extend graph mappings and their eigenvectors is via a \u201cflattening\u201d or a \u201cprojection\u201d. These are forms of linearizations where the whole hypergraph is flattened into a standard graph to which standard centrality models are applied. There are many approaches that follow this line, including linear-weighted clique expansions 10 11 12 13 14 15 where hyperedges are replaced by cliques in the flattened graph, whose adjacency matrix becomes \\[\\begin{equation}\\label{eq:clique-expansion-adjacency} A_{ij} = \\sum_{e: \\, i,j\\in e}w(e) \\end{equation}\\] with \\(w(e)\\) the weights of the original hypergraph; clique averaging 16 , where the weights \\(w(e)\\) in the sum \\(\\eqref{eq:clique-expansion-adjacency}\\) are averaged with generalized mean functions; connectivity graph expansion 17 18 , where the weights in the clique expansion are based on hyperedge degrees, for example replacing \\(w(e)\\) with \\(1/(|e|-1)\\) in \\(\\eqref{eq:clique-expansion-adjacency}\\) ; the star expansion 19 , where the flattened graph is obtained by introducing new vertices for each hyperedge, which are then connected according to the hypergraph structure. Another popular approach for centrality on hypergraphs uses a tensor representation of the data and tensor eigenvectors. This is a particularly natural approach in the case of uniform hypergraphs. A \\(m\\) -uniform hypergraph is a hypergraph \\(H=(V,E)\\) such that each hyperedge \\(e\\in E\\) contains exactly \\(m\\) nodes. Thus, a \\(2\\) -uniform hypergraph is a standard graph. As every hyperedge contains exactly \\(m\\) nodes, we can associate to \\(H\\) the adjacency tensor \\(T\\) such that \\(T_{i_1,\\dots,i_m} = w(e)\\) if the hyperedge \\(e = \\{i_1,\\dots,i_m\\}\\in E\\) , and \\(T_{i_1,\\dots,i_m}=0\\) otherwise. Clearly, \\(T\\) coincides with the adjacency matrix of the graph when \\(m=2\\) . A centrality score \\(x_i\\) for the node \\(i\\in V\\) of a \\(m\\) -uniform hypergraph is defined in 20 as being linearly proportional to the product of the centrality scores of the nodes in each hyperedge that involves \\(i\\) . This mutual reinforcing relation boils down to the constrained eigenvector equation \\[\\begin{equation}\\label{eq:tensor_eig} \\sum_{i_2,\\dots,i_k}T_{i,i_2,\\dots,i_m}x_{i_2}x_{i_3}\\cdots x_{i_m} = \\lambda \\, |x_{i}|^{p-2}x_{i} \\end{equation}\\] with \\(x>0\\) , \\(\\lambda>0\\) and \\(p>1\\) . The special cases \\(p=2\\) and \\(p={m}\\) correspond to so-called \\(Z\\) - and \\(H\\) -eigenvectors for \\(T\\) .","title":"Nonlinear eigenvector centrality for hypergraphs"},{"location":"ch2/sec2/#beyond_matrices_and_tensors","text":"Matrix and tensor eigenvector approaches are constrained to model the interaction of nodes at higher-order and across layers as either an additive or a multiplicative function. For example, in \\(\\eqref{eq:tensor_eig}\\) the importance \\(x_i\\) of node \\(i\\) is inherited by the product of the importances of the nodes on each hyperedge node \\(i\\) belongs to. Moreover, tensor representations seem inadequate to model general hypergraphs as they require a constant number of nodes in the hyperedges. We discuss below a model introduced in 21 , based on the incidence matrix of the hypergraph and a general nonlinear multihomogeneous mapping. If \\(n=|V|\\) and \\(m = |E|\\) , the incidence matrix and the diagonal weight matrix of \\(H\\) are \\(n\\times m\\) and the \\(m\\times m\\) matrices defined respectively as \\[ B_{i,e} = \\begin{cases} 1 & i\\in e \\\\ 0 & \\text{otherwise } \\end{cases} \\qquad W = \\begin{bmatrix} w(e_1) & & \\\\ & \\ddots & \\\\ & & w(e_m)\\end{bmatrix}\\, . \\] These matrices fully describe the hypergraph. For example, when each \\(e\\) has size exactly 2, i.e. we are considering a standard graph, then \\(BWB^\\top = A + D\\) where \\(A\\) is the adjacency matrix of \\(H\\) and \\(D = \\mathrm{Diag}(d_1, \\dots, d_n)\\) is the digonal matrix of the weighted node degrees \\(d_i = \\sum_{j}A_{ij}\\) . Similarly, for a general hypergraph \\(H\\) , we have \\(BWB^\\top = A+D\\) where \\(A\\) and \\(D\\) this time are the adjacency and degree matrices of the clique-expansion graph associated with \\(H\\) , as defined in \\(\\eqref{eq:clique-expansion-adjacency}\\) . However, unlike the clique-expanded adjacency matrix, \\(B\\) allows us to model the structure of \\(H\\) \u201cbefore\u201d the flattening step. This is the basis of the model below where we describe a spectral (thus mutually reinforcing) model for both nodes and edges of a hypergraph. Let \\(x\\) and \\(y\\) be nonnegative vectors whose entries will provide centrality scores for the nodes and hyperedges of \\(H\\) , respectively. We would like the importance \\(y_e\\) for an edge \\(e\\in E\\) to be a nonnegative number proportional to a function of the importances of the nodes in \\(e\\) , for example \\(y_e \\propto \\sum_{i\\in e} x_i\\) . Similarly, we require that the centrality \\(x_i\\) of node \\(i\\in V\\) is a nonnegative number proportional to a function of the importances of the edges it participates in, for example \\(x_i \\propto \\sum_{e: i\\in e}w(e)y_e\\) . As the centralities \\(x_i\\) and \\(y_e\\) are all nonnegative, these sums coincide with the weighted \\(\\ell^1\\) norm of specific sets of centrality scores. Thus, we can generalize this idea by considering the weighted \\(\\ell^p\\) norm of node and edge importances. This leads to \\[ x_i \\propto \\Big(\\sum_{e: i\\in e}w(e)y_e^p\\Big)^{1/p},\\qquad y_e \\propto \\Big(\\sum_{i\\in e} x_i^q\\Big)^{1/q}, \\] for some \\(p,q\\geq 1\\) . More generally, we can consider four functions \\(f,g,\\varphi,\\psi:\\RR_+\\to\\RR_+\\) of the nonnegative real line and require that \\[ x_i \\propto g\\Big(\\sum_{e: i\\in e}w(e)f(y_e)\\Big),\\qquad y_e \\propto \\psi\\Big(\\sum_{i\\in e}\\nu(i)\\varphi(x_i)\\Big) \\, . \\] If we extend real functions on vectors by defining them as mappings that act in a componentwise fashion, the previous relations can be compactly written as the following constrained nonlinear equations \\[\\begin{equation}\\label{eq:NEP} \\begin{cases} \\lambda x = g\\big(BW f(y)\\big) & \\\\ \\mu y = \\psi\\big(B^\\top \\varphi(x)\\big) \\end{cases}\\qquad x, y \\succ 0, \\quad \\lambda, \\mu > 0 \\, . \\end{equation}\\] Particular choices of the functions \\(f,g,\\psi\\) and \\(\\varphi\\) allow us to retrieve different models. If \\(f,g,\\psi\\) and \\(\\varphi\\) are all identity functions, then \\(\\eqref{eq:NEP}\\) boils down to a linear system of equations which is structurally reminiscent of the HITS centrality model for directed graphs, briefly reviewed above: the importance of a node is proportional to the sum of the importances of the hyperedges it belongs to and, vice-versa, the importancesof a hyperdge is proportional to the sum of the importances of the nodes it involves. When \\(f,g,\\psi\\) and \\(\\varphi\\) are logarithmic- and exponential-based, instead, the nonlinear eigenvector equation \\(\\eqref{eq:NEP}\\) allow us to extend the tensor eigenvector centrality model to the non-uniform hypergraph setting. In fact, the theorem below shows that, for uniform hypergraphs, the tensor-based eigenvector centrality \\(\\eqref{eq:tensor_eig}\\) is a particular case of \\(\\eqref{eq:NEP}\\) for logarithmic- and exponential-based nonlinear functions. Thus, when used on non-uniform hypergraphs, these choices of functions in \\(\\eqref{eq:NEP}\\) yield a tensor eigenvector like centrality for general hypergraphs. Theorem. Let \\(H\\) be a \\(k\\) -uniform hypergraph. If \\(x\\) is a positive solution of \\(\\eqref{eq:NEP}\\) with \\(f(x) = x\\) , \\(g(x) = x^{1/(p+1)}\\) , \\(\\psi(x) = e^{x}\\) and \\(\\varphi(x) = \\ln(x)\\) , then \\(x\\succ 0\\) is an eigenvector centrality solution of the tensor eigenvalue problem \\(\\eqref{eq:tensor_eig}\\) . As for HITS centrality, when \\(f=g=\\varphi=\\psi=\\text{id}\\) and we have no edge nor node weights (i.e. \\(W,N\\) are identity matrices), then \\(x, y\\) in \\(\\eqref{eq:NEP}\\) are the left and right singular vectors of a graph matrix, in this case \\(B\\) , and the matrix Perron-Frobenius theory tells us that if the bipartite graph with adjacency matrix \\[\\begin{equation}\\label{eq:bipartite} \\begin{bmatrix} 0 & B\\\\ B^\\top & 0 \\end{bmatrix} \\end{equation}\\] is connected, then \\(\\eqref{eq:NEP}\\) has a unique solution. Instead, when either \\(f,g,\\varphi\\) or \\(\\psi\\) is not linear, even the most basic question of existence of a solution to \\(\\eqref{eq:NEP}\\) may be not straightforward. However, for homogeneous functions \\(f,g,\\varphi\\) and \\(\\psi\\) , the nonlinear Perron-Frobenius theory for multihomogeneous operators allows us to give guarantees on existence, uniqueness and computability for the nonlinear singular-vector centrality model in \\(\\eqref{eq:NEP}\\) . Theorem. Let \\(f,g,\\varphi,\\psi\\) be order preserving and homogeneous of degrees \\(\\alpha,\\beta,\\gamma,\\delta\\) , respectively. Define the coefficient \\(\\rho = |\\alpha\\beta\\gamma\\delta|\\) . If either \\(\\rho<1\\) or \\(\\rho=1\\) , \\(f,g,\\varphi,\\psi\\) are differentiable and the bipartite graph with adjacency matrix as in \\(\\eqref{eq:bipartite}\\) is connected, then there exist unique \\(x^*,y^* \\succ 0\\) (up to scaling) and unique \\(\\lambda, \\mu >0\\) solution of \\(\\eqref{eq:NEP}\\) and the nonlinear power iteration x = ones ( n , 1 ) y = ones ( l , 1 ) for r = 0 , 1 , 2 , 3 , ... x = sqrt . ( x .* g ( B * W * f ( y ))) x = x / norm ( x ) y = sqrt . ( y .* \u03c8 ( B '* N * \u03c6 ( x ))) y = y / norm ( y ) converges to such \\(x^*,y^*\\) . You can find here the julia code that implements this algorithm and that runs it on a number of example datasets.","title":"Beyond matrices and tensors"},{"location":"ch2/sec2/#the_hypergraph_sunflower","text":"A sunflower is a hypergraph whose hyperedges all have one common intersection in one single node, called the core . Let \\(u\\in V\\) be that intersection. Also let \\(r\\) be the number of petals (the hyperedges) each containing \\(|e_i|\\) nodes, for \\(i=1,\\dots,r\\) . By definition \\(u\\) is the only element in all the edges \\(\\cap_i e_i = \\{u\\}\\) . If \\(|e_i|=k+1\\) for all \\(i\\) , we say that the hypergraph is a uniform sunflower. The tensor eigenvector centrality of a uniform sunflower is studied for example in 20 . In this case we can assume that all the hyperedges have the same centrality score and that the same holds for all the nodes, besides the core, which is assigned a specific value. Assuming no weights, by symmetry we may impose the constraints \\(x_{v_i}=x_v\\) for all \\(v_i\\neq u\\) and \\(y_e = y\\) for all \\(e\\in E\\) in \\(\\eqref{eq:NEP}\\) to obtain \\[ x_v \\propto g(f(y)),\\qquad x_u \\propto g(rf(y)), \\qquad y \\propto \\psi(\\varphi(x_u) + k\\varphi(x_v)). \\] So, if \\(g\\) is homogeneous of degree \\(\\beta\\) we have \\(x_u/ x_v \\propto r^\\beta\\) . This shows that the node centrality assignment in the case of a uniform sunflower hypergraph only depends on the homogeneity degree of \\(g\\) and, in particular, when \\(\\beta\\to 0\\) all the centralities tend to coincide, while \\(x_u > x_v\\) for all \\(\\beta>0\\) , confirming and extending the observation in 20 for the setting of uniform hypergraph centralities based on tensor eigenvectors. The figure below illustrates this behaviour on an example uniform sunflower hypergraph with eight petals ( \\(r=8\\) ), each having three nodes ( \\(k=3\\) ). The figure shows the nodes of the hypergraph with a blue dot whose size is proportional to its centrality value computed according to three choices of the mappings in \\(\\eqref{eq:NEP}\\) : Linear : \\(f(x) = g(x) = \\psi(x) = \\varphi(x) = x\\) Log-exp : \\(f(x) = x\\) , \\(g(x) = x^{1/2}\\) , \\(\\psi(x) = e^{x}\\) and \\(\\varphi(x) = \\ln(x)\\) Max : \\(f(x) = g(x) = x\\) , \\(\\varphi(x) = x^{10}\\) , \\(\\psi(x) = x^{1/10}\\) The value of \\(\\beta\\) is \\(1\\) for both the max and the linear centrality, and \\(1/2\\) for log-exp centrality. Thus, all the three models assign the same centrality ranking: the core node \\(u\\) has strictly larger centrality, while all other nodes have same centrality score. The situation is different for the case of a nonuniform hypergraph sunflower where we have \\(r\\) petals each containing an arbitrary number of nodes. The figure below shows the computed centrality on an example sunflower and indicates that the three models capture significantly different centrality properties: All three models recognize the core node as the most central one, however while the linear model favours nodes that belong to large hyperedges, the multiplicative log-exp model behaves in the opposite way assigning a larger centrality score to nodes being part of small hyperedges. Finally, the max model behaves like in the uniform case, assigning the same centrality value to all the nodes in the petals (core node excluded). For this hypergraph, we observe that the edge centrality follows directly from the node one: for the linear model the edge centrality is proportional to the number of nodes in the edge, for the log-exp model it is inversely proportional to the number of nodes, while for the max model all edges have the same centrality. Francesco Tudisco, Francesca Arrigo, and Antoine Gautier. Node and layer eigenvector centralities for multiplex networks. SIAM Journal on Applied Mathematics , 78 \\(2\\) :853\u2013876, 2018. \u21a9 Luis Sol\u00e1, Miguel Romance, Regino Criado, Julio Flores, Alejandro Garc\u00eda del Amo, and Stefano Boccaletti. Eigenvector centrality of nodes in multiplex networks. Chaos: An Interdisciplinary Journal of Nonlinear Science , 23 \\(3\\) :033131, 2013. \u21a9 Federico Battiston, Vincenzo Nicosia, and Vito Latora. Structural measures for multiplex networks. Physical Review E , 89 \\(3\\) :032804, 2014. \u21a9 Dengyong Zhou and Christopher JC Burges. Spectral clustering and transductive learning with multiple views. In International Conference on Machine Learning \\(ICML\\) , 1159\u20131166. 2007. \u21a9 Koji Tsuda, Hyunjung Shin, and Bernhard Sch\u00f6lkopf. Fast protein classification with multiple networks. Bioinformatics , 21 \\(suppl\\_2\\) :ii59\u2013ii65, 2005. \u21a9 Manlio De Domenico, Elisa Omodei, Sergio G\u00f3mez, Alex Arenas, and Albert Sol\u00e9-Ribalta. Centrality in interconnected multilayer networks. Nature Communications , 6 \\(arXiv: 1311\\.2906\\) :6868, 2013. \u21a9 Dane Taylor, Sean A Myers, Aaron Clauset, Mason A Porter, and Peter J Mucha. Eigenvector-based centrality measures for temporal networks. Multiscale Modeling & Simulation \\(SIAM\\) , 15 \\(1\\) :537\u2013574, 2017. \u21a9 Dane Taylor, Mason A Porter, and Peter J Mucha. Tunable eigenvector-based centralities for multiplex and temporal networks. Multiscale Modeling & Simulation \\(SIAM\\) , 19 \\(1\\) :113\u2013147, 2021. \u21a9 Christoph Rahmede, Jacopo Iacovacci, Alex Arenas, and Ginestra Bianconi. Centralities of nodes and influences of layers in large multiplex networks. Journal of Complex Networks , 6 \\(5\\) :733\u2013752, 2018. \u21a9 Timoteo Carletti, Federico Battiston, Giulia Cencetti, and Duccio Fanelli. Random walks on hypergraphs. Physical Review E , 101 \\(2\\) :022308, 2020. \u21a9 Juan Alberto Rodriguez. On the Laplacian eigenvalues and metric parameters of hypergraphs. Linear and Multilinear Algebra , 50 \\(1\\) :1\u201314, 2002. \u21a9 Juan Alberto Rodriguez. On the Laplacian spectrum and walk-regular hypergraphs. Linear and Multilinear Algebra , 51 \\(3\\) :285\u2013297, 2003. \u21a9 Juan Alberto Rodriguez. Laplacian eigenvalues and partition problems in hypergraphs. Applied Mathematics Letters , 22 \\(6\\) :916\u2013921, 2009. \u21a9 Sameer Agarwal, Kristin Branson, and Serge Belongie. Higher order learning with graphs. In International Conference on Machine Learning \\(ICML\\) , 17\u201324. 2006. \u21a9 Dengyong Zhou, Jiayuan Huang, and Bernhard Sch\u00f6lkopf. Learning with hypergraphs: clustering, classification, and embedding. In Advances in Neural Information Processing Systems \\(NeurIPS\\) , 1601\u20131608. 2007. \u21a9 Sameer Agarwal, Jongwoo Lim, Lihi Zelnik-Manor, Pietro Perona, David Kriegman, and Serge Belongie. Beyond pairwise clustering. In Conference on Computer Vision and Pattern Recognition \\(CVPR\\) , volume 2, 838\u2013845. IEEE, 2005. \u21a9 Anirban Banerjee. On the spectrum of hypergraphs. Linear Algebra and its Applications , 614:82\u2013110, 2021. \u21a9 Guilherme Ferraz de Arruda, Michele Tizzani, and Yamir Moreno. Phase transitions and stability of dynamical processes on hypergraphs. Communications Physics , 4 \\(1\\) :1\u20139, 2021. \u21a9 Jason Y Zien, Martine DF Schlag, and Pak K Chan. Multilevel spectral hypergraph partitioning with arbitrary vertex sizes. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 18 \\(9\\) :1389\u20131399, 1999. \u21a9 Austin R Benson. Three hypergraph eigenvector centralities. SIAM Journal on Mathematics of Data Science , 1 \\(2\\) :293\u2013312, 2019. \u21a9 \u21a9 \u21a9 \u21a9 Francesco Tudisco and Desmond J. Higham. Node and edge eigenvector centrality for hypergraphs. arXiv:2101.06215 , 2021. \u21a9","title":"The hypergraph sunflower"},{"location":"ch3/sec1/","text":"The Birkhoff-hopf theorem and norms of positive matrices","title":"The Birkhoff-hopf theorem and norms of positive matrices"},{"location":"ch3/sec1/#the_birkhoff-hopf_theorem_and_norms_of_positive_matrices","text":"","title":"The Birkhoff-hopf theorem and norms of positive matrices"},{"location":"ch3/sec2/","text":"Multilinear Birkhoff-Hopf theorem","title":"Multilinear Birkhoff-Hopf theorem"},{"location":"ch3/sec2/#multilinear_birkhoff-hopf_theorem","text":"","title":"Multilinear Birkhoff-Hopf theorem"},{"location":"ch4/sec1/","text":"Optimization of sub-multihomogeneous functions The connection between eigenvalue problems and constrained optimization is relatively standard. For example, the singular value equations \\(M y = \\lambda x\\) , \\(M^\\top x = \\lambda y\\) of a matrix \\(M\\) always admit a variational characterization which connects the solutions of the constrained optimization of \\(f(x,y) = x^\\top M y\\) to the singular values and vectors of \\(M\\) . In particular, the constrained optimization problem \\[\\begin{equation}\\label{eq:matrix-variational-problem} \\left\\{ \\begin{array}{ll} \\text{max} & f(x,y) \\\\[.4em] \\text{subject to}& \\|x\\|= \\|y\\|=1 \\end{array} \\right. \\end{equation}\\] is given by the extreme left and right singular vectors of \\(M\\) . Thus, while the optimization problem itself is not convex in general (for a general \\(M\\) ), we can compute it using ideas from numerical linear algebra, in particular, numerical eigensolvers. This relatively simple connection has led to powerful numerical optimizers in recent years, see e.g. 1 2 3 . A simple application of Euler\u2019s theorem shows that an analogous property holds for the more general constrained optimization problem \\[\\begin{equation}\\label{eq:constrained-opt} \\left\\{ \\begin{array}{ll} \\text{max}_{x\\in \\RR^n} & f(x^{(1)}, \\dots, x^{(s)})\\\\ \\text{subject to}& g_1(x^{(1)})=\\dots=g_s(x^{(s)})=1, \\end{array} \\right. \\end{equation}\\] where \\(f\\) and \\(g_i\\) are multihomogeneous or, more generally, sub-multihomogeneous. However, when \\(f\\) and \\(g_i\\) are not quadratic, this optimization problem can be significantly more challenging than \\(\\eqref{eq:matrix-variational-problem}\\) . As we will see, even smooth functions \\(f\\) and \\(g_i\\) may lead to NP-hard problems. However , when \\(f\\) and \\(g_i\\) are nonnegative, we can often solve \\(\\eqref{eq:constrained-opt}\\) to an arbitrary accuracy by leveraging and extending the Perron-Frobenius theory for multihomogeneous mappings. This is a main result we will present below, in the Global optimization setting section. Before doing this, we discuss an example which shows how the constrained optimization of homogeneous functions pops up in combinatorial optimization and has applications for example in graph clustering. Optimization of combinatorial ratios Consider a set of integers \\(V=\\{1,\\dots, n\\}\\) . A combinatorial ratio is a function \\(f:2^V\\to\\RR\\) of the form \\[\\begin{equation}\\label{eq:general-ratio} \\vartheta(S) = \\frac{\\alpha(S)}{\\beta(S)} \\end{equation}\\] where both \\(\\alpha\\) and \\(\\beta\\) are set functions \\(\\alpha,\\beta:2^V\\to\\RR\\) . The optimization of this type of functions arises in many context. One example of interest to us is graph clustering. The optimization of the graph conductance is a classical and successful strategy for finding a good partition of a graph into two subsets. In this case \\(V\\) is the vertex set of a graph \\(G=(V,E)\\) with adjacency matrix \\(A\\) and the global minimum of \\[ \\vartheta(S) = \\frac{A(S,\\bar S)}{\\min\\{\\nu(S),\\nu(\\bar S)\\}} \\] where \\(\\bar S = V\\setminus S\\) is the complement of \\(S\\) in \\(V\\) , \\(A(S,\\bar S) = \\sum_{i\\in S, j\\in \\bar S}A_{ij}\\) is the overall weight of edges going from \\(S\\) to \\(\\bar S\\) and \\(\\nu(S)=\\sum_{i\\in S}\\nu_i\\) is the weight of the set \\(S\\) , for some weight coefficients \\(\\nu_i\\) . Minimizing \\(\\vartheta\\) is in general a NP-hard problem, however a homogeneous relaxation based on the Lovasz extension of \\(\\alpha\\) and \\(\\beta\\) allows us to obtain arbitrarily good continuous approximation. This was first observed in [.....matthias....] for \\(1\\) -homogeneous functions, but it is not difficult to extend that idea to the case of \\(p\\) -homogeneous ones: Theorem. Let \\(\\alpha,\\beta:2^V\\to\\RR\\) be such that \\(\\alpha(S),\\beta(S)\\geq 0\\) for all \\(S\\subseteq V\\) and \\(\\alpha(V)=\\alpha(\\emptyset)=\\beta(V)=\\beta(\\emptyset)=0\\) . For any \\(p\\geq 1\\) there exist a constant \\(C\\) independent of \\(n\\) and \\(p\\) and homogeneous functions \\(f,g:\\RR^n\\to\\RR\\) with homogeneity degree \\(p\\) such that, if \\(\\lambda\\) is the solution to \\[\\begin{equation}\\label{eq:thm-cheeger} \\lambda = \\min \\Big\\{ f(x) \\text{ such that } g(x) = 1 \\Big\\} \\end{equation}\\] then \\(\\lambda \\leq \\min_S \\vartheta(S)\\leq C^{p-1} \\lambda^{1/p}\\) . In particular, \\(\\lambda \\xrightarrow{p\\to 1} \\min \\vartheta\\) . Both \\(f\\) and \\(g\\) in the theorem above are defined in terms of the Lovasz extension of \\(\\alpha\\) and \\(\\beta\\) . For certain combinatorial functions this extension admits an explicit formula. For example, when tailored to the clustering problem \\(\\alpha(S) = A(S,\\bar S)\\) and \\(\\beta(S) =\\min\\{\\nu(S),\\nu(\\bar S)\\}\\) , the \\(p\\) -homogeneous functions in the theorem are \\[ f(x) = \\frac 1 2 \\sum_{ij}A_{ij}|x_i-x_j|^p \\qquad \\text{and} \\qquad g(x) = \\|x-\\mathrm{mean}(x)\\one\\|_p^p\\, . \\] In particular, \\(f(x)\\) is the energy associated to the so-called \\(p\\) -Laplacian on \\(G\\) and in fact, it is not difficult to show that for these two \\(f\\) and \\(g\\) , \\(\\lambda\\) in the theorem coincides with the smallest nonzero eigenvalue of the graph \\(p\\) -Laplacian, defined as \\[ L_p(x) = B\\Phi(B^\\top x), \\qquad \\Phi(x) = |x|^{p-1}\\mathrm{sign}(x) \\] with \\(B\\) the incidence matrix of \\(G\\) (see the network centrality section for its definition). In particular, \\(L_p\\) boils down to the graph Laplacian matrix when \\(p=2\\) and we obtain the renowned Cheeger inequality as a corollary of the theorem above. This observation shows that the optimization of the graph conductance can be approached via the solution of so-called nonlinear eigenvalue problems with eigenvector nonlinearities. In fact, using the Euler theorem for (multi)homogeneous functions, we can always connect the constrained optimization problem \\(\\eqref{eq:constrained-opt}\\) with a nonlinear eigenvector problem, as we will discuss next. Sub-multihomogeneous mappings Our main result concerns the global solution of the constrained optimization problem \\(\\eqref{eq:constrained-opt}\\) and holds for a family of operators that generalized the already discussed set of homogeneous mappings. This is the class of sub-multihomogeneous maps. We provide here their definition, first starting from a different characterization of the multihomogeneous ones. A mapping \\(F:\\RR^n\\to\\RR^m\\) is homogeneous with homogeneity coefficient \\(p\\) if \\(F(\\lambda x)=\\lambda^p F(x)\\) for all \\(\\lambda>0\\) and \\(x\\succeq 0\\) . When \\(F\\) is differentiable, Euler\u2019s theorem for homogeneous functions allows us to equivalently characterize this property in terms of the Jacobian of \\(F\\) , which we denote by \\(\\partial F:\\RR^n\\to\\RR^{m\\times n}\\) . Precisely, we have Theorem (Euler). Let \\(F:\\RR^n\\to\\RR^{m}\\) be differentiable. Then \\(F\\) is homogeneous with homogeneity coefficient \\(p\\neq 0\\) if and only if \\(\\partial F(x)x = p F(x)\\) for all \\(x\\succ 0\\) . This result can be extended to the multihomogeneous setting. In fact, as for homogeneous functions, multihomogeneous mappings enjoy a similar Euler-based characterization. To fix the ideas, consider first the case of two variable sets \\(x = (x^{(1)},x^{(2)})\\) with \\(x^{(1)}\\in\\RR^{n_1}\\) and \\(x^{(2)}\\in \\RR^{n_2}\\) and \\(n_1+n_2=n\\) . Let \\(F= (F_1,F_2):\\RR^n\\to\\RR^n\\) be a multihomogeneous mapping with \\(2\\times 2\\) homogeneity matrix \\(\\M\\) . In this notation, each \\(F_i\\) is a map from \\(\\RR^{n_1+n_2}\\) to \\(\\RR^{n_i}\\) . Moreover, we assume \\(\\partial_j = \\partial_{x^{(j)}}\\) denotes the partial derivative with respect to the set of variables \\(x^{(j)}\\) then, for any \\(x\\in \\RR^{n_1+n_2}\\) , \\(\\partial_j F_i(x)\\) is a \\(n_i\\times n_j\\) matrix and we can partition \\(\\M\\) , \\(F\\) , and its Jacobian \\(\\partial F\\) as \\[ \\M = \\begin{bmatrix} \\M_{11} & \\M_{12}\\\\ \\M_{21} & \\M_{22} \\end{bmatrix} \\qquad F(x) = \\left[ \\begin{array}{c} \\begin{array}{c} \\\\ F_1(x)\\\\ \\, \\end{array} \\\\ \\hline F_2(x) \\end{array}\\right] \\qquad \\partial F(x) = \\left[ \\begin{array}{c|c} \\begin{array}{ccc} && \\\\ & \\partial_{1}F_1(x) &\\\\ && \\end{array} & \\partial_2 F_1(x) \\\\ \\hline \\partial_1 F_2(x) & \\partial_2 F_2(x) \\end{array}\\right] \\] If we apply the Euler\u2019s theorem to each of the blocks individually, we obtain that the following identities \\[ \\begin{cases} \\partial_1 F_1(x)x^{(1)} = \\M_{11} F_1(x) & \\partial_2 F_1(x)x^{(2)} = \\M_{12} F_1(x) \\\\ \\partial_1 F_2(x)x^{(1)} = \\M_{21} F_2(x) & \\partial_2 F_2(x)x^{(2)} = \\M_{22} F_2(x) \\end{cases} \\] equivalently characterize \\(F\\) as a multihomogeneous mapping. More in general, we have Theorem. Let \\(F=(F_1,\\dots,F_t):\\RR^n\\to\\RR^m\\) be differentiable. Then \\(F\\) is multihomogeneous of homogeneity matrix \\(\\M\\) of size \\(s\\times t\\) if and only if there exists a subdivision of the variable \\(x\\) into \\(x=(x^{(1)}, \\dots, x^{(s)})\\) such that \\[ \\partial_j F_i(x) x^{(j)} = \\M_{ij} \\, F_i(x) \\] for all \\(i,j\\) and all \\(x=(x^{(1)}, \\dots, x^{(s)})\\succ 0\\) . The theorem above fully characterizes differentiable multihomogeneous operators and, when \\(F\\) is order-preserving, i.e. \\(F(x)\\succeq 0\\) when \\(x\\succeq 0\\) , it implies that \\(|\\partial_j F_i(x)| x^{(j)} = |\\M_{ij}| \\, F_i(x)\\) for a multihomogeneous mapping, where absoulute values are taken entrywise. This motivates the following Definition. A differentiable \\(F:\\RR^n\\to\\RR^m\\) is sub-multihomogeneous of homogeneity matrix \\(\\M\\) if there exists a partition of the variable \\(x\\) such that \\[ |\\M_{ij}| = \\min\\Big\\{\\lambda \\geq 0 \\, : \\, |\\partial_j F_i(x)| x^{(j)} \\leq \\lambda \\, F_i(x) \\Big\\} \\] for all \\(i,j\\) and all \\(x\\succ 0\\) . Clearly, any multihomogeneous operator is also sub-multihomogeneous, with the same homogeneity matrix. In what follows we will consider real-valued functions \\(f:\\RR^n\\to \\RR\\) whose gradient \\(\\partial f:\\RR^n \\to\\RR^n\\) is sub-multihomogeneous, i.e. functions \\(f\\) for which there exists a splitting \\(x=(x^{(1)},\\dots, x^{(s)})\\) and a matrix \\(\\\\M\\) of size \\(s\\times s\\) , such that \\[\\begin{equation*}%\\label{eq:euler-thm-for-the-hessian} \\Big|\\partial_{j}\\partial_i f(x) \\Big|x^{(j)} = |\\M_{ij}| \\partial_i f(x) \\end{equation*}\\] holds for all \\(i,j=1,\\dots,s\\) and all \\(x\\succ 0\\) . Before moving on, it is worth noting that if \\(f\\) is itself multihomogeneous then this property is preserved by the gradient. More precisely, let \\(f:\\RR^n\\to \\RR\\) be a differentiable function and suppose \\(f\\) is multihomogeneous. As \\(m=1\\) in this case, its homogeneity matrix must be a row vector \\(\\delta^\\top\\) , and we have \\[ f(x^{(1)},\\dots, \\lambda x^{(j)}, \\dots, x^{(m)}) = \\lambda^{\\delta_j} f(x)\\, . \\] For brevity, let \\(x_j(\\lambda) = (x^{(1)},\\dots, \\lambda x^{(j)}, \\dots, x^{(m)})\\) . If we differentiate the previous identity we get \\[ \\partial_j f(x_j(\\lambda)) = \\lambda^{\\delta_j-1} \\partial_j f(x), \\qquad \\partial_i f(x_j(\\lambda)) = \\lambda^{\\delta_j} \\partial_i f(x), \\quad i\\neq j \\] which shows that \\(\\partial f\\) is homogeneous of homogeneity matrix \\(\\M\\) with \\[ \\M = \\begin{bmatrix} \\delta_1 - 1 & \\delta_2 & \\dots & \\delta_s\\\\ \\delta_1 & \\delta_2-1 & \\dots & \\delta_s\\\\ \\vdots & & \\ddots & \\vdots \\\\ \\delta_1 & \\dots & \\delta_{s-1} & \\delta_s - 1 \\end{bmatrix} = \\one\\delta^\\top - I \\, . \\] Global optimization setting Consider the following constrained optimization problem \\[\\begin{equation}\\label{eq:constrained_opt_prob_3} \\left\\{ \\begin{array}{ll} \\text{maximize} & f(x)\\\\ \\text{subject to}& g_1(x^{(1)})=\\dots = g_s(x^{(s)})=1 \\end{array} \\right. \\end{equation}\\] where the gradient of \\(f:\\RR^n\\to\\RR\\) is sub-multihomogeneous and each \\(g_i:\\RR^{n_i}\\to \\RR\\) is a real-valued homogeneous function. The following Theorem shows that, under appropriate conditions on \\(f\\) and \\(g\\) , we can always compute an arbitrary accurate approximation of the global solution to \\(\\eqref{eq:constrained_opt_prob_3}\\) . The key to this result is the transformation of \\(\\eqref{eq:constrained_opt_prob_3}\\) into a nonlinear eigenvector problem, which one obtains relatively directly. In fact, using the Lagrangian we see that \\(x\\) is a critical point only if it is a solution to the system of the nonlinear singular vector equations \\[\\begin{equation}\\label{eq:tmp} \\partial_i f(x) = \\lambda_i \\partial g_i(x) \\end{equation}\\] which, if \\(\\partial g_i\\) are invertible and have the appropriate homogeneity and order-preserving structure, we can solve via the nonlinear power method. All together we get: Theorem. Recall that a mapping \\(F\\) is positive if \\(F(x)\\succ 0\\) when \\(x\\succ 0\\) . Let \\(f:\\RR^n\\to\\RR\\) be sub-multihomogeneous with homogeneity matrix \\(\\M\\) and twice differentiable. Let \\(g_i:\\RR^{n_i}\\to\\RR\\) be homogeneous with homogeneity coefficient \\(1+\\alpha_i\\) with \\(\\alpha_i\\neq 0\\) and suppose the gradient \\(\\partial g_i\\) is invertible on \\(\\RR^{n_i}_{++}\\) and positive. Consider the matrix \\[ K = \\mathrm{Diag}(\\alpha_1,\\dots,\\alpha_s)^{-1}\\M \\] and let \\(|K|\\) denote the entrywise absolute value. If either \\(\\rho(|K|)<1/2\\) or \\(\\rho(|K|)<1\\) and \\(\\partial^2 f\\) is a positive map, then There exists a unique solution \\(x^*\\in \\RR^n\\) to \\(\\eqref{eq:constrained_opt_prob_3}\\) and \\(x^*\\succ 0\\) ; It holds \\(\\partial_i f(x^*) = \\lambda_i \\partial g_i(x^*)\\) for all \\(i=1,\\dots,s\\) and the coefficients \\(\\lambda_i\\) are maximal nonlinear singular vectors, in the sense that any other \\(\\tilde \\lambda_i\\) solution of \\(\\eqref{eq:tmp}\\) is such that \\(\\lambda_i\\geq \\tilde \\lambda_i\\) ; The nonlinear power method sketched below converges to \\(x^*\\) x = [ [ ones ( n [ i ])] for i in 1 : s ] for k = 0 , 1 , 2 , ... y = \u2202_i f ( x ) z s . t . \u2202g_i ( z ) = y x = [ [ x [ i ] ./ g_i ( x [ i ])] for i in 1 : s ] Satoru Adachi, Satoru Iwata, Yuji Nakatsukasa, and Akiko Takeda. Solving the trust-region subproblem by a generalized eigenvalue problem. SIAM Journal on Optimization , 27 \\(1\\) :269\u2013291, 2017. \u21a9 Satoru Adachi and Yuji Nakatsukasa. Eigenvalue-based algorithm and analysis for nonconvex QCQP with one constraint. Mathematical Programming , 173 \\(1\\-2\\) :79\u2013116, 2019. \u21a9 Shinsaku Sakaue, Yuji Nakatsukasa, Akiko Takeda, and Satoru Iwata. Solving generalized CDT problems via two-parameter eigenvalues. SIAM Journal on Optimization , 26 \\(3\\) :1669\u20131694, 2016. \u21a9","title":"Optimization of sub-multihomogeneous functions"},{"location":"ch4/sec1/#optimization_of_sub-multihomogeneous_functions","text":"The connection between eigenvalue problems and constrained optimization is relatively standard. For example, the singular value equations \\(M y = \\lambda x\\) , \\(M^\\top x = \\lambda y\\) of a matrix \\(M\\) always admit a variational characterization which connects the solutions of the constrained optimization of \\(f(x,y) = x^\\top M y\\) to the singular values and vectors of \\(M\\) . In particular, the constrained optimization problem \\[\\begin{equation}\\label{eq:matrix-variational-problem} \\left\\{ \\begin{array}{ll} \\text{max} & f(x,y) \\\\[.4em] \\text{subject to}& \\|x\\|= \\|y\\|=1 \\end{array} \\right. \\end{equation}\\] is given by the extreme left and right singular vectors of \\(M\\) . Thus, while the optimization problem itself is not convex in general (for a general \\(M\\) ), we can compute it using ideas from numerical linear algebra, in particular, numerical eigensolvers. This relatively simple connection has led to powerful numerical optimizers in recent years, see e.g. 1 2 3 . A simple application of Euler\u2019s theorem shows that an analogous property holds for the more general constrained optimization problem \\[\\begin{equation}\\label{eq:constrained-opt} \\left\\{ \\begin{array}{ll} \\text{max}_{x\\in \\RR^n} & f(x^{(1)}, \\dots, x^{(s)})\\\\ \\text{subject to}& g_1(x^{(1)})=\\dots=g_s(x^{(s)})=1, \\end{array} \\right. \\end{equation}\\] where \\(f\\) and \\(g_i\\) are multihomogeneous or, more generally, sub-multihomogeneous. However, when \\(f\\) and \\(g_i\\) are not quadratic, this optimization problem can be significantly more challenging than \\(\\eqref{eq:matrix-variational-problem}\\) . As we will see, even smooth functions \\(f\\) and \\(g_i\\) may lead to NP-hard problems. However , when \\(f\\) and \\(g_i\\) are nonnegative, we can often solve \\(\\eqref{eq:constrained-opt}\\) to an arbitrary accuracy by leveraging and extending the Perron-Frobenius theory for multihomogeneous mappings. This is a main result we will present below, in the Global optimization setting section. Before doing this, we discuss an example which shows how the constrained optimization of homogeneous functions pops up in combinatorial optimization and has applications for example in graph clustering.","title":"Optimization of sub-multihomogeneous functions"},{"location":"ch4/sec1/#optimization_of_combinatorial_ratios","text":"Consider a set of integers \\(V=\\{1,\\dots, n\\}\\) . A combinatorial ratio is a function \\(f:2^V\\to\\RR\\) of the form \\[\\begin{equation}\\label{eq:general-ratio} \\vartheta(S) = \\frac{\\alpha(S)}{\\beta(S)} \\end{equation}\\] where both \\(\\alpha\\) and \\(\\beta\\) are set functions \\(\\alpha,\\beta:2^V\\to\\RR\\) . The optimization of this type of functions arises in many context. One example of interest to us is graph clustering. The optimization of the graph conductance is a classical and successful strategy for finding a good partition of a graph into two subsets. In this case \\(V\\) is the vertex set of a graph \\(G=(V,E)\\) with adjacency matrix \\(A\\) and the global minimum of \\[ \\vartheta(S) = \\frac{A(S,\\bar S)}{\\min\\{\\nu(S),\\nu(\\bar S)\\}} \\] where \\(\\bar S = V\\setminus S\\) is the complement of \\(S\\) in \\(V\\) , \\(A(S,\\bar S) = \\sum_{i\\in S, j\\in \\bar S}A_{ij}\\) is the overall weight of edges going from \\(S\\) to \\(\\bar S\\) and \\(\\nu(S)=\\sum_{i\\in S}\\nu_i\\) is the weight of the set \\(S\\) , for some weight coefficients \\(\\nu_i\\) . Minimizing \\(\\vartheta\\) is in general a NP-hard problem, however a homogeneous relaxation based on the Lovasz extension of \\(\\alpha\\) and \\(\\beta\\) allows us to obtain arbitrarily good continuous approximation. This was first observed in [.....matthias....] for \\(1\\) -homogeneous functions, but it is not difficult to extend that idea to the case of \\(p\\) -homogeneous ones: Theorem. Let \\(\\alpha,\\beta:2^V\\to\\RR\\) be such that \\(\\alpha(S),\\beta(S)\\geq 0\\) for all \\(S\\subseteq V\\) and \\(\\alpha(V)=\\alpha(\\emptyset)=\\beta(V)=\\beta(\\emptyset)=0\\) . For any \\(p\\geq 1\\) there exist a constant \\(C\\) independent of \\(n\\) and \\(p\\) and homogeneous functions \\(f,g:\\RR^n\\to\\RR\\) with homogeneity degree \\(p\\) such that, if \\(\\lambda\\) is the solution to \\[\\begin{equation}\\label{eq:thm-cheeger} \\lambda = \\min \\Big\\{ f(x) \\text{ such that } g(x) = 1 \\Big\\} \\end{equation}\\] then \\(\\lambda \\leq \\min_S \\vartheta(S)\\leq C^{p-1} \\lambda^{1/p}\\) . In particular, \\(\\lambda \\xrightarrow{p\\to 1} \\min \\vartheta\\) . Both \\(f\\) and \\(g\\) in the theorem above are defined in terms of the Lovasz extension of \\(\\alpha\\) and \\(\\beta\\) . For certain combinatorial functions this extension admits an explicit formula. For example, when tailored to the clustering problem \\(\\alpha(S) = A(S,\\bar S)\\) and \\(\\beta(S) =\\min\\{\\nu(S),\\nu(\\bar S)\\}\\) , the \\(p\\) -homogeneous functions in the theorem are \\[ f(x) = \\frac 1 2 \\sum_{ij}A_{ij}|x_i-x_j|^p \\qquad \\text{and} \\qquad g(x) = \\|x-\\mathrm{mean}(x)\\one\\|_p^p\\, . \\] In particular, \\(f(x)\\) is the energy associated to the so-called \\(p\\) -Laplacian on \\(G\\) and in fact, it is not difficult to show that for these two \\(f\\) and \\(g\\) , \\(\\lambda\\) in the theorem coincides with the smallest nonzero eigenvalue of the graph \\(p\\) -Laplacian, defined as \\[ L_p(x) = B\\Phi(B^\\top x), \\qquad \\Phi(x) = |x|^{p-1}\\mathrm{sign}(x) \\] with \\(B\\) the incidence matrix of \\(G\\) (see the network centrality section for its definition). In particular, \\(L_p\\) boils down to the graph Laplacian matrix when \\(p=2\\) and we obtain the renowned Cheeger inequality as a corollary of the theorem above. This observation shows that the optimization of the graph conductance can be approached via the solution of so-called nonlinear eigenvalue problems with eigenvector nonlinearities. In fact, using the Euler theorem for (multi)homogeneous functions, we can always connect the constrained optimization problem \\(\\eqref{eq:constrained-opt}\\) with a nonlinear eigenvector problem, as we will discuss next.","title":"Optimization of combinatorial ratios"},{"location":"ch4/sec1/#sub-multihomogeneous_mappings","text":"Our main result concerns the global solution of the constrained optimization problem \\(\\eqref{eq:constrained-opt}\\) and holds for a family of operators that generalized the already discussed set of homogeneous mappings. This is the class of sub-multihomogeneous maps. We provide here their definition, first starting from a different characterization of the multihomogeneous ones. A mapping \\(F:\\RR^n\\to\\RR^m\\) is homogeneous with homogeneity coefficient \\(p\\) if \\(F(\\lambda x)=\\lambda^p F(x)\\) for all \\(\\lambda>0\\) and \\(x\\succeq 0\\) . When \\(F\\) is differentiable, Euler\u2019s theorem for homogeneous functions allows us to equivalently characterize this property in terms of the Jacobian of \\(F\\) , which we denote by \\(\\partial F:\\RR^n\\to\\RR^{m\\times n}\\) . Precisely, we have Theorem (Euler). Let \\(F:\\RR^n\\to\\RR^{m}\\) be differentiable. Then \\(F\\) is homogeneous with homogeneity coefficient \\(p\\neq 0\\) if and only if \\(\\partial F(x)x = p F(x)\\) for all \\(x\\succ 0\\) . This result can be extended to the multihomogeneous setting. In fact, as for homogeneous functions, multihomogeneous mappings enjoy a similar Euler-based characterization. To fix the ideas, consider first the case of two variable sets \\(x = (x^{(1)},x^{(2)})\\) with \\(x^{(1)}\\in\\RR^{n_1}\\) and \\(x^{(2)}\\in \\RR^{n_2}\\) and \\(n_1+n_2=n\\) . Let \\(F= (F_1,F_2):\\RR^n\\to\\RR^n\\) be a multihomogeneous mapping with \\(2\\times 2\\) homogeneity matrix \\(\\M\\) . In this notation, each \\(F_i\\) is a map from \\(\\RR^{n_1+n_2}\\) to \\(\\RR^{n_i}\\) . Moreover, we assume \\(\\partial_j = \\partial_{x^{(j)}}\\) denotes the partial derivative with respect to the set of variables \\(x^{(j)}\\) then, for any \\(x\\in \\RR^{n_1+n_2}\\) , \\(\\partial_j F_i(x)\\) is a \\(n_i\\times n_j\\) matrix and we can partition \\(\\M\\) , \\(F\\) , and its Jacobian \\(\\partial F\\) as \\[ \\M = \\begin{bmatrix} \\M_{11} & \\M_{12}\\\\ \\M_{21} & \\M_{22} \\end{bmatrix} \\qquad F(x) = \\left[ \\begin{array}{c} \\begin{array}{c} \\\\ F_1(x)\\\\ \\, \\end{array} \\\\ \\hline F_2(x) \\end{array}\\right] \\qquad \\partial F(x) = \\left[ \\begin{array}{c|c} \\begin{array}{ccc} && \\\\ & \\partial_{1}F_1(x) &\\\\ && \\end{array} & \\partial_2 F_1(x) \\\\ \\hline \\partial_1 F_2(x) & \\partial_2 F_2(x) \\end{array}\\right] \\] If we apply the Euler\u2019s theorem to each of the blocks individually, we obtain that the following identities \\[ \\begin{cases} \\partial_1 F_1(x)x^{(1)} = \\M_{11} F_1(x) & \\partial_2 F_1(x)x^{(2)} = \\M_{12} F_1(x) \\\\ \\partial_1 F_2(x)x^{(1)} = \\M_{21} F_2(x) & \\partial_2 F_2(x)x^{(2)} = \\M_{22} F_2(x) \\end{cases} \\] equivalently characterize \\(F\\) as a multihomogeneous mapping. More in general, we have Theorem. Let \\(F=(F_1,\\dots,F_t):\\RR^n\\to\\RR^m\\) be differentiable. Then \\(F\\) is multihomogeneous of homogeneity matrix \\(\\M\\) of size \\(s\\times t\\) if and only if there exists a subdivision of the variable \\(x\\) into \\(x=(x^{(1)}, \\dots, x^{(s)})\\) such that \\[ \\partial_j F_i(x) x^{(j)} = \\M_{ij} \\, F_i(x) \\] for all \\(i,j\\) and all \\(x=(x^{(1)}, \\dots, x^{(s)})\\succ 0\\) . The theorem above fully characterizes differentiable multihomogeneous operators and, when \\(F\\) is order-preserving, i.e. \\(F(x)\\succeq 0\\) when \\(x\\succeq 0\\) , it implies that \\(|\\partial_j F_i(x)| x^{(j)} = |\\M_{ij}| \\, F_i(x)\\) for a multihomogeneous mapping, where absoulute values are taken entrywise. This motivates the following Definition. A differentiable \\(F:\\RR^n\\to\\RR^m\\) is sub-multihomogeneous of homogeneity matrix \\(\\M\\) if there exists a partition of the variable \\(x\\) such that \\[ |\\M_{ij}| = \\min\\Big\\{\\lambda \\geq 0 \\, : \\, |\\partial_j F_i(x)| x^{(j)} \\leq \\lambda \\, F_i(x) \\Big\\} \\] for all \\(i,j\\) and all \\(x\\succ 0\\) . Clearly, any multihomogeneous operator is also sub-multihomogeneous, with the same homogeneity matrix. In what follows we will consider real-valued functions \\(f:\\RR^n\\to \\RR\\) whose gradient \\(\\partial f:\\RR^n \\to\\RR^n\\) is sub-multihomogeneous, i.e. functions \\(f\\) for which there exists a splitting \\(x=(x^{(1)},\\dots, x^{(s)})\\) and a matrix \\(\\\\M\\) of size \\(s\\times s\\) , such that \\[\\begin{equation*}%\\label{eq:euler-thm-for-the-hessian} \\Big|\\partial_{j}\\partial_i f(x) \\Big|x^{(j)} = |\\M_{ij}| \\partial_i f(x) \\end{equation*}\\] holds for all \\(i,j=1,\\dots,s\\) and all \\(x\\succ 0\\) . Before moving on, it is worth noting that if \\(f\\) is itself multihomogeneous then this property is preserved by the gradient. More precisely, let \\(f:\\RR^n\\to \\RR\\) be a differentiable function and suppose \\(f\\) is multihomogeneous. As \\(m=1\\) in this case, its homogeneity matrix must be a row vector \\(\\delta^\\top\\) , and we have \\[ f(x^{(1)},\\dots, \\lambda x^{(j)}, \\dots, x^{(m)}) = \\lambda^{\\delta_j} f(x)\\, . \\] For brevity, let \\(x_j(\\lambda) = (x^{(1)},\\dots, \\lambda x^{(j)}, \\dots, x^{(m)})\\) . If we differentiate the previous identity we get \\[ \\partial_j f(x_j(\\lambda)) = \\lambda^{\\delta_j-1} \\partial_j f(x), \\qquad \\partial_i f(x_j(\\lambda)) = \\lambda^{\\delta_j} \\partial_i f(x), \\quad i\\neq j \\] which shows that \\(\\partial f\\) is homogeneous of homogeneity matrix \\(\\M\\) with \\[ \\M = \\begin{bmatrix} \\delta_1 - 1 & \\delta_2 & \\dots & \\delta_s\\\\ \\delta_1 & \\delta_2-1 & \\dots & \\delta_s\\\\ \\vdots & & \\ddots & \\vdots \\\\ \\delta_1 & \\dots & \\delta_{s-1} & \\delta_s - 1 \\end{bmatrix} = \\one\\delta^\\top - I \\, . \\]","title":"Sub-multihomogeneous mappings"},{"location":"ch4/sec1/#global_optimization_setting","text":"Consider the following constrained optimization problem \\[\\begin{equation}\\label{eq:constrained_opt_prob_3} \\left\\{ \\begin{array}{ll} \\text{maximize} & f(x)\\\\ \\text{subject to}& g_1(x^{(1)})=\\dots = g_s(x^{(s)})=1 \\end{array} \\right. \\end{equation}\\] where the gradient of \\(f:\\RR^n\\to\\RR\\) is sub-multihomogeneous and each \\(g_i:\\RR^{n_i}\\to \\RR\\) is a real-valued homogeneous function. The following Theorem shows that, under appropriate conditions on \\(f\\) and \\(g\\) , we can always compute an arbitrary accurate approximation of the global solution to \\(\\eqref{eq:constrained_opt_prob_3}\\) . The key to this result is the transformation of \\(\\eqref{eq:constrained_opt_prob_3}\\) into a nonlinear eigenvector problem, which one obtains relatively directly. In fact, using the Lagrangian we see that \\(x\\) is a critical point only if it is a solution to the system of the nonlinear singular vector equations \\[\\begin{equation}\\label{eq:tmp} \\partial_i f(x) = \\lambda_i \\partial g_i(x) \\end{equation}\\] which, if \\(\\partial g_i\\) are invertible and have the appropriate homogeneity and order-preserving structure, we can solve via the nonlinear power method. All together we get: Theorem. Recall that a mapping \\(F\\) is positive if \\(F(x)\\succ 0\\) when \\(x\\succ 0\\) . Let \\(f:\\RR^n\\to\\RR\\) be sub-multihomogeneous with homogeneity matrix \\(\\M\\) and twice differentiable. Let \\(g_i:\\RR^{n_i}\\to\\RR\\) be homogeneous with homogeneity coefficient \\(1+\\alpha_i\\) with \\(\\alpha_i\\neq 0\\) and suppose the gradient \\(\\partial g_i\\) is invertible on \\(\\RR^{n_i}_{++}\\) and positive. Consider the matrix \\[ K = \\mathrm{Diag}(\\alpha_1,\\dots,\\alpha_s)^{-1}\\M \\] and let \\(|K|\\) denote the entrywise absolute value. If either \\(\\rho(|K|)<1/2\\) or \\(\\rho(|K|)<1\\) and \\(\\partial^2 f\\) is a positive map, then There exists a unique solution \\(x^*\\in \\RR^n\\) to \\(\\eqref{eq:constrained_opt_prob_3}\\) and \\(x^*\\succ 0\\) ; It holds \\(\\partial_i f(x^*) = \\lambda_i \\partial g_i(x^*)\\) for all \\(i=1,\\dots,s\\) and the coefficients \\(\\lambda_i\\) are maximal nonlinear singular vectors, in the sense that any other \\(\\tilde \\lambda_i\\) solution of \\(\\eqref{eq:tmp}\\) is such that \\(\\lambda_i\\geq \\tilde \\lambda_i\\) ; The nonlinear power method sketched below converges to \\(x^*\\) x = [ [ ones ( n [ i ])] for i in 1 : s ] for k = 0 , 1 , 2 , ... y = \u2202_i f ( x ) z s . t . \u2202g_i ( z ) = y x = [ [ x [ i ] ./ g_i ( x [ i ])] for i in 1 : s ] Satoru Adachi, Satoru Iwata, Yuji Nakatsukasa, and Akiko Takeda. Solving the trust-region subproblem by a generalized eigenvalue problem. SIAM Journal on Optimization , 27 \\(1\\) :269\u2013291, 2017. \u21a9 Satoru Adachi and Yuji Nakatsukasa. Eigenvalue-based algorithm and analysis for nonconvex QCQP with one constraint. Mathematical Programming , 173 \\(1\\-2\\) :79\u2013116, 2019. \u21a9 Shinsaku Sakaue, Yuji Nakatsukasa, Akiko Takeda, and Satoru Iwata. Solving generalized CDT problems via two-parameter eigenvalues. SIAM Journal on Optimization , 26 \\(3\\) :1669\u20131694, 2016. \u21a9","title":"Global optimization setting"},{"location":"ch4/sec2/","text":"The core-periphery detection problem Many such tasks can be viewed in terms of categorizing nodes or discovering hidden substructures that relate them. A relevant problem in network science and data mining is to discover hidden structures in a given network. Clustering and community detection are perhaps the most widely studied such problem, and it forms the basis of many classification algorithms [@BLSZ18]. Other relevant problems include the anti-community detection problem and the core-periphery one, which we discuss next. In core-periphery detection we seek a set of nodes that are highly connected internally and with the rest of the network, forming the core , and a set of peripheral nodes that are strongly connected to the core but have only sparse internal connections. This kind of structure is important for a number of reasons. For example, identifying core-periphery structures can help in identifying and categorizing hubs, i.e., well-connected nodes. As noted in 1 , such nodes often occur in real-world networks. This is an issue for some community detection methods, as hubs tend to be connected to many different communities and, thus, can be awkward to classify. Moreover, the set of core nodes can be used to identify internally cohesive subgraphs of highly central nodes. In fact, even though all core nodes typically have high centrality score, not all nodes with high centrality measures belong to the core and it is possible to find sparsely connected subgraphs of central nodes not belonging to the core [@borgatti2000models]. M Puck Rombach, Mason A Porter, James H Fowler, and Peter J Mucha. Core-periphery structure in networks. SIAM Journal on Applied mathematics , 74 \\(1\\) :167\u2013190, 2014. \u21a9","title":"The core-periphery detection problem"},{"location":"ch4/sec2/#the_core-periphery_detection_problem","text":"Many such tasks can be viewed in terms of categorizing nodes or discovering hidden substructures that relate them. A relevant problem in network science and data mining is to discover hidden structures in a given network. Clustering and community detection are perhaps the most widely studied such problem, and it forms the basis of many classification algorithms [@BLSZ18]. Other relevant problems include the anti-community detection problem and the core-periphery one, which we discuss next. In core-periphery detection we seek a set of nodes that are highly connected internally and with the rest of the network, forming the core , and a set of peripheral nodes that are strongly connected to the core but have only sparse internal connections. This kind of structure is important for a number of reasons. For example, identifying core-periphery structures can help in identifying and categorizing hubs, i.e., well-connected nodes. As noted in 1 , such nodes often occur in real-world networks. This is an issue for some community detection methods, as hubs tend to be connected to many different communities and, thus, can be awkward to classify. Moreover, the set of core nodes can be used to identify internally cohesive subgraphs of highly central nodes. In fact, even though all core nodes typically have high centrality score, not all nodes with high centrality measures belong to the core and it is possible to find sparsely connected subgraphs of central nodes not belonging to the core [@borgatti2000models]. M Puck Rombach, Mason A Porter, James H Fowler, and Peter J Mucha. Core-periphery structure in networks. SIAM Journal on Applied mathematics , 74 \\(1\\) :167\u2013190, 2014. \u21a9","title":"The core-periphery detection problem"}]}